{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Practicalli Engineering Playbook","text":"<p>Quote</p> <p>Effective Communication is the fundamental challenge for all software engineering projects</p> <p>John Stevenson, Practicalli</p> <p>The Engineering Playbook presents a collection of practical guides which effectively address common software engineering tasks.</p> <ul> <li>A Play is a guide that details practices and an approach for a specific task</li> <li>A Playbook is a collection of Plays that are related by topic, e.g. practices for deployment, accessing databases, system integration testing, optimising builds in docker, etc.</li> </ul> <p></p> <p>Engineering Playbook Initiative</p> <p>Establishing a playbook within an organisation encourages information sharing, providing an effective way for disparate teams to learn from each other.</p>"},{"location":"#discussions-and-feedback","title":"Discussions and feedback","text":"<p>Contributing to Practical.li</p>"},{"location":"#navigate-the-book","title":"Navigate the book","text":"<p>Use the mouse or built-in key bindings to navigate the pages of the book</p> <ul> <li>P , , : go to previous page</li> <li>N , . : go to next page</li> </ul> <p>Use the search box to quickly find a specific topic</p> <ul> <li>F , S , / : open search dialog</li> <li>Down , Up : select next / previous result</li> <li>Esc , Tab : close search dialog</li> <li>Enter : follow selected result</li> </ul>"},{"location":"#sponsor-practicalli","title":"Sponsor Practicalli","text":"<p>All sponsorship funds are used to support the continued development of Practicalli series of books and videos, although most work is done at personal cost and time.</p> <p>Thanks to Cognitect, Nubank and a wide range of other sponsors from the Clojure community for your continued support</p>"},{"location":"#creative-commons-license","title":"Creative commons license","text":"This work is licensed under a Creative Commons Attribution 4.0 ShareAlike License (including images &amp; stylesheets)."},{"location":"assets/images/social/","title":"Social Cards","text":"<p>Social Cards are visual previews of the website that are included when sending links via social media platforms.</p> <p>Material for MkDocs is configured to generate beautiful social cards automatically, using the colors, fonts and logos defined in <code>mkdocs.yml</code></p> <p>Generated images are stored in this directory.</p>"},{"location":"build-tool/","title":"Build Tools","text":"<p>Build tools can support a wide range of development tasks</p> <ul> <li>GNU make - language agnostic build tool, define any tasks</li> <li>babashka - write a built tool using Clojure</li> </ul>"},{"location":"build-tool/babashka-task-runner/","title":"Babashka Task Runner","text":"<p>Babashka is a scripting tool that runs Clojure code.</p> <p>Babashka provides a task runner which can be used as a basis for creating a build tool</p> <p> Babashka Task Runner</p>"},{"location":"build-tool/make/","title":"Make","text":"<p>GNU Make provide a simple and consistent way to run any development task for Clojure &amp; ClojureScript projects (or any other languages).</p> <p>Wrap any combination of tools (building, linting, formatting, testing) with make targets for a simple command line interface, with automatically tab completion, making any Clojure project really easy to work with.  Practicalli also uses make to manage docker images and containers to support Clojure development.</p> <p>All that is required is a  <code>Makefile</code> in the root of the project</p> Practicalli Makefile for Clojure projects .github/config/megalinter.yaml<pre><code># ------------------------------------------\n# Makefile: Clojure Service\n#\n# Consistent set of targets to support local development of Clojure\n# and build the Clojure service during CI deployment\n#\n# Requirements\n# - cljstyle\n# - Clojure CLI aliases\n#   - `:env/dev` to include `dev` directory on class path\n#   - `:env/test` to include `test` directory and libraries to support testing\n#   - `:test/run` to run kaocha kaocha test runner and supporting paths and dependencies\n#   - `:repl/rebel` to start a Rebel terminal UI\n#   - `:package/uberjar` to create an uberjar for the service\n# - docker\n# - mega-linter-runner\n#\n# ------------------------------------------\n\n# .PHONY: ensures target used rather than matching file name\n# https://makefiletutorial.com/#phony\n.PHONY: all lint deps dist pre-commit-check repl test test-ci test-watch clean\n\n# ------- Makefile Variables --------- #\n\n# run help if no target specified\n.DEFAULT_GOAL := help\n\n# Column the target description is printed from\nHELP-DESCRIPTION-SPACING := 24\n\n# Makefile file and directory name wildcard\n# EDN-FILES := $(wildcard *.edn)\n# ------------------------------------ #\n\n# ------- Help ----------------------- #\n# Source: https://nedbatchelder.com/blog/201804/makefile_help_target.html\n\nhelp:  ## Describe available tasks in Makefile\n    @grep '^[a-zA-Z]' $(MAKEFILE_LIST) | \\\n    sort | \\\n    awk -F ':.*?## ' 'NF==2 {printf \"\\033[36m  %-$(HELP-DESCRIPTION-SPACING)s\\033[0m %s\\n\", $$1, $$2}'\n# ------------------------------------ #\n\n# ------- Clojure Development -------- #\n\nrepl:  ## Run Clojure REPL with rich terminal UI (Rebel Readline)\n    $(info --------- Run Rebel REPL ---------)\n    clojure -M:test/env:repl/reloaded\n\n\ndeps: deps.edn  ## Prepare dependencies for test and dist targets\n    $(info --------- Download test and service libraries ---------)\n    clojure -P -X:build\n\n\ndist: build-uberjar ## Build and package Clojure service\n    $(info --------- Build and Package Clojure service ---------)\n\n\n# Remove files and directories after build tasks\n# `-` before the command ignores any errors returned\nclean:  ## Clean build temporary files\n    $(info --------- Clean Clojure classpath cache ---------)\n    - rm -rf ./.cpcache\n# ------------------------------------ #\n\n# ------- Testing -------------------- #\n\ntest-config:  ## Print Kaocha test runner configuration\n        $(info --------- Runner Configuration ---------)\n        clojure -M:test/env:test/run --print-config\n\ntest-profile:  ## Profile unit test speed, showing 3 slowest tests\n        $(info --------- Runner Profile Tests ---------)\n        clojure -M:test/env:test/run --plugin  kaocha.plugin/profiling\n\ntest:  ## Run unit tests - stoping on first error\n        $(info --------- Runner for unit tests ---------)\n        clojure -X:test/env:test/run\n\n\ntest-all:  ## Run all unit tests regardless of failing tests\n        $(info --------- Runner for all unit tests ---------)\n        clojure -X:test/env:test/run :fail-fast? false\n\ntest-watch:  ## Run tests when changes saved, stopping test run on first error\n        $(info --------- Watcher for unit tests ---------)\n        clojure -X:test/env:test/run :watch? true\n\ntest-watch-all:  ## Run all tests when changes saved, regardless of failing tests\n        $(info --------- Watcher for unit tests ---------)\n        clojure -X:test/env:test/run :fail-fast? false :watch? true\n# ------------------------------------ #\n\n\n# -------- Build tasks --------------- #\nbuild-config: ## Pretty print build configuration\n    $(info --------- View current build config ---------)\n    clojure -T:build config\n\nbuild-jar: ## Build a jar archive of Clojure project\n    $(info --------- Build library jar ---------)\n    clojure -T:build jar\n\nbuild-uberjar: ## Build a uberjar archive of Clojure project &amp; Clojure runtime\n    $(info --------- Build service Uberjar  ---------)\n    clojure -T:build uberjar\n\nbuild-clean: ## Clean build assets or given directory\n    $(info --------- Clean Build  ---------)\n    clojure -T:build clean\n# ------------------------------------ #\n\n# ------- Code Quality --------------- #\n\npre-commit-check: format-check lint test  ## Run format, lint and test targets\n\nformat-check: ## Run cljstyle to check the formatting of Clojure code\n    $(info --------- cljstyle Runner ---------)\n    cljstyle check\n\nformat-fix:  ## Run cljstyle and fix the formatting of Clojure code\n    $(info --------- cljstyle Runner ---------)\n    cljstyle fix\n\nlint:  ## Run MegaLinter with custom configuration (node.js required)\n    $(info --------- MegaLinter Runner ---------)\n    npx mega-linter-runner --flavor java --env \"'MEGALINTER_CONFIG=.github/config/megalinter.yaml'\" --remove-container\n\nlint-fix:  ## Run MegaLinter with applied fixes and custom configuration (node.js required)\n    $(info --------- MegaLinter Runner ---------)\n    npx mega-linter-runner --fix --flavor java --env \"'MEGALINTER_CONFIG=.github/config/megalinter.yaml'\" --remove-container\n\nlint-clean:  ## Clean MegaLinter report information\n    $(info --------- MegaLinter Clean Reports ---------)\n    - rm -rf ./megalinter-reports\n# ------------------------------------ #\n\n# ------- Docker Containers ---------- #\n\ndocker-build:  ## Build Clojure project and run with docker compose\n    $(info --------- Docker Compose Build ---------)\n    docker compose up --build --detach\n\ndocker-build-clean:  ## Build Clojure project and run with docker compose, removing orphans\n    $(info --------- Docker Compose Build - remove orphans ---------)\n    docker compose up --build --remove-orphans --detach\n\ndocker-down:  ## Shut down containers in docker compose\n    $(info --------- Docker Compose Down ---------)\n    docker compose down\n\n\nswagger-editor:  ## Start Swagger Editor in Docker\n    $(info --------- Run Swagger Editor at locahost:8282 ---------)\n    docker compose -f swagger-editor.yml up -d swagger-editor\n\nswagger-editor-down:  ## Stop Swagger Editor in Docker\n    $(info --------- Run Swagger Editor at locahost:8282 ---------)\n    docker compose -f swagger-editor.yml down\n# ------------------------------------ #\n\n# ------ Continuous Integration ------ #\n\n# .DELETE_ON_ERROR: halts if command returns non-zero exit status\n# https://makefiletutorial.com/#delete_on_error\n\ntest-ci: deps  ## Test runner for integration tests\n    $(info --------- Runner for integration tests ---------)\n    clojure -P -X:test/env:test/run\n\n# Run tests, build &amp; package the Clojure code and clean up afterward\n# `make all` used in Docker builder stage\n.DELETE_ON_ERROR:\nall: test-ci dist clean  ## Call test-ci dist and clean targets, used for CI\n</code></pre>"},{"location":"build-tool/make/#gnu-make-overview","title":"GNU Make overview","text":"<p>GNU Make is a programming language agnostic build automation tool which has been an integral part of building Linux/Unix operating system code and applications for decades, providing a consistent way to configure, compile and deploy code for all projects.</p> <p>A <code>Makefile</code> defines targets called via the <code>make</code> command. Each target can run one or more commands.  Targets can be dependent on other targets,  e.g the <code>dist</code> target that builds a project can be dependent on <code>deps</code> &amp; <code>test</code> targets.</p> <p>GNU Make is available on all Linux/Unix based operating systems (and Windows via chocolatey or nmake).</p> <p>Practicalli also uses <code>make</code> to configure and build the latest versions of Emacs and other Linux open source software</p>"},{"location":"build-tool/make/#defining-tasks","title":"Defining tasks","text":"<p>Create a <code>Makefile</code> in the root of a project and define a target by typing a suitable name followed by a <code>:</code> character, e.g. <code>test:</code></p> <p>Insert a tab on the next line and type a command to be called.  Further commands can be added on new lines so long as each line is tab indented.</p> <p>The <code>repl</code> target prints out an information message and then uses the Clojure CLI with aliases from Practicalli Clojure CLI Config to run a Clojure REPL process with a rich terminal UI (Rebel Readline)</p> <pre><code>repl:  ## Run Clojure REPL with rich terminal UI (Rebel Readline)\n    $(info --------- Run Rebel REPL ---------)\n    clojure -M:env/dev:env/test:repl/rebel\n</code></pre>"},{"location":"build-tool/make/#common-target-naming","title":"Common target naming","text":"<p>Targets used across Practicalli projects follow the make standard targets for users</p> <p><code>all</code> , <code>test-ci</code> , <code>deps</code> and <code>dist</code> targets are recommended for use with a CI deployment pipeline and builder stage when using Docker.</p> <ul> <li><code>all</code>  calling all targets to prepare the application to be run. e.g. all: deps test-ci dist clean</li> <li><code>deps</code> download library dependencies (depend on <code>deps.edn</code> file)</li> <li><code>dist</code> create a distribution tar file for this program or zip deployment package for AWS Lambda</li> <li><code>lint</code> run lint tools to check code quality  - e.g MegaLinter which provides a wide range of tools</li> <li><code>format-check</code> report format and style issues for a specific programming language</li> <li><code>format-fix</code> update source code files if there are format and style issues for a specific programming language</li> <li><code>pre-commit</code> run unit tests and code quality targets before considering a Git commit</li> <li><code>repl</code> run an interactive run-time environment for the programming language</li> <li><code>test-unit</code> run all unit tests</li> <li><code>test-ci</code> test running in CI build (optionally focus on integration testing)</li> <li><code>clean</code> remove files created by any of the commands from other targets (i.e. ensure a clean build each time)</li> </ul> <p> practicalli/dotfiles/Makefile also defines docker targets to build and compose images locally, inspect images and prune containers and images.</p>"},{"location":"build-tool/make/#target-dependencies","title":"Target dependencies","text":"<p>A <code>Makefile</code> target can depend on either a file name or another target in the <code>Makefile</code>.</p> <p>The all target typically depends on several <code>Makefile</code> targets to test, compile and package a service.  Add the names of the targets this target depends upon</p> <pre><code>all: deps test-ci dist clean\n</code></pre> <p>Add the filename of a file after the name of the target, to depend on if that file has changed.  If the file has not changed since make was last run then the task will not run again.</p> <p>Clojure CLI Example: If the <code>deps</code> target depends on <code>deps.edn</code> and the file has not changed since last run, the deps target will not run again.</p>"},{"location":"build-tool/make/#deps-target-depend-on-a-file","title":"deps target - depend on a file","text":"<p>The deps target would use Clojure CLI or Leiningen to download dependencies.</p> <p>Configuring the <code>deps</code> target to depend on <code>deps.edn</code> or <code>project.clj</code> file, then if the file has not changed the deps will not run again.</p> <p>A Clojure CLI example depends on the <code>deps.edn</code> file that defines all the library dependencies for the project, tools for testing and packaging the Clojure service.  The <code>-P</code> flag is the prepare option, a dry run that only downloads the dependencies for the given tasks.</p> <pre><code>deps: deps.edn  ## Prepare dependencies for test and dist targets\n    $(info --------- Download libraries to test and build the service ---------)\n    clojure -P -X:env/test:package/uberjar\n</code></pre> <p><code>:env/test</code> adds libraries to run Kaocha and libraries used to run unit tests.  <code>:package/uberjar</code> runs a tool that creates an uberjar.</p>"},{"location":"build-tool/make/#clean-target-hiding-command-failure","title":"Clean target - hiding command failure","text":"<p>The clean target should remove files and directories created by the build (compile) process, to ensure a consistent approach to building each time.</p> <p>On Linux / Unix systems files can be deleted with the <code>rm</code> command using <code>-r</code> for recursively deleting a directory and its contents. <code>-f</code> forces the deleting of files and directories, otherwise a prompt for confirmation of the delete may be shown.</p> <p><code>-</code> before a command  instructs <code>make</code> to ignore an error code, useful if the files to be deleted did not exist (i.e. the build failed part way through and not all files were created).</p> <pre><code># `-` before the command ignores any errors returned\nclean:\n    $(info --------- Clean Clojure classpath cache ---------)\n    - rm -rf ./.cpcache\n</code></pre>"},{"location":"build-tool/make/#megalinter-target-simplifying-a-command","title":"MegaLinter target - simplifying a command","text":"<p>The <code>lint</code> target is an example of how the <code>Makefile</code> simplifies the command line interface.</p> <p><code>lint:</code> target is used to call the MegaLinter runner, avoiding the need to remember the common options passed when calling MegaLinter command line tool, <code>mega-linter-runner</code></p> <p>The Java flavor of MegaLinter is less than 2Gb image (full MegaLinter image is 8Gb) and contains all the tools for a Clojure project.  The flavor can only be set via a command line option, so the make file ensures that option is always used and the full MegaLinter docker image is not downloaded by mistake.</p> <p>When MegaLinter is configured to generate reports (default), <code>lint-clean:</code> target is used to clean those reports.</p> <pre><code># Run MegaLinter with custom configuration\nlint:\n    $(info --------- MegaLinter Runner ---------)\n    mega-linter-runner --flavor java --env 'MEGALINTER_CONFIG=.github/linters/mega-linter.yml'\n\nlint-clean:\n    $(info --------- MegaLinter Clean Reports ---------)\n    - rm -rf ./megalinter-reports\n</code></pre>"},{"location":"build-tool/make/#enhancing-make-output","title":"Enhancing make output","text":"<p>The <code>info</code> message is used with each target to enhances the readability of the make output, especially when multiple targets and commands are involved, or if commands are generating excessive output to standard out.</p> <pre><code> test:\n    $(info --------- Runner for unit tests ---------)\n    ./bin/test\n</code></pre> <p></p>"},{"location":"build-tool/make/#avoiding-file-name-collisions","title":"Avoiding file name collisions","text":"<p>Although unlikely, if a filename in the root of a project has the same name as a <code>Makefile</code> target, it can be used instead of running the targets command</p> <p><code>.PHONY:</code> defines the names of targets in the <code>Makefile</code> to avoid name clashes</p> <pre><code>.PHONY: all lint deps test test-ci dist clean\n</code></pre> <p>Reference:  Makefile Tutorial: phony</p>"},{"location":"build-tool/make/#halt-on-failure","title":"Halt on failure","text":"<p><code>.DELETE_ON_ERROR:</code> halts any further commands if a command returns non-zero exit status.  Useful as short-circuit to stop tasks when further work is not valuable, e.g. if tests fail then it may not be valuable to build the Clojure project.</p> <pre><code>.DELETE_ON_ERROR:\nall: deps test-ci dist clean\n</code></pre> <p>Reference:  Makefile Tutorial:  delete_on_error</p>"},{"location":"build-tool/make/#references","title":"References","text":"<p> Makefile Tutorial by Example  practicalli/dotfiles Makefile</p>"},{"location":"build-tool/make/#summary","title":"Summary","text":"<p>A <code>Makefile</code> can simplify the command line interface for any task with a Clojure project (or any other language and tooling).</p> <p>Using the same target names across all projects reduces the cognitive load for driving any project.</p>"},{"location":"careers/","title":"Engineering Career Development","text":"<p>Practicalli general view of career development includes the following roles</p> <ul> <li>New engineer</li> <li>Experienced Engineer</li> <li>Seasoned Engineer</li> <li>Engineer team lead</li> <li>Architect</li> <li> Engineering Manager</li> </ul> <p>Draft section: sharing initial thoughts</p> <p>The Career development section is very new and the thoughs shared and structure of this section may greatly change.</p> <p>Ownership verses Responsibility</p> <p>Engineers have collective ownership of all source code and should feel encouraged to contribute to any aspect of the code (e.g. creating and reviewing Pull Requests)</p> <p>A team may have responsibility for specific components and services, ensuring their design goals are realised and maintained.  The team should guide other engineers in their use and contributions to the parts of source code they are responsible for.</p> <p>NOTE: From a legal perspective, the company you work for usually owns the code you write (explicitly defined in employment contracts).</p>"},{"location":"careers/engineer/","title":"Engineer role","text":""},{"location":"careers/engineering-manager/","title":"Engineering Manager role","text":"<p>Management roles are there to balance the needs of the organisation with the engineering teams ability to deliver</p> <p>The role naturally focuses on on the larger goals and challenges facing the engineering teams, supporting the engineers to solve specific challenges using their own skills and experiences (that is why they were hired)</p>"},{"location":"careers/engineering-manager/#meetings","title":"Meetings","text":"<p>one to one are regularly held with direct reports and those people the engineering manager reports to, ensuring there is good working relationship between the respective people.</p> <ul> <li> <p>direct reports focus on supporting that persons ability to thrive within the company and identify actions to help</p> </li> <li> <p>stakeholders to ensure their concerns are understood and they have a realistic view of current capability from engineering teams</p> </li> </ul> <p>Engineering team meetings may occasionally be joined to review how effective the format is to those involved, or should a wider issue be addressed (especially in retrospectives)</p> <ul> <li>standup can highlight issues with motivation and team cohesiveness</li> <li>retrospective can help identify action items that the engineering manager can actively support or help direct others to support</li> </ul> <p>Performance reviews supported by one-to-one meetings</p> <p>Where performance reviews are applicable, include the review as an aspect to raise with one-to-one meetings with direct reports throughout the year.</p> <p>The need to complete a review should be raised as early as possible in one-to-one meetings and regularly raised (even if only briefly) to ensure supporting tasks are carried out to swiftly complete the review at the appropriate time.</p> <p>Creating regular journal entries can speed up the completion of a self-written performance review submission</p>"},{"location":"careers/engineering-manager/#journal","title":"Journal","text":"<p>Write regular journal entries and encourage others to do the same by sharing the journal across the organisation (except for private notes)</p> <p>Use the knowledge sharing tool for the organisation, e.g. Atlassian Confluence,  Notion,  Tettra, etc.</p> <p>TODO: example jornal, e.g. from Billie &amp; 100 days of code</p>"},{"location":"careers/engineering-manager/#management-anti-patters","title":"Management anti-patters","text":"<p>micro-management is highly ineffective, demonstrates lack of trust, highlights poor communication and lack of management skills.  Shows lack of understanding of the needs of the business and engineering teams.</p> <p>overt control suggests a lack of confidence, demoralises the team, incurs a lack of trust and engagement by team under control.  Drives inefficiency, stifles agency and creativity.</p> <p>emotional pain degrading and emotionally attacking a person has no place in a positive relationship.</p> <p>dominating by tearing people down to (allegedly) build them up in the (perceived) right way is an act of emotional torture and highly counter productive in the long term.</p>"},{"location":"change-management/","title":"Change Management","text":"<p>Change in any organization is very challenging and usually takes a lot of time or a lot of pain, i.e. evolution = time, revolution = pain (redundancies, people leaving, etc)</p> <p>Consider change in others as a potential side-effect of the work I do, rather than a specific goal I want to achieve.  Having change in others as a goal can contain much frustration and disappointment.</p> <p>Discovering what really motivates people is an interesting challenge in itself but a useful tool to create a meaningful basis for communication.</p> <p>For an example, if introducing Clojure into a company you first need to identify what is painful or highly valuable to people.  Then talk about removing that pain or adding extra value when referring to Clojure.</p> <p>I see a lot of people get excited about a technology for themselves, but they forget they have to help other people see how it benefits them if they want their adoption.</p>"},{"location":"code-quality/","title":"Code Quality","text":"<p>Tools can provide a base level of code quality and consistency when developing locally.</p> <p>Those same tools can check important quality rules when sharing code via continuous integration workflows, focusing the scope of code reviews on the more valuable aspect of design decisions</p> <ul> <li>Lint tools identify syntax and formatting issues for a specific programming language or configuration file</li> <li>Format tools ensures code follows a common set of format and indent rules</li> </ul> <p>The development team should agree on the rules that are valueable for these tools to apply, including those recommended by the community around the programming language or configuration file specification.</p>"},{"location":"code-quality/#recommended-tools","title":"Recommended Tools","text":"<p>MegaLinter provides an extensive range of lint tools all in one, using docker images to remove tool installation requirements.</p> <p>MegaLinter - extensive collection of lint tools</p> <p>Clojure Quality Tools</p>"},{"location":"code-quality/clojure/","title":"Clojure Quality tools","text":""},{"location":"code-quality/clojure/#syntax-idiom-check","title":"Syntax &amp; Idiom check","text":"<p>clj-kondo is a lint tool that highlights syntactic errors and suggests idioms for Clojure, ClojureScript and EDN.</p> <p>Use clj-kondo with your preferred editor to warning about errors as you type so issues can be fixed as soon as they occur, enhancing your joy of Clojure.</p> <p>clj-kondo can also be used as a command line tool for checking projects in development environments and continuous integration service, such as the setup-clojure GitHub action.</p> <p>Clojure LSP includes clj-kondo</p> <p>Clojure LSP install includes clj-kondo, removing the need for a separate install of clj-kondo</p> <p>clj-kondo install guide</p> <p>Clj-kondo config contains additional configuration for using clj-kondo with libraries that extend the Clojure language via macros.</p>"},{"location":"code-quality/clojure/#command-line","title":"Command Line","text":"<p>Run <code>clj-kondo</code> with the --lint option and specify a file or path</p> <p>To analyse a specific file</p> <pre><code>clj-kondo --lint ~/.config/deps.edn\n</code></pre> <p>Analyse a project, running the clj-kondo command from the root of the project</p> <pre><code>clj-kondo --lint .\n</code></pre>"},{"location":"code-quality/clojure/#github-workflow","title":"GitHub workflow","text":"<p>Add clj-kondo linting to continuous integration workflow checks for syntax errors with Clojure files.</p> <p>Setup Clojure Clojure Lint &amp; Reviewdog Setup clj-kondo</p> <p>Setup Clojure with clj-kondo and cljstyle checks</p> <pre><code>---\nname: \"Quality Checks\"\n\non:\n  pull_request:\n  push:\n    branches:\n      - live\n\njobs:\n  tests:\n    name: \"Check Code Quality\"\n    runs-on: ubuntu-latest\n    steps:\n      - run: echo \"\ud83d\ude80 Job automatically triggered by ${{ github.event_name }}\"\n      - run: echo \"\ud83d\udc27 Job running on ${{ runner.os }} server\"\n      - run: echo \"\ud83d\udc19 Using ${{ github.ref }} branch from ${{ github.repository }} repository\"\n\n      - name: \"Checkout code\"\n        uses: actions/checkout@v3.3.0\n      - run: echo \"\ud83d\udc19 ${{ github.repository }} repository was cloned to the runner.\"\n\n      - name: \"Install tools\"\n        uses: DeLaGuardo/setup-clojure@10.1\n        with:\n          cljstyle: 0.15.0\n          clj-kondo: 2022.11.02\n\n      - name: \"Lint Clojure\"\n        run: clj-kondo --lint deps.edn --config '{:output {:pattern \"::{{level}} file={{filename}},line={{row}},col={{col}}::{{message}}\"}}'\n\n      - name: \"Check Clojure Style\"\n        run: cljstyle check --report\n\n      - run: echo \"\ud83c\udfa8 style and format of Clojure code checked\"\n\n      - run: echo \"\ud83c\udf4f Job status is ${{ job.status }}.\"\n</code></pre> Setup clojure-lsp Action <p>Setup Clojure-LSP contains clj-kondo so can also be used within a continuous integration workflow as well as provide .</p>"},{"location":"code-quality/clojure/#format-check-and-fix","title":"Format Check and Fix","text":"<p>Code is easier to read and work with when it is consistent format that follows common rules.</p> <p>Clojure community style guide provides a common style for Clojure code.  While most style recommendations are widely used, others are more contentious.  Ultimately the development team for the project should define a workable set of style rules that makes them productions, ideally using much of those rules from the style guide.</p> <p>A consistent format between editors also minimises version control changes not related to code design.  The following format tools for clojure can all be configured to be consistent with each other (although zprint defaults will require more customisation):</p> <ul> <li>cljfmt - library, also included in Clojure LSP</li> <li>cljstyle - binary and library (re-write of cljfmt)</li> <li>zprint - binary &amp; library</li> </ul> Tooling that uses the Clojure Style Guide <p>Emacs <code>clojure-mode</code> and Clojure LSP (via cljfmt) format code following the most common Clojure style guide rules, although cljfmt rules are quite strick so Practicalli disables many of them.</p> <p>cljstyle default configuration follows the majority of styles and has the same defaults as cljfmt. Practicalli Clojure CLI Config tweaks a few rules to make code more readable and allow for repl design experiments.</p>"},{"location":"code-quality/clojure/#cljstyle","title":"cljstyle","text":"<p>Cljstyle is a rewrite of cljfmt, designed to be easier to configure. The default rules implement many of the style rules from the Clojure community style guide and is compatible with cljfmt.</p> <p>Call with the <code>check</code> option to report formatting issues, providing a coloured diff view of the format changes</p> <p> </p> <p>Call with <code>fix</code> option to automatically update all Clojure files with fixes, indicating which files have changed.</p> <p>Cljstyle will examine all files in the current directory and any sub-directories.</p> <p><code>.cljstyle</code> configuration file in the root of the project can override the default customisation, including indentation rules.</p> <p>cljstyle config used by Practicalli</p> <p>Clojure App template repository contains the <code>.cljstyle</code> configuration file used for all Practicalli projects</p> BinaryPracticalli Clojure CLI ConfigMakefile <p>Install the latest binary release from the cljstyle GitHub repository onto the operating system path, e.g. <code>$HOME/.local/bin</code></p> <pre><code>cljstyle check\n</code></pre> <p><code>fix</code> option automatically updates all source code files that have format issues.</p> <pre><code>cljstyle fix\n</code></pre> <p>cljstyle can be used as a library without installing the cljstyle binary.  Practicalli Clojure CLI Config defines the <code>:format/cljstyle</code> alias which should be passed wither the <code>check</code> or <code>format</code> option</p> <p>Check all the Clojure files (.clj .cljc .edn .cljs) in the current project <pre><code>clojure -M:format/cljstyle\n</code></pre></p> <pre><code>clojure -M:format/cljstyle!\n</code></pre> <p>Clojure Alias for cljstyle</p> <pre><code>:format/cljstyle\n{:extra-deps\n {mvxcvi/cljstyle {:git/url \"https://github.com/greglook/cljstyle.git\"\n                   :git/sha \"14c18e5b593c39bc59f10df1b894c31a0020dc49\"}}\n :main-opts [\"-m\" \"cljstyle.main\" \"check\"]}\n\n:format/cljstyle!\n{:extra-deps\n {mvxcvi/cljstyle {:git/url \"https://github.com/greglook/cljstyle.git\"\n                   :git/sha \"14c18e5b593c39bc59f10df1b894c31a0020dc49\"}}\n :main-opts [\"-m\" \"cljstyle.main\" \"fix\"]}\n</code></pre> <p>Use a Makefile to run common commands such as checking style, running tests, building uberjars, etc.</p> <p>Practicalli Clojure App template repository contains an example Makefile that contains common tasks for Clojure development</p> <p>This example calls the cljstyle binary, but could be changed to call the <code>clojure -M:format/cljstyle check</code> and <code>clojure -M:format/cljstyle fix</code> aliases instead.</p> <pre><code># ------- Code Quality --------------- #\nformat-check: ## Run cljstyle to check the formatting of Clojure code\n    $(info --------- cljstyle Runner ---------)\n    cljstyle check\n\nformat-fix:  ## Run cljstyle and fix the formatting of Clojure code\n    $(info --------- cljstyle Runner ---------)\n    cljstyle fix\n# ------------------------------------ #\n</code></pre> <p>Stage changes before automatically fixing format</p> <p>Practicalli suggests staging (or committing) changes before running <code>cljstyle fix</code> to easily undo undesired changes or simply confirm what changes have been made</p> <p>Practicall configuration</p> <p>Practicalli updated the default cljstyle configuration with the following changes</p> <p>Configure list indent to one character</p> .cljstyle<pre><code>  :indentation\n  {:enabled? true,\n   :list-indent 1,\n\n  }\n</code></pre> <p>Do not warn about duplicate var names (def, defn names) - excluded to stop warning about REPL experiments and design journal rich comments that contain alternative designs.</p> .cljstyle<pre><code>  :vars\n  {:enabled? false}\n</code></pre>"},{"location":"code-quality/clojure/#cljfmt","title":"cljfmt","text":"<p>cljfmt is not available as a separate binary, although it a fixed part of the Clojure LSP server implementation.</p> <p>whist typing Clojure code, Clojure LSP will format using cljfmt rules</p> <p>Define a cljfmt configuration via Clojure LSP to define rules and indentation settings for all projects.</p> .config/clojure-lsp/config.edn<pre><code> :cljfmt-config-path \"cljfmt.edn\"\n</code></pre> <p>Or specify cljfmt configuration within the Clojure LSP configuration file</p> .config/clojure-lsp/config.edn<pre><code> :cljfmt {}\n</code></pre> Practicalli Clojure LSP config - LSP and cljfmt <p>Practicalli Clojure LSP config provides an example config.edn configuration file for Clojure LSP that uses a cljfmt.edn configuration file for a minimum set of Clojure format rules</p> <p>The default cljfmt rules feel overly strict and Practicalli configuration disables the more draconian rules to make code far more readable</p>"},{"location":"code-quality/clojure/#zprint","title":"zprint","text":"<p>zprint is a highly configurable format tool for both Clojure code and Clojure/EDN structures, available as a library and command line tool</p> <p>zprint has advanced features over cljstyle and cljfmt, although may require some additional configuration work especially to format consistently with these tools.</p> <p>zprint available styles</p> No built-in diff option <p>zprint requires an external diff tool to see the format changes made, as zprint only reports on the files changed and not the content of those files that has changed.</p> <p>zprint can write changes to a new file and a file comparison made.  Or files can be staged / committed in a local Git repository before running zprint and a Git client used to see the diff.</p> <p>Once the desirable styles and configuration are established there is less need for an external diff tool, although its always useful to have a quick way to check what format tools are doing.</p> BinaryPracticalli Clojure CLI ConfigNode.js <p>Download zprint for Linux or MacOSX using the latest binary released on the GitHub repository</p> <p>Move the binary to the executable path for the operating system, updating the name to <code>zprint</code> (or use a symbolic link)</p> <p><pre><code>mv ~/Downloads/zprintl-1.2.5 ~/.local/bin/zprint\n</code></pre> Make the binary executable <pre><code>chmod a+x ~/.local/bin/zprint\n</code></pre> Ensure the zprint binary is working and examine the default configuration for zprint, including all default values and highlighting where non-default values are set <pre><code>zprint --explain-all\n</code></pre> Using zprint to check the Clojure files in the current directory and list which files require formatting <pre><code>zprint --formatted-check *.clj\n</code></pre></p> <p>A more detailed zprint report checking all the Clojure files a project, including files in the route directory and all sub-directories (i.e. <code>**/*.cjl</code> pattern) <pre><code>zprint --list-formatted-summary-check **/*.clj **/*.edn\n</code></pre> Or using short form flags <pre><code>zprint -lfsc **/*.clj **/*.edn\n</code></pre></p> <p>Update formatting for all the files in a projects, showing details of the files processed and changed <pre><code>zprint -lfsw **/*.clj *.edn *.clj\n</code></pre></p> <p>zprint can be used as a library without installing the binary.  Practicalli Clojure CLI Config defines the <code>:format/zprint</code> alias which checks the format of a file and reports which files required</p> <pre><code>clojure -M:format/zprint deps.edn\n</code></pre> <pre><code>clojure -M:format/zprint filename\n</code></pre> Clojure Alias for zprint <p>Add <code>:format/zprint</code> alias to check format and <code>:format/zprint!</code> to write format changes to a given file or filename pattern User or project deps.edn file<pre><code>:format/zprint\n{:extra-deps {zprint/zprint {:mvn/version \"1.2.4\"}}\n :main-opts  [\"-m\" \"zprint.main\"\n              \"{:style :indent-only}\"\n              \"--list-formatted-summary-check\"]}\n\n:format/zprint!\n{:extra-deps {zprint/zprint {:mvn/version \"1.2.4\"}}\n :main-opts  [\"-m\" \"zprint.main\"\n              \"{:style :indent-only}\"\n              \"--list-formatted-summary-write\"]}\n</code></pre> Use the alise</p> <p>zprint is available as an NPM package <pre><code>sudo --install --global zprint-clj\n</code></pre> Run zprint-clj over all Clojure files <pre><code>zprint-clj **/*.{clj,cljs,cljc,edn}\n</code></pre></p> <p>Configure zprint</p> <p>It is assumed that the majority of format needs are met by one of the following style rule sets</p> <ul> <li><code>{:style :indent-only}</code> only formats indentation, less likely to change the general style of code</li> <li><code>{:style :community}</code> a quite strict adhearence to the Clojure Community Guide (which Practicalli finds a little to strict)</li> </ul> <p>Unless the code is really messy (e.g. not written in a clojure aware editor with live linting) then <code>{:style :indent-only}</code> is a simple starting point.</p> <p>If the team have adopted most if not all community styles, then <code>{:style :community}</code> may be a more appropriate start point.  Use --explain-all flag with the zprint command to see all the rules that are applied with a partiular style and modify as appropriate</p> <p><code>$HOME/.zprintrc</code> is used for the configuration applied to all files, although this can be overridden in each project (or even as zprint comments in particular files)</p> <p>zprint - GitHub repo zprint - clojars zprint - cljdoc</p>"},{"location":"code-quality/megalinter/","title":"MegaLinter","text":"<p> MegaLinter verifies code and configuration with a a wide range of Lint and formatting tools, all from within a Docker image that minimises the install requirements.</p> <p>MegaLinter is an Open-Source tool for CI/CD workflows that analyses the consistency of your code, IAC, configuration, and scripts in your  repository sources, to ensure all your projects sources are clean and formatted whatever IDE/toolbox is used.</p> <p>The declarative MegaLinter configuration is relatively easy to learn. Typically time is spent tweaking the features (tools) of the  supported Linters</p> <p>Supported Linters have their own page listing the lint and format tools it provides, e.g.  Clojure. Each tool has its own page listing the configuration options and links to the tool homepage, e.g.  clj-kondo.</p> <p>Requirements</p> <p> Node.js is required to install and run the <code>mega-linter-runner</code> tool. Docker (Docker Desktop) is required to run MegaLinter containers locally, optionally with</p> <p>MegaLinter run via a GitHub workflow has no requirements</p>"},{"location":"code-quality/megalinter/#megalinter-locally","title":"MegaLinter Locally","text":"<p>The most effective way to manage lint and format issues is to run MegaLinter locally, before pushing changes to a Continuous Integration service.</p> <p>The MegaLinter runner uses a MegaLinter docker image will all lint tools pre-installed and configured, minimising the need to install tools locally.</p> <p>Install a recent version of  Node.js, version 18 (Long Term Support) is recommended.</p> <p><code>npx mega-linter-runner</code> runs MegaLinter without the need for a specific npm install.</p>"},{"location":"code-quality/megalinter/#create-a-configuration","title":"Create a configuration","text":"<p>Use the Practicalli MegaLinter Configuration</p> <p>Practicalli MegaLinter configuration configures tools relevant to Clojure development. Practicalli Project Templates also include Megalinter configuration and GitHub workflow.</p> <p>Create a configuration for a specific project using the MegaLinter runner command in the root of the project directory</p> <pre><code>npx mega-linter-runner\n</code></pre> <p><code>.mega-linter.yml</code> file is created at the root of the project. Review the file and ensure it has the Linters required.  MegaLinter Configuration describes the purpose of each variable.</p>"},{"location":"code-quality/megalinter/#run-megalinter","title":"Run MegaLinter","text":"<p>Use a specific flavour of MegaLinter to reduce the size of the Docker image used. A flavor is a docker image configuration for a specific programming language and its tooling.</p> <ul> <li><code>--flavor java</code> for Clojure projects</li> <li><code>--flavor documentation</code> for markdown / documentation only projects</li> </ul> <p>Other useful options include</p> <ul> <li><code>--env \"'MEGALINTER_CONFIG=.github/config/megalinter.yaml'\"</code> to pass a specific MegaLinter configuration file</li> <li><code>--remove-container</code> to remove the docker container once MegaLinter run has completed (the MegaLinter image will still be available)</li> </ul> <p>Run the MegaLinter runner with these options</p> <pre><code>npx mega-linter-runner --flavor java --env \"'MEGALINTER_CONFIG=.github/config/megalinter.yaml'\" --remove-container\n</code></pre> <p>The initial run will be slow as the MegaLinter docker image downloads and its overlays are cached.  All following runs will be significantly faster.</p> <p>make lint</p> <p><code>make lint</code> will run the MegaLinter runner using the above options, using the <code>lint</code> task from Practicalli Makefile</p>"},{"location":"code-quality/megalinter/#automatically-fix-issues","title":"Automatically fix issues","text":"<p>Use the <code>--fix</code> option with the local MegaLinter runner to automatically apply lint and format fixes that MegaLinter discovers.</p> Stage or Commit changes before MegaLinter runner automatic fix <p>If code and configuration changes are staged or committed before running with <code>--fix</code> then automatic fixes can easily be discarded or treated as a separate commit using any Git tool.</p> <pre><code>npx mega-linter-runner --fix --flavor java --env \"'MEGALINTER_CONFIG=.github/config/megalinter.yaml'\" --remove-container\n</code></pre>"},{"location":"code-quality/megalinter/#make-tasks","title":"Make tasks","text":"<p>Add a <code>lint</code> task to the project Makefile and run <code>make lint</code> to call the <code>mega-linter-runner</code>. Especially useful when using a different <code>--flavor</code> or using an alternative location for the configuration file, so the same approach is used each time.</p> <p>Makefile tasks for MegaLinter Runner</p> <pre><code>lint:\n $(info --------- MegaLinter Runner ---------)\n    npx mega-linter-runner --flavor java --env \"'MEGALINTER_CONFIG=.github/config/megalinter.yaml'\" --remove-container\n\nlint-fix:\n $(info --------- MegaLinter Runner ---------)\n    npx mega-linter-runner --fix --flavor java --env \"'MEGALINTER_CONFIG=.github/config/megalinter.yaml'\" --remove-container\n</code></pre> <p>Options for MegaLinter Runner</p> <p>MegaLinter Runner options</p>"},{"location":"code-quality/megalinter/#megalinter-configuration","title":"MegaLinter Configuration","text":"<p>Create a configuration file to use with the local <code>mega-linter-runner</code> and GitHub workflow</p> <p><code>.github/config/megalinter.yaml</code> is a recommeded location for the MegaLinter configuration file, especially when using GitHub workflow.</p> Practicalli MegaLinter Configuration <pre><code>---\n# Configuration file for MegaLinter\n#\n# General configuration:\n# &lt;https://oxsecurity.github.io/megalinter/configuration/&gt;\n#\n# Specific Linters:\n# &lt;https://oxsecurity.github.io/megalinter/latest/supported-linters/&gt;\n\n# ------------------------\n# Linters\n\n# Run linters in parallel\nPARALLEL: true\n\n# ENABLE specific linters, all other linters automatically disabled\nENABLE:\n  - CLOJURE\n  - CREDENTIALS\n  - DOCKERFILE\n  - MAKEFILE\n  - MARKDOWN\n  - GIT\n  - SPELL\n  - YAML\n  - REPOSITORY\n\n# Linter specific configuration\n\nCLOJURE_CLJ_KONDO_CONFIG_FILE: \".github/config/clj-kondo-ci-config.edn\"\n# CLOJURE_CLJ_KONDO_ARGUMENTS: \"--lint deps.edn\"\n# CLOJURE_CLJ_KONDO_FILTER_REGEX_EXCLUDE: \"dev|develop\"\nCLOJURE_CLJ_KONDO_FILTER_REGEX_EXCLUDE: \"resources\"\n\n# CREDENTIALS_SECRETLINT_DISABLE_ERRORS: true\nCREDENTIALS_SECRETLINT_CONFIG_FILE: \".github/config/secretlintrc.json\"\n\nMARKDOWN_MARKDOWNLINT_CONFIG_FILE: \".github/config/markdown-lint.jsonc\"\nMARKDOWN_MARKDOWNLINT_FILTER_REGEX_EXCLUDE: \".github/pull_request_template.md|CHANGELOG.md\"\n# MARKDOWN_MARKDOWNLINT_DISABLE_ERRORS: false\nMARKDOWN_MARKDOWN_LINK_CHECK_CONFIG_FILE: \".github/config/markdown-link-check.json\"\n# MARKDOWN_MARKDOWN_LINK_CHECK_CLI_LINT_MODE: \"project\"\n# MARKDOWN_MARKDOWN_LINK_CHECK_DISABLE_ERRORS: false\nMARKDOWN_REMARK_LINT_DISABLE_ERRORS: true\n# MARKDOWN_MARKDOWN_TABLE_FORMATTER_DISABLE_ERRORS: false\n\n# SPELL_CSPELL_DISABLE_ERRORS: true\nSPELL_MISSPELL_DISABLE_ERRORS: true\n\n# YAML_PRETTIER_FILTER_REGEX_EXCLUDE: (docs/)\n# YAML_YAMLLINT_FILTER_REGEX_EXCLUDE: (docs/)\n\n# Explicitly disable linters to ensure they are never run\n# DISABLE:\n#   - COPYPASTE # checks for excessive copy-pastes\n#   - SPELL # spell checking - often creates many false positives\n#   - CSS #\n\n# Disable linter features\nDISABLE_LINTERS:\n  - YAML_PRETTIER # draconian format rules\n  - SPELL_CSPELL # many clojure references causing false positives\n  - YAML_YAMLLINT # vague error mesages, investigation required\n  - REPOSITORY_GIT_DIFF # warnings about LF to CRLF\n  - REPOSITORY_SECRETLINT # reporting errors in its own config file\n  # - REPOSITORY_DEVSKIM # unnecessary URL TLS checks\n  - REPOSITORY_CHECKOV # fails on root user in Dockerfile\n  - REPOSITORY_SECRETLINT\n\n# Ignore all errors and return without error status\n# DISABLE_ERRORS: true\n\n# ------------------------\n\n# ------------------------\n# Reporting\n\n# Activate sources reporter\nUPDATED_SOURCES_REPORTER: false\n\n# Show Linter timings in summary table at end of run\nSHOW_ELAPSED_TIME: true\n\n# Upload reports to file.io\nFILEIO_REPORTER: false\n# ------------------------\n\n# ------------------------\n# Over-ride errors\n\n# detect errors but do not block CI passing\n# DISABLE_ERRORS: true\n# ------------------------\n</code></pre>"},{"location":"code-quality/megalinter/#optomise-run","title":"Optomise Run","text":"<p>Run the linters in paralell to speed up the overall MegaLinter run</p> <pre><code>PARALLEL: true\n</code></pre>"},{"location":"code-quality/megalinter/#linter-groups","title":"Linter groups","text":"<p>Enable the linter groups to run. Specific linters within these groups can be enabled/disabled using Linter specific variables</p> <pre><code>ENABLE:\n  - CLOJURE\n  - CREDENTIALS\n  - DOCKERFILE\n  - MAKEFILE\n  - MARKDOWN\n  - GIT\n  - SPELL\n  - YAML\n  - REPOSITORY\n</code></pre>"},{"location":"code-quality/megalinter/#linter-configuration","title":"Linter configuration","text":"<p>Configure the Clojure lint tools, which currently includes clj-kondo</p> <ul> <li><code>CLOJURE_CLJ_KONDO_CONFIG_FILE</code> to define clj-kondo configuration and lint rules to use</li> <li><code>CLOJURE_CLJ_KONDO_ARGUMENTS</code> pass arguments such as <code>--lint</code> to define a path to check rather than the whole project</li> <li><code>CLOJURE_CLJ_KONDO_FILTER_REGEX_EXCLUDE</code> to exclude one or more files or directories</li> </ul> <p>Example:</p> <pre><code>CLOJURE_CLJ_KONDO_CONFIG_FILE: \".github/config/clj-kondo-ci-config.edn\"\nCLOJURE_CLJ_KONDO_ARGUMENTS: \"--lint deps.edn\"\nCLOJURE_CLJ_KONDO_FILTER_REGEX_EXCLUDE: \"dev|develop\"\n</code></pre> <p>Explicitly disable linter tools to ensure they are never run</p> <pre><code>DISABLE_LINTERS:\n  - YAML_PRETTIER # draconian format rules\n  - SPELL_CSPELL # many clojure references causing false positives\n  - YAML_YAMLLINT # vague error mesages, investigation required\n  - REPOSITORY_GIT_DIFF # warnings about LF to CRLF\n  - REPOSITORY_SECRETLINT # reporting errors in its own config file\n  # - REPOSITORY_DEVSKIM # unnecessary URL TLS checks\n  - REPOSITORY_CHECKOV # fails on root user in Dockerfile\n  - REPOSITORY_SECRETLINT\n</code></pre> <p>Disable all errors if there are configuration issues that need troubleshooting</p> <pre><code># DISABLE_ERRORS: true\n</code></pre>"},{"location":"code-quality/megalinter/#reporting","title":"Reporting","text":"<p>Show Linter timings in summary table at end of run</p> <pre><code>SHOW_ELAPSED_TIME: true\n</code></pre> <p>Upload reports to file.io</p> <pre><code>FILEIO_REPORTER: false\n</code></pre>"},{"location":"code-quality/megalinter/#github-workflow","title":"GitHub Workflow","text":"<p> Practicalli MegaLinter GitHub Workflow</p> <p>Create a <code>.github/workflows/megalinter.yaml</code> workflow configuration, e.g  Practicalli MegaLinter Workflow</p>"},{"location":"continuous-integration/","title":"Continuous integration","text":"<p> Docker Containers &amp; tools</p>"},{"location":"continuous-integration/#hosted-ci-services","title":"Hosted CI Services","text":"<ul> <li>CircleCI</li> <li> GitHub Workflows and Actions</li> <li>GitLab CI</li> <li>Render.com</li> <li>fly.io</li> <li>Vercel - front-end focus</li> <li>Heroku - commercial only</li> </ul> <p>Practicalli uses Vercel to host scripts to update GitHub profiles, i.e.  practicalli-johnny</p>"},{"location":"continuous-integration/docker/","title":"Docker","text":"<p>Docker containers an isolated environment for repeatable build &amp; runtime systems.</p> <p> Dockerfile configuration defines a consistent approach to building the source code of project locally, or within a continuous integration (CI) system.</p> <p>Designing a  <code>Dockerfile</code> configuration to maximise use of image overlays (layers) greatly enhances the speed of the build, as libraries and other unchanged assets are cached on first run.</p> <p> Docker compose orchestrates multiple services locally. Heath checks and conditions can be set to ensure dependant services start in the correct order.</p> <p>Docker compose can use a  <code>Dockerfile</code> configuration to build a project when starting services. An <code>on-watch</code> feature can rebuild the project from sourcw when code changes are written to file.</p> Recommended Docker Images <ul> <li> alpine linux or  debian linux as the base operating system</li> <li>Clojure - official Docker Clojure image</li> <li>Eclipse Temurin - Offical Docker image for OpenJDK</li> </ul> <p> Detailed Docker Image Descriptions</p> Docker and Clojure projects <p>A Docker Compose workflow complements the Clojure REPL Driven Development workflow.  </p> <p>A REPL connected editor provides instant feedback on the code as it is written, with Docker Compose providing a consistent build and orchestration of services.</p> <p> Practicalli REPL Reloaded Workflow</p>"},{"location":"continuous-integration/docker/#general-workflow","title":"General Workflow","text":"<ul> <li> Install Docker Desktop &amp;  Extensions</li> <li> Select a Docker image as a base or builder stage</li> <li> Create a Dockerfile, e.g  a multi-stage build and run-time Dockerfile</li> <li> Compose services together, adding health checks and conditional starts</li> <li>(optional) Automatic rebuild of Clojure project when  watching for code changes (experimental feature)</li> </ul> <p>Try the Docker getting started tutorial</p> <p>Follow the Docker Getting Started tutorial from within Docker Desktop or via the command line. <pre><code>docker run -d -p 80:80 docker/getting-started\n</code></pre></p> Practicalli Project Templates includes docker configuration <p>Practicalli Project Templates create projects that include a multi-stage <code>Dockerfile</code>, <code>.dockerignore</code> and <code>compose.yaml</code> configurations for Clojure development.</p>"},{"location":"continuous-integration/docker/clojure-multi-stage-dockerfile/","title":"Multi-Stage Dockerfile for Clojure","text":"<p>Building a Clojure project from source to create a docker image provides an effective workflow for local testing and system integration.</p> <p>A multi-stage Dockerfile uses a separate build stage that separates tools and temporary artefacts from the final Docker image, producing a runtime image with minimal size and resources used.</p> Practicalli Project Templates provide a Multi-stage Dockerfile <p> Practialli Project Templates provides <code>Dockerfile</code>, <code>.dockerignore</code> and <code>compose.yaml</code> configurations to optimise the build and run of Clojure projects. <pre><code>clojure -T:project/create :template practicalli/service :name practicalli/gameboard\n</code></pre> Practicalli Project Templates are included in the  Practicalli Clojure CLI Config which provides aliases for running community tools to support a wide range of development tasks.</p> Java 21 Long Term Support release recommended <p>This guide uses Docker images with Java 21, the latest Long Term Support release (LTS) as of October 2023.</p> <p>Examples will be provided using Alpine Linux and Debian Linux operating system.</p>"},{"location":"continuous-integration/docker/clojure-multi-stage-dockerfile/#builder-stage","title":"Builder stage","text":"<p>The builder stage packages the Clojure service into an uberjar containing the Clojure standard library and the build of the Clojure service.</p> <p>Practicalli uses the Official Clojure Docker image with Clojure CLI as the build environment.</p> Clojure Docker image uses OpenJDK Eclipse Temuring image <p>OpenJDK   Eclipse Temurin is used as the base for all Clojure Official docker images.</p> <p>Practicalli recommends using the same OpenJDK Long Term Support image tag and Operating system for the Clojure and Eclipse Termurin images.</p> Alpine LinuxDebian Linux <p> Alpine Linux image variants are used to keep the image file size as small as possible, reducing local resource requirements (and image download time).</p> <p>Set builder stage image using specific Java version</p> Dockerfile<pre><code>FROM clojure:temurin-21-tools-deps-alpine AS builder\n</code></pre> <p>clojure:temurin-21-tools-deps-alpine image details</p> <p><code>CLOJURE_VERSION</code> will over-ride the version of Clojure CLI in the Clojure image (which defaults to latest Clojure CLI release). Or choose an image that has a specific Clojure CLI version, e.g. <code>temurin-21-tools-deps-1.11.1.1413-alpine</code></p> <p>Set builder stage image with specific Clojure version</p> Dockerfile<pre><code>FROM clojure:temurin-21-alpine AS builder\nENV CLOJURE_VERSION=1.12.0.1479\n</code></pre> <p> Debian Linux bookworm is the latest stable image, using <code>slim</code> variant for a minimal set of packages</p> <p>Set builder stage image using specific Java version</p> Dockerfile<pre><code>FROM clojure:temurin-21-tools-deps-bookworm-slim AS builder\n</code></pre> <p>clojure:temurin-21-tools-deps-alpine image details</p> <p><code>CLOJURE_VERSION</code> will over-ride the version of Clojure CLI in the Clojure image (which defaults to latest Clojure CLI release). Or choose an image that has a specific Clojure CLI version, e.g. <code>temurin-17-tools-deps-1.11.1.1182-alpine</code></p> <p>Set builder stage image with specific Clojure version</p> Dockerfile<pre><code>FROM clojure:temurin-21-tools-deps-bookworm-slim AS builder\nENV CLOJURE_VERSION=1.12.0.1479\n</code></pre> Clojure CLI release history <p>Clojure.org Tool Releases page shows the current release and history of each released version.</p> <p>Create directory for building the project code and set it as the working directory within the Docker container to give RUN commands a path to execute from.</p> <p>Create working directory for build</p> Dockerfile<pre><code>RUN mkdir -p /build\nWORKDIR /build\n</code></pre> Clojure CLI and tools.build <p> Practicalli Clojure - Tools.Build is the recommended approach to creating an Uberjar from a Clojure CLI project.</p> <p>As long as the required files are copied, any build tool and task can be used to create the Uberjar file that packages the Clojure service for deployment.</p>"},{"location":"continuous-integration/docker/clojure-multi-stage-dockerfile/#cache-dependencies","title":"Cache Dependencies","text":"<p>Clojure CLI is used to download library dependencies for the project and any other tooling used during the build stage, e.g. test runners, packaging tools to create an uberjar.</p> <p>Once a library has been downloaded it becomes part of the cached layer so the download should only occur once.</p> <p>If the <code>deps.edn</code> file changes then the layer will run again and update the cache if changes to the library dependencies were made.</p> MakefileClojure CLIBabashka Task Runner <p> Practicalli Makefile includes a <code>deps</code> task that downloads all the library dependency for a project.</p> <p>The <code>deps</code> Makefile target uses the <code>clojure -P</code> command to download the dependencies without running any other Clojure code or tools.</p> Makefile deps target using Clojure CLI <pre><code> deps: deps.edn  ## Prepare dependencies for test and dist targets\n    $(info --------- Download test and service libraries ---------)\n    clojure -P -X:build\n</code></pre> <p> Practicalli Makefile tasks</p> <p>Copy the essential build configuration files:</p> <ul> <li><code>Makefile</code> which defines the <code>deps</code> target</li> <li><code>deps.edn</code> file which includes the project dependencies and a <code>:project/build</code> alias that includes the tools.build library.</li> <li><code>build.clj</code> script to define built tasks using <code>tools.build</code></li> </ul> <p>Create dependency cache overlay</p> Dockerfile<pre><code>COPY Makefile deps.edn build.clj /build/\nRUN make deps\n</code></pre> <p>The dependencies are cached in the Docker overlay (layer) and this cache will be used on successive docker builds unless the <code>deps.edn</code> file or <code>Makefile</code> is change.</p> <p>Copy the <code>deps.edn</code> file to the build stage and use the <code>clojure -P</code> prepare (dry run) command to download the dependencies without running any Clojure code or tools.</p> <p>Create dependency cache overlay</p> Dockerfile<pre><code>COPY deps.edn build.clj /build/\nRUN clojure -P -X:build\n</code></pre> <p>The dependencies are cached in the Docker overlay (layer) and this cache will be used on successive docker builds unless the <code>deps.edn</code> file is change.</p> <p>Pull request welcome </p> <p><code>deps.edn</code> in this example contains the project dependencies and <code>:build</code> alias used build the Uberjar.</p>"},{"location":"continuous-integration/docker/clojure-multi-stage-dockerfile/#build-uberjar","title":"Build Uberjar","text":"<p>Copy all the project files to the docker builder working directory, creating another overlay.</p> <p>Copying the src and other files in a separate overlay to the <code>deps.edn</code> file ensures that changes to the Clojure code or configuration files does not trigger downloading of the dependencies again.</p> MakefileClojure CLIBabashka Task Runner <p>Run the <code>dist</code> task to generate an Uberjar for distribution.</p> <p>Use tools.build to generate an Uberjar</p> Dockerfile<pre><code>COPY ./ /build\nRUN make dist\n</code></pre> <p>Run the <code>tools.build</code> command to generate an Uberjar.</p> <p>Use tools.build to generate an Uberjar</p> Dockerfile<pre><code>COPY ./ /build\nRUN clojure -T:project/build uberjar\n</code></pre> <p><code>:project/build</code> is an alias to include Clojure tools.build dependencies which is used to build the Clojure project into an Uberjar.</p> <p>Pull request welcome</p>"},{"location":"continuous-integration/docker/clojure-multi-stage-dockerfile/#docker-ignore-patterns","title":"Docker Ignore patterns","text":"<p><code>.dockerignore</code> file in the root of the project defines file and directory patterns that Docker will ignore with the COPY command.  Use <code>.dockerignore</code> to avoid copying files that are not required for the build</p> <p>Keep the <code>.dockerignore</code> file simple by excluding all files with <code>*</code> pattern and then use the <code>!</code> character to explicitly add files and directories that should be copied</p> MakefileClojure CLIBabashka Task Runner <p>Docker ignore - only include specific patterns</p> <pre><code># Ignore all files\n*\n\n# Include Clojure code and config\n!deps.edn\n!build.clj\n!Makefile\n!src/\n!test/\n!test-data/\n!resources/\n</code></pre> <p>Docker ignore - only include specific patterns</p> <pre><code># Ignore all files\n*\n\n# Include Clojure code and config\n!deps.edn\n!src/\n!test/\n!test-data/\n!resources/\n</code></pre> <p>Pull request welcome</p> <p><code>test-data</code> directory is commonly used by Practicalli to include scripts and data for testing the system once running.</p> <p>The classic approach for Docker ignoer patters is to specify all files and directories to exclude in a Clojure project, although this can lead to more maintenance as the project grows.</p>"},{"location":"continuous-integration/docker/clojure-multi-stage-dockerfile/#run-time-stage","title":"Run-time stage","text":"Alpine LinuxDebian Linux <p>The Alpine Linux version of the Eclipse Temurin image is used as it is around 5Mb in size, compared to 60Mb or more of other operating system images.</p> <p>Image for run-time stage</p> Dockerfile<pre><code>FROM eclipse-temurin:17-alpine AS final\n</code></pre> <p>The Alpine Linux version of the Eclipse Temurin image is used as it is around 5Mb in size, compared to 60Mb or more of other operating system images.</p> <p>Image for run-time stage</p> Dockerfile<pre><code>FROM eclipse-temurin:21-bookworm-slim AS final\n</code></pre> <p>Run-time containers are often cached in a repository, e.g. AWS Container Repository (ECR).  <code>LABEL</code> adds metadata to the container helping it to be identified in a repository or in a local development environment.</p> <pre><code>LABEL org.opencontainers.image.authors=\"nospam+dockerfile@practicall.li\"\nLABEL io.github.practicalli.service=\"Gameboard API Service\"\nLABEL io.github.practicalli.team=\"Practicalli Engineering Team\"\nLABEL version=\"1.0\"\nLABEL description=\"Gameboard API service\"\n</code></pre> <p>Use <code>docker inspect</code> to view the metadata</p>"},{"location":"continuous-integration/docker/clojure-multi-stage-dockerfile/#additional-packages","title":"Additional Packages","text":"<p>Optionally, add packages to support running the service or helping to debug issue in the container when it is running.  For example, add <code>dumb-init</code> to manage processes, <code>curl</code> and <code>jq</code> binaries for manual running of system integration testing scripts for API testing.</p> Alpine LinuxDebian Linux <p><code>apk</code> is the package tool for Alpine Linux and <code>--no-cache</code> option ensures the install file is not part of the resulting image, saving resources.  Alpine Linux recommends setting versions to use any point release with the <code>~=</code> approximately equal version, so any same major.minor version of the package can be used.</p> <p>Add Alpine packages</p> Dockerfile<pre><code>RUN apk add --no-cache \\\n    dumb-init~=1.2.5 \\\n    curl~=8.0.1 \\\n    jq~=1.6\n</code></pre> <p>Check Alpine packages if new major versions are no longer available (low frequency)</p> <p><code>apt</code> is the Advanced Package Tool for Debian Linux and any Debian based distrobution like Ubuntu.</p> <p>Add Alpine packages</p> Dockerfile<pre><code>ENV DEBIAN_FRONTEND=noninteractive\nRUN apt update &amp;&amp; \\\n    apt install -y dumb-init curl jq &amp;&amp; \\\n    apt autoremove &amp;&amp;\n    rm -rf /var/lib/apt/lists/*\n</code></pre> <p>Debian Linux packages if new major versions are no longer available (low frequency)</p>"},{"location":"continuous-integration/docker/clojure-multi-stage-dockerfile/#non-root-group-and-user","title":"Non-root group and user","text":"<p>Docker runs as root user by default and if a container is compromised the root permissions and could lead to a compromised host system. Docker recommends creating a user and group in the run-time image  to run the service</p> Alpine LinuxDebian Linux <p>Create a non-privileged user account</p> <pre><code>ARG UID=10001\nRUN adduser \\\n    --disabled-password \\\n    --gecos \"\" \\\n    --home \"/nonexistent\" \\\n    --shell \"/sbin/nologin\" \\\n    --no-create-home \\\n    --uid \"${UID}\" \\\n    clojure\n</code></pre> <p>Create Non-root group and user</p> <pre><code>RUN addgroup -S practicalli &amp;&amp; adduser -S practicalli -G practicalli\n</code></pre> <p>Create directory to contain service</p> <p>Create directory to contain service archive, owned by non-root user <pre><code>RUN mkdir -p /service &amp;&amp; chown -R clojure. /service\n</code></pre></p> <p>Set user</p> <p>Instruct docker that all future commands should run as the appuser user <pre><code>USER clojure\n</code></pre></p>"},{"location":"continuous-integration/docker/clojure-multi-stage-dockerfile/#copy-uberjar-to-run-time-stage","title":"Copy Uberjar to run-time stage","text":"<p>Create a directory to run the service or use a known existing path that will not clash with any existing files from the image.</p> <p>Set the working directory and copy the uberjar archive file from Builder image</p> <p>Copy Uberjar from build stage</p> <pre><code>WORKDIR /service\nCOPY --from=builder --chown=clojure:clojure /build/practicalli-service.jar /service/practicalli-service.jar\n</code></pre> <p>Optionally, add system integration testing scripts to the run-time stage for testing from within the docker container.</p> <pre><code>RUN mkdir -p /service/test-scripts\nCOPY --from=builder --chown=clojure:clojure /build/test-scripts/curl--* /service/test-scripts/\n</code></pre>"},{"location":"continuous-integration/docker/clojure-multi-stage-dockerfile/#service-environment-variables","title":"Service Environment variables","text":"<p>Define values for environment variables should they be required (usually for debugging), ensuring no sensitive values are used. Environment variables are typically set by the service provisioning the containers (AWS ECS / Kubernettes) or on the local OS host during development (Docker Desktop).</p> <pre><code># optional over-rides for Integrant configuration\n# ENV HTTP_SERVER_PORT=\n# ENV MYSQL_DATABASE=\nENV SERVICE_PROFILE=prod\n</code></pre>"},{"location":"continuous-integration/docker/clojure-multi-stage-dockerfile/#java-virtual-machine-optimisation","title":"Java Virtual Machine Optimisation","text":"<p>Clojure Uberjar runs on the Java Virtual Machine which is a highly optimised environment that rarely needs adjusting, unless there are noticeable performance or resource issue use.</p> <p>Minimum and maximum heap sizes, i.e. <code>-XX:MinRAMPercentage</code> and <code>-XX:MaxRAMPercentage</code> are typically the only optimisations required.</p> <p><code>java -XshowSettings -version</code> displays VM settings (vm), Property settings (property), Locale settings (locale), Operating System Metrics (system) and the version of the JVM used.  Add the category name to show only a specific group of settings, e.g. <code>java -XshowSettings:system -version</code>.</p> <p><code>JDK_JAVA_OPTIONS</code> can be used to tailor the operation of the Java Virtual Machine, although the benefits and constraints of options should be well understood before using them (especially in production).</p> <p>Show settings on JVM startup</p> <p>Show system settings on startup, force container mode and set memory heap maximum to 85% of host memory size. <pre><code>ENV JDK_JAVA_OPTIONS \"-XshowSettings:system -XX:+UseContainerSupport -XX:MaxRAMPercentage=85\"\n</code></pre></p> Use relative heap memory settings, not fixed sizes <p>Relative heap memory settings (<code>-XX:MaxRAMPercentage</code>) should be used for containers rather than the fixed value options (<code>-Xmx</code>) as the provisioning service for the container may control and change the resources available to a container on deployment (especially a Kubernettes system).</p> <p>Options that are most relevant to running Clojure &amp; Java Virtual Machine in a container include:</p> <ul> <li><code>-XshowSettings:system</code> display (container) system resources on JVM startup</li> <li><code>-XX:InitialRAMPercentage</code> Percentage of real memory used for initial heap size</li> <li><code>-XX:MaxRAMPercentage</code> Maximum percentage of real memory used for maximum heap size</li> <li><code>-XX:MinRAMPercentage</code> Minimum percentage of real memory used for maximum heap size on systems with small physical memory</li> <li><code>-XX:ActiveProcessorCount</code> specifies the number of CPU cores the JVM should use regardless of container detection heuristics</li> <li><code>-XX:\u00b1UseContainerSupport</code> force JVM to run in container mode, disabling container detection (only useful if JVM not detecting container environment)</li> <li><code>-XX:+UseZGC</code> low latency Z Garbage collector (read the Z Garbage collector documentation and understand the trade-offs before use) - the default Hotspot garbage collector is the most effective choice for most services</li> </ul> Only optimise if performance test data shows issues <p>Without performance testing of a specific Clojure service and analysis of the results, let the JVM use its own heuristics to determine the most optimum strategies it should use</p>"},{"location":"continuous-integration/docker/clojure-multi-stage-dockerfile/#expose-clojure-service","title":"Expose Clojure service","text":"<p>If Clojure service listens to network requests when running, then the port it is listening on should be exposed so the world outside the container can communicate to the Clojure service.</p> <p>e.g. expose port of HTTP Server that runs the Clojure service</p> <p>Expose service port</p> <pre><code>EXPOSE 8080\n</code></pre>"},{"location":"continuous-integration/docker/clojure-multi-stage-dockerfile/#entrypoint","title":"Entrypoint","text":"<p>Finally define how to run the Clojure service in the container.  The <code>java</code> command is used with the <code>-jar</code> option to run the Clojure service from the Uberjar archive.</p> <p>The <code>java</code> command will use arguments defined in <code>JDK_JAVA_OPTIONS</code>.</p> <p>ENTRYPOINT directive defines the command to run the service</p> <pre><code>ENTRYPOINT [\"java\", \"-jar\", \"/service/practicalli-service.jar\"]\n</code></pre> Docker ENTRYPOINT directive <p><code>ENTRYPOINT</code> is the recommended way to run a service in Docker.  <code>CMD</code> can be used to pass additional arguments to the <code>ENTRYPOINT</code> command, or used instead of <code>ENTRYPOINT</code>.</p> <p><code>jshell</code>is the default <code>ENTRYPOINT</code> for the Eclipse Temurin image. <code>jshell</code> will run if a <code>CMD</code> directive is not included in the run-time stage of the <code>Dockerfile</code>.</p> <p>The <code>ENTRYPOINT</code> command runs as process id 1 (PID 1) inside the docker container.  In a Linux system PID 1 should respond to all TERM and SIGTERM signals.</p> <p>dump-init provides a simple process supervisor and init system, designed to run as PID 1 and manage all signals and child processes effectively.</p> <p>Use <code>dumb-init</code> as the <code>ENTRYPOINT</code> command and <code>CMD</code> to pass the java command to start the Clojure service as an argument.  <code>dumb-init</code> ensures <code>TERM</code> signals are sent to the Java process and all child processes are cleaned up on shutdown.</p> <p>ENTRYPOINT to cleanly manage service</p> <pre><code>ENTRYPOINT [\"/usr/bin/dumb-init\", \"--\"]\nCMD [\"java\", \"-jar\", \"/service/practicalli-service.jar\"]\n</code></pre> <p>Alternatively, run dumb-jump and java within the <code>ENTRYPOINT</code> directive, <code>ENTRYPOINT [\"/usr/bin/dumb-init\", \"--\", \"java\", \"-jar\", \"/service/practicalli-service.jar\"]</code>.  This approach cannot be overridden with an additional CMD directive on the command line when using <code>docker run</code>.</p>"},{"location":"continuous-integration/docker/clojure-multi-stage-dockerfile/#build-and-run","title":"Build and Run","text":"<p>Ensure docker services are running, i.e. start docker desktop.</p> <p>Build the service and create an image to run the Clojure service in a container with <code>docker build</code>.  Use a <code>--tag</code> to help identify the image and specify the context (in this example the root directory of the current project, <code>.</code>)</p> <pre><code>docker build --tag practicalli/service-name:1.1 .\n</code></pre> <p>After the first time building the docker image, any parts of the build that havent changed will use their respective cached layers in the builder stage.  This can lead to very fast, even zero time builds.</p> <p></p> <p>Maximise use of Docker cache</p> <p>Maximising the docker cache by careful consideration of command order and design in a <code>Dockerfile</code> can have a significant positive affect on build speed.</p> <p>Each command is an overlay (layer) in the Docker image and if its respective files have not changed, then the cached version of the command will be used.</p> <p>Run the built image in a docker container using <code>docker run</code>, publishing the port number so it can be used from the host (developer environment or deployed environment).  Use the name of the image created by the tag in the docker build command.</p> <pre><code>docker run --publish 8080:8080 practicalli/service-name\n</code></pre> <p>Orchestrate multiple services with Compose</p> <p>Create a <code>compose.yml</code> file to defines all services to run to support local integration testing, optionally adding health checks and conditional starts.</p> <p>Run <code>docker compose up</code> to start all the services.</p>"},{"location":"continuous-integration/docker/compose/","title":"Compose","text":"<p>Docker Compose provides a declarative configuration for orchestrating multiple services locally.</p> <p>Compose can build images for Clojure projects using a <code>Dockerfile</code> and conditionally run services based on the health check of other supporting services, e.g. Clojure service runs once Postgres Database service is healthy.</p> <p>Each service can define a heart beat used as a conditional startup for other services which depend upon them.</p> <p>Docker Compose Overview</p>"},{"location":"continuous-integration/docker/compose/#compose-configuration","title":"Compose Configuration","text":"<p><code>compose.yaml</code> is the configuration file for Docker Compose</p> <p><code>docker compose</code> command will start all services defined in the <code>compose.yaml</code> configuration (or <code>--file filename</code> to specify an alternative configuration file).</p> <pre><code>docker compose up\n</code></pre> <ul> <li><code>--build</code> option will run the build process for services that contain a <code>build:</code> configuration</li> <li><code>--detach</code> runs services in the background, compose startup logs are still shown in the foreground</li> </ul> <p>Shutdown the services with <code>down</code> (specifying <code>--file filename</code> if used in the compose up command)</p> <pre><code>docker compose down\n</code></pre> Compose simplified after Docker integration <p><code>compose.yaml</code> is the new Compose configuration file for orchestrating services locally, a simplified and extended version of <code>docker-compose.yaml</code>.  The main benefit is that a version number is no longer required, as the Compose version shipped with Docker is used.</p> <p><code>docker compose</code> the new command, replacing <code>docker-compose</code>, although the options and arguments to this command are the same for now.</p>"},{"location":"continuous-integration/docker/compose/#build-clojure-service","title":"Build Clojure Service","text":"<p>The simplest approach to building a service is to include a <code>build:</code> configuration specifying the location of a multi-stage <code>Dockerfile</code>, which has a <code>builder</code> stage to create an uberjar that is run in the run-time image.</p> <p>The <code>build:</code> option for the Clojure service with the path to the multi-stage Dockerfile for the project (typically in the same root directory of the project, although a remote Git repository can also be used)</p> <p>Clojure project build configuration</p> <pre><code># --- Docker Compose Configuration --- #\n# - Docker Compose V2\n# - https://docs.docker.com/compose/compose-file/\n#\n# Build the Clojure Service from source code\n# and run on port 8080\n\nname: \"practicalli\"\nservices:\n  # --- Clojure Service --- #\n  gameboard-service:\n    platform: linux/amd64\n    # Build using Dockerfile - relative path or Git repository\n    build:\n      context: ./ # Use Dockerfile in project root\n    environment:\n      - COMPOSE_PROJECT_NAME\n    ports: # host:container\n      - 8080:8080\n</code></pre> <ul> <li><code>name:</code> (optional) is used to set the container prefix of the service name, via the <code>environment:</code> variable <code>COMPOSE_PROJECT_NAME</code>, helping to identify the container by name when running.  The above container would be <code>practicalli-gameboard-service</code>.  This option is more valuable as the number of services grows</li> <li><code>services:</code> contains one or more service definitions with a unique name, e.g. <code>gameboard-service</code></li> <li><code>gameboard-service:</code> is a unique name for a service configuration</li> <li><code>platform:</code> defines the operating system and hardware architecture that should be used for the service</li> <li><code>build: context:</code> defines the location of the <code>Dockerfile</code> to use, either a local file or a Git repository URL</li> <li><code>ports:</code> defines the host:container mapping for the service port.  <code>8080:8000</code> value would map the service running within the container on <code>8000</code> to the host (e.g. engineers' computer) on port 8080.</li> </ul> <p></p> <p>Use a multi-stage <code>Dockerfile</code> to provide greater opportunity to create image overlays for the build stage which can be cached, e.g. downloading project dependencies, speeding up the build process on consecutive runs.</p>"},{"location":"continuous-integration/docker/compose/#compose-services","title":"Compose services","text":"<p>Add more unique service configurations under <code>services:</code>, e.g. <code>postgres-database</code>.  All the services that support the local testing and integration of the Clojure project can be added to the <code>compose.yaml</code> file (database, cache, mock APIs, coupled services, etc.).</p> <p>The Clojure service defines a dependency on a Postgres Database.  The dependency has a condition so the Clojure service is only started once the Postgres service is healthy</p> <p>Compose Clojure project build with Postgres database</p> <pre><code>services:\n  clojure-service:\n    platform: linux/amd64\n    build: ./\n    ports: # host:container\n      - 8080:8080\n    depends_on:\n      postgres-database:\n        condition: service_healthy\n\n  postgres-database:\n    image: postgres:15.2-alpine\n    environment:\n      POSTGRES_PASSWORD: \"$DOCKER_POSTGRES_ROOT_PASSWORD\"\n    healthcheck:\n      test: [ \"CMD\", \"pg_isready\" ]\n      timeout: 45s\n      interval: 10s\n      retries: 10\n    ports:\n      - 5432:5432\n</code></pre> <ul> <li><code>depends_on:</code> include one or more services that the service depends on, optionally adding a start <code>condition:</code></li> <li><code>depends_on: &lt;service-name&gt; condition:</code> a condition that should be true in order for the service image to start, typically checking <code>service_healthy</code> of another service in the configuration</li> <li><code>heathcheck:</code> define a command and options to determine if the specific service running is healthy, with the <code>test:</code> command specific to the type of service.</li> </ul> <p>Build and run the services using the <code>--build</code> flag with <code>docker compose</code></p> <pre><code>docker compose up --build\n</code></pre> <p>The Clojure project is built in parallel with running the Postgres database.</p> <p>On the initial run the Clojure project may take longer to run that starting the Postgres database, in which case the Clojure uberjar will run straight away.</p> <p>On consecutive runs, at least part of the Clojure build process should be cached and images for all the services will have been downloaded and cached.  The Clojure uberjar may wait for the Postgres database to start up.</p> <p>Each service can define a health check that can be used as a conditional startup trigger and ensure all services start in a meaningful order.</p> <p></p>"},{"location":"continuous-integration/docker/compose/#build-on-change","title":"Build on Change","text":"<p>Docker provides <code>watch</code> as an experimental feature which can rebuild the Clojure service when a file change is detected.</p> <p>The watch approach seems most useful for Clojure projects when troubleshooting issues that occur during system integration testing.</p> <p>When additional tools need to run outside of Clojure code, e.g. SaSS build, data loading / ETL, then <code>watch</code> can compliment the Clojure REPL workflow by providing incremental change management for non-clojur aspects of the project.</p> <p>Add an <code>x-develop</code> configuration with <code>watch</code> under the Clojure service configuration.  Define the action for each path to create a simple but comprehensive way to update the service</p> <p>Clojure Project - Build on Change</p> <pre><code>services:\n  clojure-service:\n    platform: linux/amd64\n    build: ./\n    ports: # host:container\n      - 8080:8080\n    depends_on:\n      postgres-database:\n        condition: service_healthy\n    x-develop:\n      watch:\n        - path: ./deps.edn\n          action: rebuild\n        - path: ./src\n          action: rebuild\n</code></pre> <p>Start the services and the file watch mode</p> <pre><code>docker compose up --detach &amp;&amp; docker compose alpha watch\n</code></pre> <p>Save changes to files and a new image for the Clojure service will be built and deployed when ready.</p> <p>Optimise Docker Cache in Build process</p> <p>Using build on change approach can run quite frequently, so an optimised build process in the <code>Dockerfile</code> or <code>compose.yaml</code> configuration is especially important to make build on change fast and therefore effective to use.</p>"},{"location":"continuous-integration/docker/compose/#compose-services_1","title":"Compose services","text":"<p>Define a <code>compose.yaml</code> file that builds the Clojure project and run services that the Clojure service requires or talks too (database, cache, mock API, etc.).</p> <p>Each service can define a heart beat which can be used as a conditional startup for other services.</p> compose.yaml new Compose configuration file <p><code>compose.yaml</code> is the new configuration file for orchestrating services locally, a simplified and extended version of <code>docker-compose.yaml</code>.</p> <p>Include the <code>build:</code> option for the Clojure service with the path to the multi-stage Dockerfile for the project (typically in the same root directory of the project, although a remote Git repository can also be used)</p> <p>The Clojure service defines a dependency on a Postgres Database.  The dependency has a condition so the Clojure service is only started once the Postgres service is healthy</p> <p>Clojure Service with Postgres Database</p> <pre><code>services:\n  clojure-service:\n    platform: linux/amd64\n    build: ./\n    ports: # host:container\n      - 8080:8080\n    depends_on:\n      postgres-database:\n        condition: service_healthy\n\n  postgres-database:\n    image: postgres:15.2-alpine\n    environment:\n      POSTGRES_PASSWORD: \"$DOCKER_POSTGRES_ROOT_PASSWORD\"\n    healthcheck:\n      test: [ \"CMD\", \"pg_isready\" ]\n      timeout: 45s\n      interval: 10s\n      retries: 10\n    ports:\n      - 5432:5432\n</code></pre> <p>Run the services using docker from the root of the project</p> <pre><code>docker compose up --build\n</code></pre>"},{"location":"continuous-integration/docker/compose/#file-watcher","title":"File watcher","text":"<p>Docker provides <code>watch</code> as an experimental feature which can rebuild the Clojure service when a file change is detected.  This seems most useful when troubleshooting issues that occur during system integration testing.</p> <p>Add an <code>x-develop</code> configuration with watch under the Clojure service configuration</p> <p>Automated rebuild on file change</p> <pre><code>    x-develop:\n      watch:\n        - path: ./deps.edn\n          action: rebuild\n        - path: ./src\n          action: rebuild\n</code></pre> <p>Start the services and the file watch mode</p> <pre><code>docker compose up --detach &amp;&amp; docker compose alpha watch\n</code></pre> <p>Save changes to files and a new image for the Clojure service will be built and deployed when ready.</p>"},{"location":"continuous-integration/docker/dockerfile/","title":"Dockerfile","text":"<p>A Dockerfile is a declarative configuration that defines how an image is assembled and can use another image to provide much of the configuration, minimising the amount of design and maintenance required.</p> <p>Each line in the <code>Dockerfile</code> configuration creates an overlay, a layer of the image that is applied to create the final image.  Overlays are cached by docker on the first run and if the files or commands the overlays are created from do not change, then the cache is used on consecutive use of the docker image.</p> <p>A Multi-stage <code>Dockerfile</code> is an effective way to build and run projects via a continuous integration pipeline and local development (in concert with supporting services via a <code>compose.yaml</code> configuration).</p> <p>Docker Hub provides a wide range of images, supporting development, continuous integration and system integration testing.</p>"},{"location":"continuous-integration/docker/dockerfile/#multi-stage-dockerfile","title":"Multi-stage Dockerfile","text":"<p>A multi-stage <code>Dockerfile</code> contains builder stage and an unnamed stage used as the run-time.  The builder stage can be designed optimally for building the Clojure project and the run-time stage optimised for running the service efficiently and securely.</p> <p>The builder stage caches dependencies to optimise building Clojure and the run-time stage optimises running the service efficiently and securely.</p> <p>The uberjar created by the builder image is copied over to the run-time image to keep that image as clean and small as possible (to minimise resource use).</p> <p></p> Practicalli Multi-stage Dockerfile for Clojure <p> Practicalli Multi-stage <code>Dockerfile</code> for Clojure projects derived from the configuration currently used for commercial and open source work.</p> <p>The Dockerfile uses make targets, which are Clojure commands defined in the Practicalli Makefile</p> <p>Multi-stage <code>Dockerfile</code> for Clojure projects Docker Multi-stage builds docs</p> <p>Docker init - beta feature</p> <p><code>docker init</code> is a new (beta) feature to create <code>Dockerfile</code>, <code>.dockerignore</code> and<code>compose.yaml</code> files using Docker recommended practices.</p> <p>Docker Build overview</p>"},{"location":"continuous-integration/docker/dockerfile/#official-docker-images","title":"Official Docker images","text":"<p>Docker Hub contains a large variety of images, using those tagged with Docker Official Image is recommended.</p> <ul> <li>Clojure - official Docker Image - provides tools to build Clojure projects (Clojure CLI, Leiningen, Boot)</li> <li>Eclipse temurin OpenJDK - official Docker image - built by the community - provides the Java run-time</li> </ul> <p>Ideally a base image should be used where both builder and run-time images share the same ancestor, this helps maintain consistency between build and run-time environments.</p> <p>The Eclipse OpenJDK image is used by the Clojure docker image, so they implicitly use the same base image without needed to be specified in the project <code>Dockerfile</code>.  The Eclipse OpenJDK image could be used as a base image in the <code>Dockerfile</code> but it would mean repeating (and maintaining) much the work done by the official Clojure image)</p> <p>Alternative Docker images</p> <ul> <li>CircleCI Convenience Images =&gt; Clojure - an optimised Clojure image for use with the CircleCI service</li> <li>Amazon Corretto is an alternative version of OpenJDK</li> </ul> <p>An Official Docker Image means the configuration of that image follows the Docker recommended practices, is well documented and designed for common use cases.  There is no implication at to the correctness of tools, languages or service that image provides, only in the means in which they are provided.</p>"},{"location":"continuous-integration/docker/dockerfile/#docker-init","title":"Docker init","text":"<p><code>docker init</code> provides a command line wizard to create set of Docker configurations that follow the recommended practices</p> <pre><code>docker init\n</code></pre> <p>Specific configurations are provides for Go and Python, other languages should use the General Purpose option.</p> <p>The General purpose option provides almost all the configuration required.  Update the RUN command in the build stage to that of the specific language, ideally copying the project dependency file to the build stage and inititing a dry run to download the dependency files to create a Docker overlay cache of dependencies.</p>"},{"location":"continuous-integration/docker/dockerfile/#dockerfile-syntax","title":"Dockerfile Syntax","text":"<p>The form of <code>Dockerfile</code> instructions</p> <pre><code># Comment\nINSTRUCTION arguments\n</code></pre> <p><code>#</code> at the start of a line defines a line comment.  <code>#</code> within a line is considered an argument to the instruction.</p> <p><code>\\</code> is a line continuation character, allowing a cleaner format when using longer commands as instruction arguments.</p> <pre><code>HEALTHCHECK \\\n    CMD [\"curl\", \"--fail\", \"http://localhost:8080/system-admin/status\"]\n</code></pre> <p>Start by learning the common instructions used in <code>Dockerfile</code> configurations and consult  Docker Docs extensive Dockerfile reference to understand additional instructions are required</p> <p> Docker Docs Dockerfile reference</p>"},{"location":"continuous-integration/docker/dockerfile/#from","title":"FROM","text":"<p><code>FROM</code> instruction should be the first instruction in the <code>Dockerfile</code> and specifies the image from which the current configuration uses as a base.</p> <p>FROM can be preceded ARG instructions to declare arguments used in the <code>FROM</code> instruction.  Comments <code>#</code> and parser directives can also appear before <code>FROM</code></p>"},{"location":"continuous-integration/docker/dockerfile/#copy","title":"COPY","text":"<p>Copy files from the local directory or from a URL to the Docker images.</p> <p>COPY replaces ADD</p> <p>Avoid the use of <code>ADD</code> and use <code>COPY</code> instead</p>"},{"location":"continuous-integration/docker/dockerfile/#workdir","title":"WORKDIR","text":"<p>Set the directory in the Docker image in which all future commands will run.</p> <p>Setting <code>WORKDIR</code> can simplify <code>COPY</code> and <code>RUN</code> commands</p>"},{"location":"continuous-integration/docker/dockerfile/#user","title":"USER","text":"<p>Set user account to run all further commands</p>"},{"location":"continuous-integration/docker/dockerfile/#run","title":"RUN","text":"<p>Execute any command recongnised within the container</p>"},{"location":"continuous-integration/docker/dockerfile/#healthcheck","title":"HEALTHCHECK","text":"<p>Docker Service heathcheck provides a simple mechanism to report that a service is running.  A health check is a vital configuration to provide for production systems to ensure failing containers are replaced by working containers automatically.</p> <p>The status of the service can be checked manually using the <code>docker inspect</code> command</p> <pre><code>docker inspect --format='{{json .State.Health}}' container-name\n</code></pre> <p>Define a <code>HEALTHCHECK CMD</code> to identify the current status of the service running within the container, e.g. a curl command to test an API endpoint that demonstrated the service is running.</p> <p>Heathcheck options include</p> <ul> <li><code>--interval</code> frequency to run the health check</li> <li><code>--timeout</code> maximum time to wait for a response to the health check command</li> <li><code>--start-period</code> grace period for the service to start up before the first health check runs</li> <li><code>--retries</code> number of times to repeat the command before reporting failure (reports .... until success or failure)</li> </ul> <p>Wait 10 seconds before running the first health check, then try every 5 seconds and wait 3 seconds for a response.  Run the health check a maximum of 2 times before reporting failure (assuming successful check has not been returned).</p> <pre><code>--interval=5s --timeout=3s --start-period=10s --retries=2\n</code></pre> <p>HEALTHCHECK using shell command</p> <pre><code>HEALTHCHECK \\\n  CMD curl --fail http://localhost:8080/system-admin/status || exit 1\n</code></pre> <p>HEALTHCHECK using Docker Exec array command</p> <pre><code>HEALTHCHECK \\\n    CMD [\"curl\", \"--fail\", \"http://localhost:8080/system-admin/status\"]\n</code></pre> <p>Compose can also define a health check</p> <p><code>compose.yaml</code> can include a health check for each service defined which can be used by the service being built to conditionally start.</p> <p>Add a health-check to the <code>compose.yaml</code> configuration when the service being built requires a database or similar service that should be available before starting.</p> <p>Avoid defining a health-check in the <code>compose.yaml</code> for services that already have an adequate health-check defined in ther <code>Dockerfile</code></p>"},{"location":"continuous-integration/docker/dockerfile/#entrypoint","title":"ENTRYPOINT","text":"<p>Define the default command to run once the container is started.</p> <p>The image used to create the final stage from may have a default command, e.g. Eclipse Temurin image provides the <code>jshell</code> ENTRYPOINT, which is overridden by supplying an ENTRYPOINT in the <code>Dockerfile</code> for the current project.</p> <p><code>ENTRYPOINT</code> is typically used with <code>CMD</code></p> <p>Process manager ENTRYPOINT with service CMD</p> <p>Start dumb-init to manage the CMD process that calls the java run-time. <code>dumb-init</code> ensures SIGTERM signals are sent to the <code>java</code> process ensuring a clean shutdown <pre><code>ENTRYPOINT [\"/usr/bin/dumb-init\", \"--\"]\nCMD [\"java\", \"-jar\", \"/service/practicalli-suit-you-sir-standalone.jar\"]\n</code></pre></p> <p> Docker Entrypoint documentation</p>"},{"location":"continuous-integration/docker/dockerfile/#cmd","title":"CMD","text":"<p>The command to run the service within the container, typically paired with ENTRYPOINT</p> <p>CMD can be supplied on the command line when calling docker to over-ride the CMD defined in the <code>Dockerfile</code>, e.g. to help test a Docker image that is not running correctly or run variations of the service such as debug mode.</p>"},{"location":"continuous-integration/docker/images/","title":"Docker Images","text":"<p>Docker images provide a quick approach to trying services and different operating systems.  An image can be used as the base for other images.</p>"},{"location":"continuous-integration/docker/images/#selecting-images","title":"Selecting Images","text":"<p>Docker Official Images are highly recommended.  Look for the Docker Official Image tag on the image page.</p> Docker Official Image meaning <p>An Official Docker Image means the configuration of that image follows the Docker recommended practices, is well documented and designed for common use cases.</p> <p>There is no implication as to the correctness of tools, languages or service that image provides, only in the means in which they are provided.</p> <p>However, if time was invested in creating an image good enough to pass the Docker review, then it has a higher probability of being a useful image that others that are not official.</p> <p>Common Official Docker images include</p> <ul> <li> Alpine Linux minimal operating system (musl lib)</li> <li> Debian Linux or  Ubuntu Linux operating system</li> <li> Clojure - built by the Clojure community, provides tools to build Clojure projects (Clojure CLI, Leiningen)</li> <li> OpenJDK - Eclipse temurin - built by the community - provides the Java run-time</li> <li> OpenJDK - Amazon Corretto is an OpenJDK distribution by Amazon AWS team,  Amazon Corretto can also be installed for the local development environment</li> <li> Postgres open-source object-relational database management system</li> <li> Redis open-source, networked, in-memory, key-value data store with optional durability</li> <li> nginx open source reverse proxy &amp; load balancing for HTTP, HTTPS, SMTP, POP3 &amp; IMAP protocols, HTTP cache and a web server</li> <li> mariadb open source relational database by the original developers of MySQL and is much more efficient</li> </ul>"},{"location":"continuous-integration/docker/images/#operating-systems","title":"Operating Systems","text":"<p>Operating systems are the base for many other Docker images, providing essential tools required for all software.</p> <p>Alpine Linux is ultra-minimal operating system.  Debian and Ubuntu Linux are glibc based operating systems which are commonly used.</p> <p>Operating System Image Tags</p> Image tag compressed size debian:bookworm-slim 27.8 MB debian:bookworm 47.29 MB alpine:latest 3.24 MB ubuntu:latest 28.17 MB"},{"location":"continuous-integration/docker/images/#alpine-linux","title":"Alpine Linux","text":"<p> Alpine Linux images have a very small footprint and provide the essential tools for running services that do not depend on specific operating system packages.</p> <p>As images are very small, resources used both locally and in stage and production environments are minimal.  This can be a simple way to reduce costs and do more with far fewer resources.</p> <p>Alpine Linux uses the <code>pkg</code> tool for package management, to add tools and libraries to support a Dockerfile build stage if required.</p> <p>Alpine uses musl libc rather than glibc so testing software run on top of Alpine is important.  Java 16 release officially supports Alpine Linux using musl.  If required, there are several options to running glibc software on Alpine.</p> <p>Alpine Linux Official Image</p>"},{"location":"continuous-integration/docker/images/#debian-linux","title":"Debian Linux","text":"<p> Debian Linux is a completely free operating system.</p> <p><code>bookworm</code> is the name for Debian Linux 12, the latest stable release.</p> <p>Debian Slim image provides a glib based operating system with a relatively minimal size, although not as small as Alpine Linux.</p> <p>Debian Linux latest stable image - slim variant</p> <pre><code>FROM debian:bookworm-slim\n</code></pre> <p>Ubuntu Linux is built from Debian Linux, adding kernel patches, sudo adminstrative access and other enhancements.</p> <p>Debian Linux Ubuntu</p>"},{"location":"continuous-integration/docker/images/#openjdk","title":"OpenJDK","text":"<p>OpenJDK is the most commonly used run-time environment for Java and JVM languages (Clojure, Kotlin, Scala, Gradle, Jython, JRuby, etc.).</p> <p>A permissive use and distribution license is provided by all OpenJDK image variants.</p>"},{"location":"continuous-integration/docker/images/#eclipse-temurin","title":"Eclipse Temurin","text":"<p> Eclipse temurin OpenJDK - official Docker image is built by the  Java community and provides the Java run-time (Java Virtual Machine).  Eclipse Temurin provides all Long Term Support (LTS) versions from Java 8 onward and current intermediate releases.  Variants are available with Alpine Linux and a wide range of system architectures (<code>amd64</code>, <code>arm32v7</code>, <code>arm64v8</code>, <code>ppc64le</code>, <code>s390x</code>, <code>windows-amd64</code>)</p>"},{"location":"continuous-integration/docker/images/#amazon-corretto","title":"Amazon Corretto","text":"<p> Amazon Corretto is an OpenJDK distribution by Amazon AWS team and may be an appropriate choice if relying on Amazon support.</p> <p>Amazon Corretto can also be installed for the local development environment, providing a consistent run-time between development and production.</p>"},{"location":"continuous-integration/docker/images/#custom-built-image","title":"Custom built image","text":"<p>OpenJDK can be built with a custom set of modules, optomising the size of a container used to run a Java or JVM service</p> <p><code>jdeps</code> analyses a <code>.class</code> file, directory or <code>.jar</code> file and lists Java module dependencies.</p> <p>Clojure Uberjar files built with tools.build use  multi-release jars, so a Java release version should be specified to see the respective dependencies. JDK 17 dependencies for Clojure Uberjar<pre><code>jdeps --multi-release 17 target/practicalli-todo-standalone.jar &gt; jdeps-report.txt\n</code></pre></p> <p><code>java --list-modules</code> lists the modules contained in the current Java environment.</p> Custom Java runtime using jlink in multi-stage container build <pre><code>FROM eclipse-temurin:21 as jre-build\n\n# Create a custom Java runtime\n#(1)!\nRUN $JAVA_HOME/bin/jlink \\\n         --add-modules java.base \\\n         --strip-debug \\\n         --no-man-pages \\\n         --no-header-files \\\n         --compress=2 \\  \n         --output /javaruntime  \n\n# Define base image\nFROM debian:bookworm-slim\nENV JAVA_HOME=/opt/java/openjdk\nENV PATH \"${JAVA_HOME}/bin:${PATH}\"\nCOPY --from=jre-build /javaruntime $JAVA_HOME\n\n# Continue with application deployment\n</code></pre> <ol> <li><code>--compress=2</code> option for Jlink uses Zip compression</li> </ol> <p>If the JVM based application is still evolving it may be prudent to include the Jdeps command within the Jlink command</p> <pre><code>FROM eclipse-temurin:21 as jre-build\n\n# Create a custom Java runtime\n#(1)!\nRUN $JAVA_HOME/bin/jlink \\\n         --module-path practicalli-todo-uber.jar \\\n         --add-modules $(jdeps --ignore-missing-deps \\\n                               --multi-release 21 \\\n                               --print-module-deps practicalli-todo-uber.jar) \\\n         --no-header-files \\\n         --no-man-pages \\\n         --strip-debug \\\n         --compress=2 \\  \n         --output /javaruntime  \n\n# Define base image\nFROM debian:bookworm-slim\nENV JAVA_HOME=/opt/java/openjdk\nENV PATH \"${JAVA_HOME}/bin:${PATH}\"\nCOPY --from=jre-build /javaruntime $JAVA_HOME\n\n# Continue with application deployment\n</code></pre> <p> jlink - Oracle Java SE help center</p> <p> jlink - Assembel &amp; Optomise Modules</p> <p> jdeps - Oracle Java SE help center</p> <p> Modules - Java 17 API Specification</p>"},{"location":"continuous-integration/docker/images/#clojure","title":"Clojure","text":"<p> Clojure - official Docker Image - built by the Clojure community, provides tools to build Clojure projects (Clojure CLI, Leiningen)</p> <p>Use the Clojure image within a <code>Dockerfile</code>, specifying <code>tools-deps</code> or <code>lein</code> variants</p> <p>Clojure CLI Docker Image as builder stage</p> Dockerfile<pre><code>FROM clojure:tools-deps AS builder\n</code></pre> <p>The Clojure image is built from the equivalent Eclipse Temurin image which can be used as the run-time image for a Multi-stage Dockerfile final stage.  As the build and final stages are built upon the same underlying image, a separate base stage is not required.</p> <p> Clojure - official Docker Image</p> <p>Multi-stage Dockerfile for Clojure</p>"},{"location":"continuous-integration/docker/images/#clojure-image-tags","title":"Clojure image tags","text":"Image tag compressed size temurin-17-tools-deps-bookworm-slim 234.56 MB temurin-21-tools-deps-bookworm-slim 247.68 MB temurin-17-tools-deps-alpine 178.96 MB temurin-21-tools-deps-alpine 191.89 MB <p>Consider building a custom Java JDK to reduce the image size</p>"},{"location":"continuous-integration/docker/images/#megalinter","title":"MegaLinter","text":"<p>MegaLinter greatly simplifies applying quality</p> <p>The MegaLinter image is an example of a docker image that provides a large number of tools which otherwise need to be installed directly on the operating system.</p>"},{"location":"continuous-integration/docker/install/","title":"Install Docker Desktop","text":"<p>Docker community edition provides the back-end services to run docker images in containers.</p> <p>Docker Desktop provides a graphical UI for managing images, containers and volumes.</p>"},{"location":"continuous-integration/docker/install/#install-docker-desktop_1","title":"Install Docker Desktop","text":"<p>Docker desktop depends on Docker Community Edition and will install all the respective packages.</p> Debian Linux <p>Install Docker Desktop on Debian - Docker Docs</p> <p>Debian Linux based distributions has several prerequisites packages (may already be installed).</p> <pre><code>apt install ca-certificates curl\n</code></pre> <p>Add the Docker team public key to Debian package manager, ensuring only official Docker packages are used</p> <pre><code>install -m 0755 -d /etc/apt/keyrings &amp;&amp; \\\ncurl -fsSL https://download.docker.com/linux/debian/gpg -o /etc/apt/keyrings/docker.asc &amp;&amp; \\\nchmod a+r /etc/apt/keyrings/docker.asc\n</code></pre> <p>Add the Docker PPA to the Debian package manager, creating a <code>/etc/apt/sources.list.d/docker.list</code> file.</p> <pre><code>echo \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\\n  $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n\necho \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/debian \\\n  $(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\") stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n</code></pre> <p>Update the available packages list with the Docker repository <pre><code>apt-get update\n</code></pre></p> <p> Download the DEB package for Docker Desktop UI and install using Debian package manager <pre><code>apt install ./Downloads/docker-desktop-4.19.0-amd64.deb\n</code></pre></p>"},{"location":"continuous-integration/docker/install/#post-install","title":"Post Install","text":"<p>User accounts that will run docker should be included in the <code>docker</code> group.</p> Debian Linux <p>Add the current operating system user account to the <code>docker</code> operating system group, creating the <code>docker</code> group if it doesn't already exist</p> <pre><code>groupadd docker &amp;&amp; \\\nusermod -aG docker $USER\n</code></pre> <p>Logout of the current session (e.g. logout of Desktop) to apply the <code>docker</code> group to the user account.</p> <p><code>groups</code> command will list the operating system groups assigned to the current user.</p>"},{"location":"continuous-integration/docker/install/#start-docker-docker-desktop","title":"Start Docker &amp; Docker Desktop","text":"<p>Starting Docker Desktop will automatically start the underlying Docker community edition that provides the run-time for docker containers.</p> <p>Launch Docker Desktop via the desktop application Launcher or via the command line:</p> Debian Linux <p>Use the Debian application launcher to start Docker Desktop, or use the <code>systemctl</code> command from a terminal.</p> <pre><code>systemctl --user start docker-desktop\n</code></pre> <p>Docker desktop may automatically restart itself on first run</p>"},{"location":"continuous-integration/docker/install/#check-docker-works","title":"Check Docker works","text":"<p>Use the Docker tutorial image to check that Docker can run a container from an image (and also learn about Docker if new to the tools).</p> <pre><code>docker run -dp 80:80 docker/getting-started\n</code></pre> <p>The tutorial image will be downloaded and the image run in a container</p> <pre><code>\u276f docker run -dp 80:80 docker/getting-started\n\nUnable to find image 'docker/getting-started:latest' locally\nlatest: Pulling from docker/getting-started\nc158987b0551: Already exists\n1e35f6679fab: Pull complete\ncb9626c74200: Pull complete\nb6334b6ace34: Pull complete\nf1d1c9928c82: Pull complete\n9b6f639ec6ea: Pull complete\nee68d3549ec8: Pull complete\n33e0cbbb4673: Pull complete\n4f7e34c2de10: Pull complete\nDigest: sha256:d79336f4812b6547a53e735480dde67f8f8f7071b414fbd9297609ffb989abc1\nStatus: Downloaded newer image for docker/getting-started:latest\n215c033924260874013394d1f27fa5ec587f183ee9851d3a48884a1422fcc732\n</code></pre> <p>Open the tutorial website at http://localhost/ and follow the tutorial steps to learn more about Docker.</p> <p>Increase concurrent download of image overlays</p> <p>increase the max-concurrent-downloads value from the default 3 to 24, to download multi-layered docker images faster .docker/config.json<pre><code>{\n \"auths\": {},\n \"credsStore\": \"desktop\",\n \"currentContext\": \"default\",\n \"max-concurrent-downloads\": 24\n}\n</code></pre></p>"},{"location":"continuous-integration/docker/install/#check-installed-versions","title":"Check installed versions","text":"<p>Print the version of Docker installed and if Docker Desktop is running, then its version is also printed.</p> <pre><code>docker version\n</code></pre> <p>Example</p> Output from running Docker Desktop <pre><code>\u276f docker version\nClient: Docker Engine - Community\n Cloud integration: v1.0.31\n Version:           23.0.6\n API version:       1.42\n Go version:        go1.19.9\n Git commit:        ef23cbc\n Built:             Fri May  5 21:18:13 2023\n OS/Arch:           linux/amd64\n Context:           desktop-linux\n\nServer: Docker Desktop 4.19.0 (106363)\n Engine:\n  Version:          23.0.5\n  API version:      1.42 (minimum version 1.12)\n  Go version:       go1.19.8\n  Git commit:       94d3ad6\n  Built:            Wed Apr 26 16:17:45 2023\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          1.6.20\n  GitCommit:        2806fc1057397dbaeefbea0e4e17bddfbd388f38\n runc:\n  Version:          1.1.5\n  GitCommit:        v1.1.5-0-gf19387a\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n</code></pre> <p>Check compose version</p> <pre><code>docker compose version\n</code></pre> Example output <pre><code>\u276f docker compose version\nDocker Compose version v2.17.3\n</code></pre>"},{"location":"continuous-integration/docker/install/#optimise-log-rotation","title":"Optimise Log rotation","text":"<p>Docker uses the <code>json-file</code> driver which creates JSON objects of log events from all containers.  To avoid disk space issues, configure log rotation in a <code>/etc/docker/daemon.json</code> file</p> <p>Configure Log rotation</p> /etc/docker/daemon.json<pre><code>{\n  \"log-driver\": \"json-file\",\n  \"log-opts\": {\n    \"max-size\": \"10m\",\n    \"max-file\": \"3\"\n  }\n}\n</code></pre> <p>Use the local file logging driver if a longer logging history is desirable.  The local file logging driver preserves 100Mb of logs per container (5 x 20Mb files) and uses automatic compression to greatly reduce disk consumption.</p> <p>Use local file logging driver</p> /etc/docker/daemon.json<pre><code>{\n  \"log-driver\": \"local\",\n  \"log-opts\": {\n    \"max-size\": \"10m\"\n  }\n}\n</code></pre> <p>--log-driver flag with <code>docker container create</code> or <code>docker run</code> commands sets the log driver for the specific container, over-riding the global setting</p> <ul> <li>JSON File logging driver</li> </ul>"},{"location":"continuous-integration/docker/desktop/","title":"Docker Desktop Overview","text":"<p>Docker desktop provides an easy way to manage Docker images, containers and volumes.  Sign in to Docker Desktop to manage your images on DockerHub.</p> <p> </p> <p>Docker Desktop Overview Docker Docs</p>"},{"location":"continuous-integration/docker/desktop/#extensions-marketplace","title":"Extensions Marketplace","text":"<p>There is a growing marketplace of extensions that provide very useful tools to extend the capabilities of Docker Desktop.  Search within the Docker Desktop extensions or for extensions on Docker Hub.</p> <ul> <li>Resource Usage monitor resources (cpu, memory, network, disk) used by containers and docker compose systems over time</li> <li>Disk Usage optimise use of local disk space by removing unused images, containers and volumes</li> <li>Volumes Backup &amp; Share to backup, clone, restore and share Docker volumes easily</li> <li>Logs Explorer view all container logs in one place to assist troubleshooting</li> <li>Postgres Admin PGAdmin4 open source management tool for Postgres</li> <li>Trivy scan local and remote images for security vulnerabilities</li> <li>Snyk scan local and remote images for security vulnerabilities</li> <li>Ddosify high-performance, open-source and simple load testing tool</li> </ul> <p> </p> <p>Docker Extensions Overview</p>"},{"location":"continuous-integration/docker/desktop/#images","title":"Images","text":"<p>Pre-defined images can be used directly or used to build custom images.</p> <p> Docker Images Overview and Practicalli recommendations</p>"},{"location":"continuous-integration/docker/desktop/#containers","title":"Containers","text":"<ul> <li>container status</li> <li>view details</li> <li>logs</li> <li>inspect - environment, port values</li> <li>terminal shell within the container</li> <li>files - file browser for the container (check for uberjar, test-data, etc)</li> <li> <p>stats- basic resource usage  (Resource usage extension provides view over all containers)</p> </li> <li> <p>start / stop containers</p> </li> </ul>"},{"location":"continuous-integration/docker/desktop/#volumes","title":"Volumes","text":"<p>Persist data to the local computer by mounting a part of the OS filespace as a volume in a Docker container.</p>"},{"location":"continuous-integration/docker/desktop/#dev-environments","title":"Dev Environments","text":""},{"location":"continuous-integration/docker/desktop/extensions/","title":"Docker Desktop Extensions","text":"<p>Docker Marketplace contains many useful and freely available extensions to manage the docker environment and services running within containers.</p> <p>Docker Desktop Extensions overview</p>"},{"location":"continuous-integration/docker/desktop/extensions/#resources","title":"Resources","text":"<p> Resource usage</p> <p> Disk usage</p>"},{"location":"continuous-integration/docker/desktop/extensions/#logging","title":"Logging","text":"<p> Logs Explorer</p>"},{"location":"continuous-integration/docker/desktop/extensions/#backup","title":"Backup","text":"<p> Volumes backup &amp; Share</p>"},{"location":"continuous-integration/docker/desktop/extensions/#security","title":"Security","text":"<p> Snyk security vulnerability scanner</p>"},{"location":"continuous-integration/docker/desktop/extensions/#database","title":"Database","text":"<p> PostgreSQL database monitoring</p>"},{"location":"continuous-integration/github/","title":"GitHub Workflows and Actions","text":"<p>A GitHub workflow defines one or more jobs to carry out as part of a continuous integration run.</p> <p>Workflows are triggered by events, usually bit Git related actions (commit, PR, etc.) or by schedule or human interaction with the GitHub website.</p> <p>GitHub Actions provide common jobs that can be used within a workflow, greatly reducing the configuration required to get tasks done.</p>"},{"location":"continuous-integration/github/actions/","title":"GitHub Actions","text":"<p>GitHub Actions are pre-defined tasks that can be used within a GitHub Workflow, triggered by events such as committing to a branch or pull request.</p> <p>GitHub Actions Marketplace  contains a wide range of actions that can be used to quickly create a workflow.</p> <p>Use major version only</p> <p>Use the major version of a GitHub action within a GitHub workflows to minimise the maintenance of action versions in workflow configuration.  e.g. if the latest version is <code>v5.2.1</code>, then use version <code>v5</code> to use the latest version available within that major version.</p>"},{"location":"continuous-integration/github/actions/#authentication","title":"Authentication","text":"<p>GitHub actions have read access to repositories, allowing them to checkout code and configuration.  For more permissions the GitHub Action requires an authorisation token.</p> <p>GitHub automatically creates a token that GitHub actions can use to make an authenticated API request, scoped to the current repository.</p> Use automatically created token<pre><code>      - uses: actions/action-name@v2\n        with:\n          repo-token: ${{ secrets.GITHUB_TOKEN }}\n</code></pre> <p>For customised authentication or access to a different GitHub repository, create a developer token and save it as a user or organisation secret.</p> <p>GitHub - Automatic token authentication</p>"},{"location":"continuous-integration/github/actions/#common-github-actions","title":"Common GitHub Actions","text":"Action Description Checkout Checkout repository to enable workflow to access Cache cache dependencies and build outputs to improve workflow execution time Changelog Enforcer checks the CHANGELOG.md file has been updated for a pull request MegaLinter verify code and configuration consistency Setup reviewdog Setup reviewdog post review comments to pull requests, typically used with lint tools Clojure lint clj-kondo lint with reviewdog comments added inline to pull request Setup java jdk setup up Java run-time environment (specify version, distribution, etc) Setup clojure Setup Clojure environment and common Clojure tools for quality (clj-kondo, cljstyle, unit testing, etc) Setup go sets up a Go language environment Setup node setup a version of the Node.js environment, caching npm/yarn /pnpm dependencies Setup python Provides python or PyPy, optionally cache pip dependencies GitHub Pages Deploy deploy to GitHub pages, including cross-repository deployments <p>Actions to investigate</p> <ul> <li>clj-watson - software composition analysis (SCA)</li> <li>clj-homes - SAST (Static application security testing) tool</li> <li>Clojure Dependency Update Action - update project dependency versions for Clojure CLI, Shadow-cljs, Leiningen, Boot and Maven Pom.xml</li> <li>Clojure-Autodoc Action - generate HTML API documentation from a Clojure project</li> <li>Clojars Release Action</li> </ul>"},{"location":"continuous-integration/github/common-jobs/","title":"Common GitHub Workflow Jobs","text":"<p>Common job configuration snippets taken from GitHub workflows, showing options typically used with GitHub actions.</p>"},{"location":"continuous-integration/github/common-jobs/#basic-workflow-information","title":"Basic workflow information","text":"<p>Echo information regarding the triggering of the workflow, to help diagnose issues should they arise</p> <p>Git Checkout Action</p> <pre><code>jobs:\n  workflow:\n    name: workflow-name\n    runs-on: ubuntu-latest\n    steps:\n      - run: echo \"\ud83d\ude80 Job automatically triggered by ${{ github.event_name }}\"\n      - run: echo \"\ud83d\udc27 Job running on ${{ runner.os }} server\"\n      - run: echo \"\ud83d\udc19 Using ${{ github.ref }} branch from ${{ github.repository }} repository\"\n</code></pre> <p>Add a summary of the workflow at the end of the configuration.</p> <pre><code>      # Summary and status\n      - run: echo \"\ud83c\udfa8 Workflow name completed\"\n      - run: echo \"\ud83c\udf4f Job status is ${{ job.status }}.\"\n</code></pre>"},{"location":"continuous-integration/github/common-jobs/#git-checkout","title":"Git Checkout","text":"<p>Check-out the project from version control, fetching the whole history with <code>fetch-depth: 0</code>. Set <code>fetch-depth:</code> to 1 (or remove the option) to checkout the single commit for the ref/SHA that triggered the workflow</p> <p>Echo the GitHub Repository that was cloned to the workflow log, to support debugging efforts.</p> <p>Git Checkout Action</p> <pre><code>      # Git Checkout\n      - name: Checkout Code\n        uses: actions/checkout@v3\n        with:\n          token: \"${{ secrets.PAT || secrets.GITHUB_TOKEN }}\"\n          fetch-depth: 0   # fetch all history\n      - run: echo \"\ud83d\udc19 ${{ github.repository }} repository was cloned to the runner.\"\n</code></pre>"},{"location":"continuous-integration/github/common-jobs/#sparse-checkout","title":"Sparse Checkout","text":"<p>Using <code>git sparse-checkout</code> can save signficant time by checking out a sub-set of files, especially from a large repository with thousands of commits.</p> <p><code>git sparse-checkout</code> always checks out all files in the root of the repository, so specify only the additional files and directories required.</p> <p>Git Sparse Checkout Action</p> <pre><code>      # Git Checkout\n      - name: Checkout Code\n        uses: actions/checkout@v3\n        with:\n          token: \"${{ secrets.PAT || secrets.GITHUB_TOKEN }}\"\n          fetch-depth: 0   # fetch all history\n          sparse-checkout: |\n            docs\n            overrides\n      - run: echo \"\ud83d\udc19 ${{ github.repository }} repository was cloned to the runner.\"\n</code></pre>"},{"location":"continuous-integration/github/common-jobs/#pull-request-first-interaction","title":"Pull Request first interaction","text":"<p>first-interaction returning Bad Credentials error</p> <p>Add messages to a contributor's first issue or pull request to a repository.</p> <p>In this example, the contributing guide is added as a comment to the issue or pull request.</p> <p>First Interaction Action</p> <pre><code>      # Message on first interaction\n      - name: First interaction\n        uses: actions/first-interaction@v1.1.1\n        with:\n          # Token for the repository\n          repo-token: \"{{ secrets.GITHUB_TOKEN }}\"\n          # Comment to post on an individual's first issue\n          issue-message: \"[Practicalli Contributing Guide](https://practical.li/spacemacs/introduction/contributing/)\"\n          # Comment to post on an individual's first pull request\n          pr-message: \"[Practicalli Contributing Guide](https://practical.li/spacemacs/introduction/contributing/)\"\n</code></pre>"},{"location":"continuous-integration/github/trigger-events/","title":"Trigger Workflows","text":"<p>GitHub workflows are triggered by events.</p> <p>Events related to a GitHub workflow include</p> <ul> <li>pull request</li> <li>push</li> <li>workflow_dispatch (Manual run)</li> <li>Schedule (Cron)</li> </ul> <p>Scheduled Version Check</p> <p>Scheduled Version Check GitHub workflow uses the cron schedule to check for newer versions of Clojure libraries and GitHub actions</p>"},{"location":"continuous-integration/github/trigger-events/#cron-schedule","title":"Cron schedule","text":"<p>GitHub workflows can use <code>cron:</code> to define a schedule as to when a workflow should run.</p> <p>A POSIX Cron pattern is used to define the schedule</p> <ul> <li>Minute [0,59]</li> <li>Hour [0,23]</li> <li>Day of the month [1,31]</li> <li>Month of the year [1,12]</li> <li>Day of the week ([0,6] with 0=Sunday)</li> </ul> <pre><code>    name: \"Scheduled Version Check\"\n    on:\n      schedule:\n        - cron: \"0 4 * * *\" # at 04:04:04 ever day\n        # - cron: \"0 4 * * 5\" # at 04:04:04 ever Friday\n        # - cron: \"0 4 1 * *\" # at 04:04:04 on first day of month\n</code></pre> <p>Scheduled version check uses cron schedule</p> Cron references <ul> <li><code>on.schedule</code> - GitHub workflow syntax</li> <li>Cron Expression Syntax Cheatsheet</li> </ul>"},{"location":"continuous-integration/github/trigger-events/#workflow-dispatch","title":"Workflow Dispatch","text":"<p>Add <code>workflow_dispatch:</code> without arguments to the <code>on:</code> directive in a workflow configuration file to manually trigger the running of the workflow.</p> <p>Visit GitHub.com &gt; Actions &gt; Workflow Name in the repository and select <code>Run</code></p>"},{"location":"continuous-integration/github/trigger-events/#conditional-on-other-workeflow","title":"Conditional on other workeflow","text":"<p>Run a workflow based on the outcome of running another GitHub workflow</p> <p>Run workflow if MegaLinter workflow returns success</p> <pre><code>    name: \"Publish Documentation\"\n    on:\n      # Run work flow conditional on linter workflow success\n      workflow_run:\n        workflows:\n          - \"MegaLinter\"\n        paths-ignore:\n          - README.md\n          - CHANGELOG.md\n          - .gitignore\n        branches:\n          - main\n        types:\n          - completed\n</code></pre>"},{"location":"continuous-integration/github/workflows/","title":"GitHub workflows","text":"<p>The marketplace page for a GitHub action should specify how it is used within a GitHub workflow.</p>"},{"location":"continuous-integration/github/workflows/#event-triggers","title":"Event Triggers","text":"<p>Workflows can be triggered by</p> <ul> <li>commit to branch</li> <li><code>pull_request</code></li> <li><code>cron</code> scheduled event</li> <li><code>workflow_dispatch</code> manual trigger</li> </ul>"},{"location":"continuous-integration/github/workflows/#include-exclude-patterns","title":"Include Exclude patterns","text":"<p>Files and directories to include or exclude within the scope of a trigger.  </p> <p>e.g. ingnore a pull request when changes are only in the <code>README.md</code> file</p> <pre><code>    name: Workflow name\n    on:\n      pull_request:\n        paths-ignore:\n          - \"README.md\"\n</code></pre>"},{"location":"continuous-integration/github/workflows/#github-action-version","title":"GitHub Action version","text":"<p>GitHub actions typically use semantic versioning for their releases</p> Semantic versioning <p>Given a version number MAJOR.MINOR.PATCH, increment the:</p> <ul> <li>MAJOR version for incompatible API changes</li> <li>MINOR version to add functionality in a backward compatible manner</li> <li>PATCH version for backward compatible bug fixes</li> <li>Additional labels for pre-release and build metadata are available as extensions to the MAJOR.MINOR.PATCH format.</li> </ul> <p>Semantic Versioning 2.0.0 Specification </p> <p>Specify the major release the major version will use the latest version within that scope, e.g. <code>action/checkout@v3</code> will use <code>v3.5.2</code>, the latest version within that major version</p> <pre><code>steps:\n    - uses: actions/checkout@v3\n</code></pre> Use major version of Action in workflow configuration <p>Use the major version of a GitHub action within a GitHub workflows to minimise maintenance of the workflow configuration.</p> <p>Use a specific patch release tag when a specific version is reqiured, providing a consistent version that is used each time.</p> <pre><code>steps:\n    - uses: actions/checkout@v1.0.1\n</code></pre> <p>Use a branch name, commonly used for release management</p> <pre><code>steps:\n    - uses: actions/checkout@v1-beta\n</code></pre> <p>Using a commit's SHA for release management Every Git commit has a unique and immutable SHA value, calculated in part from the contents of the commit.</p> <p>A SHA value can be more reliable than specifying a tag value which could be deleted or moved.</p> <p>Using a SHA value does means only that specific commit is used, so a new SHA value must be used if a newer version is required. The full SHA value of the commit must be used, not the abbreviated value.</p> <pre><code>steps:\n    - uses: actions/checkout@172239021f7ba04fe7327647b213799853a9eb89\n</code></pre> <p> GitHub: creating actions reference</p>"},{"location":"continuous-integration/github/workflows/luarocks/","title":"Lua Rocks","text":"<p>Publish package to Lua Rocks</p> Lua Rocks API key required <p>Add Lua Rocks API to GitHub repository (or organisation) secrets</p> <p>GitHub Workflow to Publish package to Lua Rocks</p> <pre><code>name: \"release\"\non:\n  push:\n    tags:\n      - 'v*'\njobs:\n  luarocks-upload:\n    runs-on: ubuntu-22.04\n    steps:\n      - uses: actions/checkout@v3\n      - name: LuaRocks Upload\n        uses: nvim-neorocks/luarocks-tag-release@v4\n        env:\n          LUAROCKS_API_KEY: ${{ secrets.LUAROCKS_API_KEY }}\n        with:\n          detailed_description: |\n            A meaingful description of the package that should show on Lua Rocks.\n          dependencies: |\n            plenary.nvim\n</code></pre>"},{"location":"continuous-integration/github/workflows/megalinter/","title":"MegaLinter Workflow","text":"<p>The MegaLinter Workflow uses a configuration file to define which linters should be run as well as specify linter specific configuration files.</p> <p>Practicalli MegaLinter Workflow</p> <pre><code>---\n# MegaLinter GitHub Action configuration file\n# More info at https://megalinter.github.io\n# All variables described in https://megalinter.github.io/configuration/\n\nname: MegaLinter\non:\n  workflow_dispatch:\n  pull_request:\n    branches: [main]\n  push:\n    branches: [main]\n\n# Run Linters in parallel\n# Cancel running job if new job is triggered\nconcurrency:\n  group: \"${{ github.ref }}-${{ github.workflow }}\"\n  cancel-in-progress: true\n\njobs:\n  megalinter:\n    name: MegaLinter\n    runs-on: ubuntu-latest\n    steps:\n      - run: echo \"\ud83d\ude80 Job automatically triggered by ${{ github.event_name }}\"\n      - run: echo \"\ud83d\udc27 Job running on ${{ runner.os }} server\"\n      - run: echo \"\ud83d\udc19 Using ${{ github.ref }} branch from ${{ github.repository }} repository\"\n\n      # Git Checkout\n      - name: Checkout Code\n        uses: actions/checkout@v3\n        with:\n          token: \"${{ secrets.PAT || secrets.GITHUB_TOKEN }}\"\n          fetch-depth: 0\n      - run: echo \"\ud83d\udc19 ${{ github.repository }} repository was cloned to the runner.\"\n\n      # MegaLinter Configuration\n      - name: MegaLinter Run\n        id: ml\n        ## latest release of major version\n        uses: oxsecurity/megalinter/flavors/java@v6\n        env:\n          # ADD CUSTOM ENV VARIABLES OR DEFINE IN MEGALINTER_CONFIG file\n          MEGALINTER_CONFIG: .github/config/megalinter.yaml\n\n          GITHUB_TOKEN: \"${{ secrets.GITHUB_TOKEN }}\" # report individual linter status\n          # Validate all source when push on main, else just the git diff with live.\n          VALIDATE_ALL_CODEBASE: &gt;-\n            ${{ github.event_name == 'push' &amp;&amp; github.ref == 'refs/heads/main'}}\n\n      # Upload MegaLinter artifacts\n      - name: Archive production artifacts\n        if: ${{ success() }} || ${{ failure() }}\n        uses: actions/upload-artifact@v3\n        with:\n          name: MegaLinter reports\n          path: |\n            megalinter-reports\n            mega-linter.log\n\n      # Summary and status\n      - run: echo \"\ud83c\udfa8 MegaLinter quality checks completed\"\n      - run: echo \"\ud83c\udf4f Job status is ${{ job.status }}.\"\n</code></pre>"},{"location":"continuous-integration/github/workflows/megalinter/#apply-fixes","title":"Apply Fixes","text":"<p>The MegaLinter workflow can also apply fixes it finds to pull requests and commit_message</p> <p>Applying fixes can make for confusing commits</p> <p>Using the <code>--fix</code> option with the  local MegaLinter runner is a more effective way to manage automatic fixes by MegaLinter, especially if code and configuration changes are staged or committed before automatically fixing.  Automatic fixes can then be discarded or treated as a separate commit using any Git tool.</p> MegaLinter Workflow with Apply Fixes .github/workflow/megalinter.yaml<pre><code>---\n# MegaLinter GitHub Action configuration file\n# More info at https://megalinter.github.io\n# All variables described in https://megalinter.github.io/configuration/\n\nname: MegaLinter\non:\n  workflow_dispatch:\n  pull_request:\n    branches: [main]\n  push:\n    branches: [main]\n\nenv:\n  # Apply linter fixes configuration\n  APPLY_FIXES: all # APPLY_FIXES must be defined as environment variable\n  APPLY_FIXES_EVENT: pull_request # events that trigger fixes on a commit or PR (pull_request, push, all)\n  APPLY_FIXES_MODE: pull_request # are fixes are directly committed (commit) or posted in a PR (pull_request)\n\n# Run Linters in parallel\n# Cancel running job if new job is triggered\nconcurrency:\n  group: \"${{ github.ref }}-${{ github.workflow }}\"\n  cancel-in-progress: true\n\njobs:\n  megalinter:\n    name: MegaLinter\n    runs-on: ubuntu-latest\n    steps:\n      - run: echo \"\ud83d\ude80 Job automatically triggered by ${{ github.event_name }}\"\n      - run: echo \"\ud83d\udc27 Job running on ${{ runner.os }} server\"\n      - run: echo \"\ud83d\udc19 Using ${{ github.ref }} branch from ${{ github.repository }} repository\"\n\n      # Git Checkout\n      - name: Checkout Code\n        uses: actions/checkout@v3\n        with:\n          token: \"${{ secrets.PAT || secrets.GITHUB_TOKEN }}\"\n          fetch-depth: 0\n      - run: echo \"\ud83d\udc19 ${{ github.repository }} repository was cloned to the runner.\"\n\n      # MegaLinter Configuration\n      - name: MegaLinter Run\n        id: ml\n        ## latest release of major version\n        uses: oxsecurity/megalinter/flavors/java@v6\n        env:\n          MEGALINTER_CONFIG: .github/config/megalinter.yaml\n          GITHUB_TOKEN: \"${{ secrets.GITHUB_TOKEN }}\" # report individual linter status\n          # Validate all source when push on main, else just the git diff with live.\n          VALIDATE_ALL_CODEBASE: &gt;-\n            ${{ github.event_name == 'push' &amp;&amp; github.ref == 'refs/heads/main'}}\n\n\n      # Upload MegaLinter artifacts\n      - name: Archive production artifacts\n        if: ${{ success() }} || ${{ failure() }}\n        uses: actions/upload-artifact@v2\n        with:\n          name: MegaLinter reports\n          path: |\n            megalinter-reports\n            mega-linter.log\n\n      # Create pull request if applicable (for now works only on PR from same repository, not from forks)\n      - name: Create Pull Request with applied fixes\n        id: cpr\n        if: steps.ml.outputs.has_updated_sources == 1 &amp;&amp; (env.APPLY_FIXES_EVENT == 'all' || env.APPLY_FIXES_EVENT == github.event_name) &amp;&amp; env.APPLY_FIXES_MODE == 'pull_request' &amp;&amp; (github.event_name == 'push' || github.event.pull_request.head.repo.full_name == github.repository) &amp;&amp; !contains(github.event.head_commit.message, 'skip fix')\n        uses: peter-evans/create-pull-request@v4\n        with:\n          token: ${{ secrets.PAT || secrets.GITHUB_TOKEN }}\n          commit-message: \"[MegaLinter] Apply linters automatic fixes\"\n          title: \"[MegaLinter] Apply linters automatic fixes\"\n          labels: bot\n      - name: Create PR output\n        if: steps.ml.outputs.has_updated_sources == 1 &amp;&amp; (env.APPLY_FIXES_EVENT == 'all' || env.APPLY_FIXES_EVENT == github.event_name) &amp;&amp; env.APPLY_FIXES_MODE == 'pull_request' &amp;&amp; (github.event_name == 'push' || github.event.pull_request.head.repo.full_name == github.repository) &amp;&amp; !contains(github.event.head_commit.message, 'skip fix')\n        run: |\n          echo \"Pull Request Number - ${{ steps.cpr.outputs.pull-request-number }}\"\n          echo \"Pull Request URL - ${{ steps.cpr.outputs.pull-request-url }}\"\n\n      # Push new commit if applicable (for now works only on PR from same repository, not from forks)\n      - name: Prepare commit\n        if: steps.ml.outputs.has_updated_sources == 1 &amp;&amp; (env.APPLY_FIXES_EVENT == 'all' || env.APPLY_FIXES_EVENT == github.event_name) &amp;&amp; env.APPLY_FIXES_MODE == 'commit' &amp;&amp; github.ref != 'refs/heads/main' &amp;&amp; (github.event_name == 'push' || github.event.pull_request.head.repo.full_name == github.repository) &amp;&amp; !contains(github.event.head_commit.message, 'skip fix')\n        run: sudo chown -Rc $UID .git/\n      - name: Commit and push applied linter fixes\n        if: steps.ml.outputs.has_updated_sources == 1 &amp;&amp; (env.APPLY_FIXES_EVENT == 'all' || env.APPLY_FIXES_EVENT == github.event_name) &amp;&amp; env.APPLY_FIXES_MODE == 'commit' &amp;&amp; github.ref != 'refs/heads/main' &amp;&amp; (github.event_name == 'push' || github.event.pull_request.head.repo.full_name == github.repository) &amp;&amp; !contains(github.event.head_commit.message, 'skip fix')\n        uses: stefanzweifel/git-auto-commit-action@v4\n        with:\n          branch: ${{ github.event.pull_request.head.ref || github.head_ref || github.ref }}\n          commit_message: \"[MegaLinter] Apply linters fixes\"\n\n\n      # Summary and status\n      - run: echo \"\ud83c\udfa8 MegaLinter quality checks completed\"\n      - run: echo \"\ud83c\udf4f Job status is ${{ job.status }}.\"\n</code></pre> <p>Reference: MegaLinter Configuration</p> <p>MegaLinter Installation defines a GitHub workflow which includes creating a commit or pull request to automatically apply fixes.</p>"},{"location":"continuous-integration/github/workflows/practicalli/","title":"Workflows for Practicalli","text":"<p>Practicalli books and other content websites use the following GitHub workflows.</p>"},{"location":"continuous-integration/github/workflows/practicalli/#megalinter","title":"MegaLinter","text":"<p> Practicalli MegaLinter workflow</p>"},{"location":"continuous-integration/github/workflows/practicalli/#changelog-update-check","title":"Changelog Update Check","text":"<p>Check the CHANGELOG.md file has been updated for a pull request, providing a reminder to add a summary of changes for the pull request</p> <p>Defines <code>changelog-check-skip</code> label on a pull request instructs the workflow not to run</p> <p>Changelog Checker</p> .github/workflows/changelog-check.yml<pre><code>---\n# Check CHANGELOG.md file updated for every pull request\n\nname: Changelog Check\non:\n  pull_request:\n    paths-ignore:\n      - \"README.md\"\n    types: [opened, synchronize, reopened, ready_for_review, labeled, unlabeled]\n\njobs:\n  changelog:\n    name: Changelog Update Check\n    runs-on: ubuntu-latest\n    steps:\n      - run: echo \"\ud83d\ude80 Job automatically triggered by ${{ github.event_name }}\"\n      - run: echo \"\ud83d\udc27 Job running on ${{ runner.os }} server\"\n      - run: echo \"\ud83d\udc19 Using ${{ github.ref }} branch from ${{ github.repository }} repository\"\n\n      # Git Checkout\n      - name: Checkout Code\n        uses: actions/checkout@v3\n        with:\n          token: \"${{ secrets.PAT || secrets.GITHUB_TOKEN }}\"\n      - run: echo \"\ud83d\udc19 ${{ github.repository }} repository was cloned to the runner.\"\n\n      # Changelog Enforcer\n      - name: Changelog Enforcer\n        uses: dangoslen/changelog-enforcer@v3\n        with:\n          changeLogPath: \"CHANGELOG.md\"\n          skipLabels: \"skip-changelog-check\"\n\n      # Summary and status\n      - run: echo \"\ud83c\udfa8 Changelog Enforcer quality checks completed\"\n      - run: echo \"\ud83c\udf4f Job status is ${{ job.status }}.\"\n</code></pre>"},{"location":"continuous-integration/github/workflows/practicalli/#clojure-lint-with-reviewdog","title":"Clojure Lint with Reviewdog","text":"<p>clj-kondo lint with reviewdog reports</p> <pre><code>---\n# Clojure Lint with clj-kondo and reviewdog\n#\n# Lint errors raised as comments on pull request conversation\n\nname: Lint Review\non: [pull_request]\n\njobs:\n  clj-kondo:\n    name: runner / clj-kondo\n    runs-on: ubuntu-latest\n    steps:\n      - run: echo \"\ud83d\ude80 Job automatically triggered by ${{ github.event_name }}\"\n      - run: echo \"\ud83d\udc27 Job running on ${{ runner.os }} server\"\n      - run: echo \"\ud83d\udc19 Using ${{ github.ref }} branch from ${{ github.repository }} repository\"\n\n      # Git Checkout\n      - name: Checkout Code\n        uses: actions/checkout@v3\n        with:\n          token: \"${{ secrets.PAT || secrets.GITHUB_TOKEN }}\"\n      - run: echo \"\ud83d\udc19 ${{ github.repository }} repository was cloned to the runner.\"\n\n      - name: clj-kondo\n        uses: nnichols/clojure-lint-action@v2\n        with:\n          pattern: \"*.clj\"\n          clj_kondo_config: \".clj-kondo/config-ci.edn\"\n          level: \"error\"\n          exclude: \".cljstyle\"\n          github_token: ${{ secrets.github_token }}\n          reporter: github-pr-review\n\n      # Summary and status\n      - run: echo \"\ud83c\udfa8 Lint Review checks completed\"\n      - run: echo \"\ud83c\udf4f Job status is ${{ job.status }}.\"\n</code></pre>"},{"location":"continuous-integration/github/workflows/practicalli/#clojure-quality-check","title":"Clojure quality check","text":"<ul> <li>clj-kondo syntax check for code and project configuration</li> <li>cljstyle code format check</li> <li>Kaocha unit test runner</li> </ul> <p>Clojure Quality Checks</p> <pre><code>---\nname: \"Clojure Quality Check\"\n\non:\n  pull_request:\n  push:\n    branches:\n      - main\n\njobs:\n  tests:\n    name: \"Clojure Quality Checks\"\n    runs-on: ubuntu-latest\n    steps:\n\n      # Git Checkout\n      - name: Checkout Code\n        uses: actions/checkout@v3\n      - run: echo \"\ud83d\udc19 ${{ github.repository }} repository was cloned to the runner.\"\n\n      - name: \"Prepare Java runtime\"\n        uses: actions/setup-java@v3\n        with:\n          distribution: \"temurin\"\n          java-version: \"17\"\n\n      - name: \"Cache Clojure Dependencies\"\n        uses: actions/cache@v3\n        with:\n          path: |\n            ~/.m2/repository\n            ~/.gitlibs\n          key: clojure-deps-${{ hashFiles('**/deps.edn') }}\n          restore-keys: clojure-deps-\n\n      - name: \"Install Clojure tools\"\n        uses: DeLaGuardo/setup-clojure@10\n        with:\n          cli: 1.11.1.1165 # Clojure CLI\n          cljstyle: 0.15.0 # cljstyle\n          clj-kondo: 2022.10.05 # Clj-kondo\n          # bb: 0.7.8           # Babashka\n\n      - name: \"Lint with clj-kondo\"\n        run: clj-kondo --lint deps.edn src resources test --config .clj-kondo/config-ci.edn\n\n      - name: \"Check Clojure Style\"\n        run: cljstyle check --report\n\n      - name: \"Kaocha test runner\"\n        run: clojure -X:env/test:test/run\n</code></pre>"},{"location":"continuous-integration/github/workflows/practicalli/#mkdocs-publisher","title":"mkdocs publisher","text":"<p>A workflow used to publish Practicalli books.</p> <ul> <li><code>workflow_dispatch:</code> for manual trigger of workflow</li> <li><code>workflow_run:</code> to depend on a successful run of the <code>MegaLinter</code> workflow</li> <li><code>paths-ignore</code> defining paths to ignore changes from</li> <li>actions/setup-python installs python version 3</li> <li><code>pip</code> to install Material for MkDocs packages used for Practialli books</li> </ul> <p>MkDocs Publish Book workflow</p> <pre><code>---\nname: Publish Book\non:\n  # Manually trigger workflow\n  workflow_dispatch:\n\n  # Run work flow conditional on linter workflow success\n  workflow_run:\n    workflows:\n      - \"MegaLinter\"\n    paths-ignore:\n      - README.md\n      - CHANGELOG.md\n      - .gitignore\n    branches:\n      - main\n    types:\n      - completed\n\npermissions:\n  contents: write\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - run: echo \"\ud83d\ude80 Job automatically triggered by ${{ github.event_name }}\"\n      - run: echo \"\ud83d\udc27 Job running on ${{ runner.os }} server\"\n      - run: echo \"\ud83d\udc19 Using ${{ github.ref }} branch from ${{ github.repository }} repository\"\n\n      - name: \"Checkout code\"\n        uses: actions/checkout@v3\n        with:\n          fetch-depth: 0\n      - run: echo \"\ud83d\udc19 ${{ github.repository }} repository was cloned to the runner.\"\n\n      # setup-python only required to use non-default python version\n      # - uses: actions/setup-python@v4\n      #   with:\n      #     python-version: 3.x\n      - uses: actions/cache@v3\n        with:\n          key: ${{ github.ref }}\n          path: .cache\n      - run: pip install mkdocs-material mkdocs-callouts mkdocs-glightbox mkdocs-git-revision-date-localized-plugin mkdocs-redirects pillow cairosvg\n      - run: mkdocs gh-deploy --force\n\n      # Summary\n      - run: echo \"\ud83c\udfa8 MkDocs book built and deployed to GitHub Pages\"\n      - run: echo \"\ud83c\udf4f Job status is ${{ job.status }}.\"\n</code></pre>"},{"location":"continuous-integration/github/workflows/practicalli/#scheduled-version-check","title":"Scheduled Version Check","text":"<p>Use liquidz/antq-action to check for new versions of Clojure libraries and GitHub action.</p> <p>The GtiHub action can use the following actions</p> <ul> <li><code>excludes:</code> list of space separated artefact names to exclude from the version check, use <code>groupId/artifactId</code> for Java libraries</li> <li><code>directories:</code> search paths to check, space separated.</li> <li><code>skips:</code> project types to skip to search, space separated. One of boot, clojure-cli, github-action, pom, shadow-cljs or leiningen.</li> </ul> <p><code>on: schedule: cron:</code> is used to set the frequency for running the workflow, using a POSIX cron syntax.</p> <p> GitHub Docs: GitHub Actions - schedule</p> <p>Scheduled Antq Version check with Manual Trigger</p> <pre><code>---\n# ------------------------------------------\n# Scheduled check of versions\n# - use as non-urgent report on versions\n# - Uses POSIX Cron syntax\n#   - Minute [0,59]\n#   - Hour [0,23]\n#   - Day of the month [1,31]\n#   - Month of the year [1,12]\n#   - Day of the week ([0,6] with 0=Sunday)\n#\n# Using liquidz/anta to check:\n# - GitHub workflows\n# - deps.edn\n# ------------------------------------------\n\nname: \"Scheduled Version Check\"\non:\n  schedule:\n    # - cron: \"0 4 * * *\" # at 04:04:04 ever day\n    - cron: \"0 4 * * 5\" # at 04:04:04 ever Friday\n    # - cron: \"0 4 1 * *\" # at 04:04:04 on first day of month\n  workflow_dispatch: # Run manually via GitHub Actions Workflow page\n\njobs:\n  scheduled-version-check:\n    name: \"Scheduled Version Check\"\n    runs-on: ubuntu-latest\n    steps:\n      - run: echo \"\ud83d\ude80 Job automatically triggered by ${{ github.event_name }}\"\n      - run: echo \"\ud83d\udc27 Job running on ${{ runner.os }} server\"\n      - run: echo \"\ud83d\udc19 Using ${{ github.ref }} branch from ${{ github.repository }} repository\"\n\n      - name: \"Checkout code\"\n        uses: actions/checkout@v3\n      - run: echo \"\ud83d\udc19 ${{ github.repository }} repository was cloned to the runner.\"\n\n      - name: \"Antq Version Check\"\n        uses: liquidz/antq-action@main\n        with:\n          excludes: \"org.clojure/tools.deps.alpha\"\n          # excludes: \"qualifier/libary-name groupId/artifactId\"\n          # directories: \"search/path/1 search/path/2\"\n          # skips: \"boot clojure-cli github-action pom shadow-cljs leiningen\"\n\n      # Summary\n      - run: echo \"\ud83c\udfa8 library versions checked with liquidz/antq\"\n      - run: echo \"\ud83c\udf4f Job status is ${{ job.status }}.\"\n</code></pre>"},{"location":"continuous-integration/github/workflows/system-catalog/","title":"System catalog","text":""},{"location":"continuous-integration/github/workflows/system-catalog/#backstageio","title":"Backstage.io","text":"<p>Backstage is an open platform for building developer portals, using a centralised software catalog.</p> <p>The Backstage Validator workflow checks the Backstage configuration of the current project.</p> <p>Validate Backstage.io configuration files</p> <pre><code>---\n# --- Validate Backstage.io configuration files ---#\n# https://github.com/marketplace/actions/backstage-entity-validator\n\n# trigger workflow if yaml files `.backstage/` directory updated\nname: Backstage Validator\non:\n  pull_request:\n    paths:\n      - \".backstage/\"\njobs:\n  # Validate backstage configuration\n  backstage-validator:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - uses: RoadieHQ/backstage-entity-validator@v0.3.2\n        with:\n          path: \".backstage/*.yaml\"\n</code></pre>"},{"location":"culture/","title":"Culture","text":"<p>Open, honest and transparent communication is the foundation to developing and maintaining a culture for engineering teams and the wider organisation.</p> <p>The culture of an organisation comes from the sum of its parts, the people and their actions.</p> <p>This is the way - Mandolorian Culture, Star Wars</p> <p>A thriving culture is demonstrated by the way things are naturally done by the engineering team.</p> <p>Unconstructive actions would also naturally feel the incorrect way.</p> Code of Conduct <p>Establishing a code of conduct sets the environment in which people interact with each other.</p> <p>The code should define expectations of positive and constructive behaviour an guide people away from innappropriate and offensive behaviour.</p> <p>The code often aludes to the type of language that should be used when interacting with others, either verbal, written or body language.</p>"},{"location":"culture/#create-a-culture","title":"Create a culture","text":"<p>A culture should be deliberately designed and cultivated.</p> <p>Culture as a Strategy - too important to leave to chance. - Maria Campbell</p> <p>Create an environment for culture to grow and encourage collaboration so that culture becomes part of everyones daily flow.</p> <ul> <li>discuss the concept of culture and why its valuable</li> <li>identify important cultural aspects (individual, team, organisation)</li> <li>share &amp; ingrain cultural aspects throughout work and interaction</li> <li>continually encourage cultural discussions and feedback to reinforce adoption</li> </ul> <p>Inception of a Culture - John Stevenson, Practicalli</p> <p>Inception of a Culture to everyone in an organisation would make adoption simpler (but not like it works in the movie)</p> Culture alignment over carbon copy <p>Culture can be viewed as a living entity and as with biological species a culture must be open to growing and adapting to changing circumstances whist still remaining true to its core values and ideals.</p> <p>When people are only hired with carbon copies of existing culture there is no room for growth and culture can become stifling and too much of a constraint.  When significant change has to be managed by this carbon copy culture, there is a far greater risk of failure.</p>"},{"location":"culture/#be-the-change-you-seek","title":"Be the change you seek","text":"<p>When a person believes a benefitial change can be made to culture then they should feel able to drive that change.</p> <p>Initial feedback on the the value and context of a change should be gained, helping form how that change can be effectively adopted.</p>"},{"location":"culture/anti-patterns/","title":"Culture Anti-patterns","text":"<p>Most anti-patterns stem from attempts to control or dictate actions of people within the organisation.</p> <p>Cultural adoption and effectiveness comes from collaboration and deep feeling of inclusion in defining the organisational culture.</p> <p>Initial draft - feedback welcome</p>"},{"location":"culture/anti-patterns/#perk-culture","title":"Perk Culture","text":"<p>Introducing perks like video game consoles, pool tables, etc can provide some basic interaction for people in the organisation.</p> <p>Perks are separate from the work that people are doing so serve as a distraction to help people wind-down.</p> <p>Howerver, perks alone are very limited as they are not connected to the work and not really part of an effective culture.  </p> <p>Perks can also be a source of division and conflict, especially when they are made overly competitive or not equally accessible.</p>"},{"location":"culture/anti-patterns/#imposed-culture","title":"Imposed Culture","text":"<p>Top-down cultural ideas may start a conversation within an organisation about the culture they value.</p> <p>Top-down cultural decrees impose specific cultural aspects that are often rejected or ignored by the organisation because they are not relevant to individual people within the organisation.</p> <p>Imposing a culture will stifle the adoption of cultural ideas and turn people away from discussing cultural aspects that are relevant.</p>"},{"location":"culture/anti-patterns/#homogenious-culture","title":"Homogenious culture","text":"<p>Cultural homogenization is the reduction in cultural diversity through popularization of customs, ideas and values.</p> <p>When culture is the same then everyone must act the same, leaving no room for cultural growth.</p> <p>As an organisation grows it may try too hard to maintain a particular culture, to the exclusion and detrement of other cultural ideas.  This can go as far as new hires being a carbon copy of existing people, which can stifle innovation and limit resilience to change.</p> Globalisation <p>Cultural homogenization is a central aspect of a global culture.</p> <p>Homogenization can breakdown cultural barriers and yet force the global adoption of a single culture, removing different identities and culture to form a mono-culture.</p> <p>David E. O'Connor defines cultural homogenization as \"the process by which local cultures are transformed or absorbed by a dominant outside culture\".</p> <p>Globalisation is used in the context of Western culture dominating and destroying other cultures.  However, cultural homogenization is not one-way and involves many cultures mixing with people adopting various elements from other cultures.</p>"},{"location":"culture/anti-patterns/#blame-culture","title":"Blame Culture","text":"<p>When people resourt to blaming others for failure, this is a blame culture.</p> <p>Blame becomes systemic within the organisation and it becomes easier to pass the failure off as someone elses fault.</p> <p>Managers may blame the people they are responsible for when</p> <ul> <li>the manager does not properly understand the type or amount of work involved for the people they manage</li> <li>poorly support those people to carry out realistic amount of work</li> <li>micro-manage people</li> <li>assiging conflicting priorities without any sense of value</li> <li>just get it done approach</li> <li>separate from the day to day work</li> </ul> <p>Its Not A Blame Culture</p> <p>Managers and staff start saying \"this isnt a blame culture\" when the organisation has alread decended into blame culture.</p> <p>When an organisation is truely not a blame culture, then there should not be a need for anyone to say, \"this isnt a blame culture\"</p>"},{"location":"culture/anti-patterns/#individual-blame","title":"individual blame","text":"<p>When mistakes do occur then it is far too simple to blame the individual that was the trigger (e.g. the person who deleted a database table in production)</p> <p>When discussing individuals involved in mistakes, it should always be framed as the action that was at fault and not the person carrying out that action.</p> <p>The system a person was able to make a mistake in was the actual root cause of the issue.</p> <p>The events that lead up to the mistake happening should be reviewed as part of a root cause analysis.  This should include technical, process and cultural aspects as potential influencers of the mistake.</p>"},{"location":"culture/anti-patterns/#a-culture-change-that-isnt","title":"A Culture change that isnt","text":"<p>Change that doesnt actually change anything, but often has new words for things instead.</p> <p>An insurance company realised they were drastically loosing out against their competition, dropping from a strong 1st to a weak 6th (out of 6) in the last year.</p> <p>The Prince2 process was percieved as one of the constraints to success for the company, so they scheduled a two day workshop to devise a more effective process that worked for all</p> <p>Whist there were representatives from across the company, the workshop predominantly consisted of project managers (most of their projects were blocked or being implemented by engineering teams)</p> <p>The workshop was structured and facilitated and many great ideas were discussed and presented.</p> <p>By the end of the workshop a new process had been created, which when looked at with dispationate eyes was mostly Prince2 with a few nods to agile terms.  So infact nothing had actually changed, except for introducing a new name.</p> <p>I asked one of the main internal stakeholders (in charge of overall direction) what was the unique selling point of the company.  This question seems to catch the stakeholder off-guard, even though I had assumed it should be forefront in peoples minds.</p> <p>Eventually the stakeholder said: \"Its just insurance, there isnt any difference\"</p> <p>It was a few days after that revelation I started planning on an exit strategy from the company.  Six months later, I had left and out of the original 164 people in the IT department, only 9 people remained.  The company had been aquired by stakeholders who knew what they valued, selling off much of the company they did not value.</p> <p>Macheaveleon Culture</p> <p>A focus on gaining individual power through manipulation and control, with the precept that it is for the greater prosperity of those being controlled.</p> <p>The Machiavellian Manager - lead with cunning &amp; Wisdom </p> <p>how to deal with a Machiavellian boss </p> <p>Adopting Machiavellian Survival Tactics </p> <p>Machiavellian Leader Effectiveness: The Moderating Role of Political Skill </p>"},{"location":"culture/burnout/","title":"Burnout","text":"<p>Burnout</p> <p>Being tired after a long day</p> <p>When a team must push themselves to the limit, it is vital to undersand what that limit is</p>"},{"location":"culture/burnout/#identify","title":"Identify","text":"<p>It is very hard for a person to recognise and acknolwlege burnout in themselves, especially in the critical early stages.</p>"},{"location":"culture/burnout/#-rest-does-not-make-a-person-feel-reseted","title":"- rest does not make a person feel reseted","text":""},{"location":"culture/community/","title":"Community","text":""},{"location":"culture/community/#building","title":"Building","text":"<p>Building a community is very rewarding and a useful exercise of leadership skills.</p> If you build it, they will come"},{"location":"culture/documentation/","title":"Documentation Culture","text":"<p>The written word is a very effective way to communicate and has been for many thousands of years of human culture.</p> <p>Writing documentation is a skill and therefore benefits with practice and regular use.</p> <p>Writing should be a natural activity that is highly valued as a perminant record of communication.</p>"},{"location":"culture/documentation/#personal-journal","title":"Personal Journal","text":"<p>For the writer of the journal, there is a feeling of accomplishment as well as a record of challenges they have been facing.</p> <p>For the rest of the organisations journals highlight</p> <p>Share Daily Journal</p> <p>Use the organisation documentation tool to share a daily journal, sharing challenges, lessons learned.</p> Personal Journal as practice <p>Writing every day in a journal provides lots of practice in the art of writing and a</p>"},{"location":"culture/documentation/#design-journal","title":"Design Journal","text":"<p>Capture design decisions as they are made, indicating all the options considered and why a particular choice was made.</p> <p>!!! \"Clojure Code journal\"     In expressive languages like Clojure, it is common to capture evolving designs and alternative options in souce code.</p> <pre><code>Clojure uses the `(comment ,,,)` expression to contain experimental code and keep it separate from production code.  Short design journals may appear at the end of the namespace they belong too.  Longer design journals are defined in their own namespace, often under the `dev` directory.\n\n[:fontawesome-solid-book-open: Practicalli Clojure REPL workflow - Design Journal](https://practical.li/clojure/introduction/repl-workflow/#design-journal){target=_blank .md-button}\n</code></pre> <p>Architecture Decison Record</p> <p> Architecture Decison Record (ADR) captures all the information around a single decision and complements a more general design journal for an engineering team.</p>"},{"location":"culture/documentation/#technical-writing","title":"Technical writing","text":"<p>Sharing how to write engaging documents that cover the relevant technical detail required for understanding.</p> <p>Playbooks</p> <p>Create specific guides to common tasks for the team, to share collective knowledge and greatly increase effectiveness across the organisation.</p>"},{"location":"culture/leadership/","title":"Leadership","text":"<p>Leadership is about owning the responsibility for the collective actions of a group or organisation as a whole.</p> <p>Leaders should be inspirational and motivational, encouraging the people they lead to use and maximise their skills for the benefit of the organisation and themselves.</p> <p>An effective leader:</p> <ul> <li>exudes empathy for others which increases levels of communication and trust</li> <li>has situational awareness and takes an holistic approach to challenges and solutions</li> <li>effectively communicates vision and direction across all people and teams</li> <li>a source of inspiration to all those around them</li> <li>supports people to bring out the best of them, increasing employee satisfaction</li> </ul> <p>An organisation becomes more effective when leaders can take a predominantly servant-leader approach role, dedicating themselves to those they are responsible for.</p> <p>Red berries - Leading by example</p> <p>Monkeys are afraid to eat red berries until they see another monkey eat them and see that they are safe.</p> <p>A leader should do actions and behaviour they wish to see in others as an effective way to encourage those they lead to do the same.</p> Anti-pattern: Control <p>It is an illustion that people can be controllled, certainly not without violent action.</p> <p>Actions that attempt to drive control over others distroy trust and greatly limit communication.  Given all collaborative work relies on trust and communication, that work will eventually fail.</p> <p>Micro-management demonstrates a derth of trust from leaders to those they should otherwise be nurturing.</p> Anti-patter: Ego <p>Leadership should not be about one persons ego or career progression.</p> <p>Excessive ego is a sign of weakness.</p> <p>Taking sole credit for others work is an insideous action and such behaviour doesnt belong anywhere near a leadership position (or any one who considers themselves a decent human being)</p> Ten Common Leadership Styles <p>Coach drives motivation</p> <p>Visonary progress focused and inspirational</p> <p>Servant is hunble and protective</p> <p>Autocratic authoritarian and result-focused</p> <p>Laissez-far (hands-off) autocratic and delegatory</p> <p>Democratic supportive and innovative</p> <p>Pacesetter helpful and motivational</p> <p>Transformational challenging and communicative</p> <p>Transactional performace focused</p> <p>Bureaucratic hierachical and duty-focused</p>"},{"location":"culture/leadership/#servant-leadership","title":"Servant leadership","text":"<p>Leaders are there for the people they lead, putting the needs and success of those they lead first and foremost.</p> <p>A servant leader shares power, puts the needs of the employees first and helps people develop and perform as highly as possible.[1] Instead of the people working to serve the leader, the leader exists to serve the people.</p> <p>Stewards who are entrusted to develop and empower followers to reach their fullest potential</p> <p>The traits of a servant leader:</p> <ul> <li>empathy &amp; deep listening</li> <li>foresight and awareness</li> <li>stewardship, persuasion &amp; healing</li> <li>commitment to the growth of people</li> <li>building community</li> </ul> <p> Wikipedia - Servant Leadership</p> <p> Atlassian - Putting People First: Servant Leadership</p>"},{"location":"introduction/contributing/","title":"Contributing to Practicalli","text":"<p>Practicalli books are written in markdown and use MkDocs to generate the published website via a GitHub workflow.  MkDocs can also run a local server using the <code>make docs</code> target from the <code>Makefile</code></p> <p>By submitting content ideas and corrections you are agreeing they can be used in this workshop under the Creative Commons Attribution ShareAlike 4.0 International license.  Attribution will be detailed via GitHub contributors.</p> <p>All content and interaction with any persons or systems must be done so with respect and within the Practicalli Code of Conduct.</p>"},{"location":"introduction/contributing/#book-status","title":"Book status","text":""},{"location":"introduction/contributing/#book-source-code","title":"Book Source code","text":"<p>practicalli/engineering-playbook repository contains the content for this book</p> HTTPSSSH <pre><code>git clone https://github.com/practicalli/engineering-playbook.git\n</code></pre> <pre><code>git clone git@github.com:practicalli/engineering-playbook.git\n</code></pre>"},{"location":"introduction/contributing/#submit-and-issue-or-idea","title":"Submit and issue or idea","text":"<p>If something doesnt seem quite right or something is missing from the book, please raise an issue via the GitHub repository explaining in as much detail as you can.</p> <p>Raising an issue before creating a pull request will save you and the maintainer time.</p>"},{"location":"introduction/contributing/#considering-a-pull-request","title":"Considering a Pull request?","text":"<p>Pull Request Commits must be cryptographically signed</p> <p>All commits contributed to Practicalli must be signed via a legitimate SSH or GPG key to avoid the risk of commit spoofing.</p> <p>Configure commit signing with SSH key - Practicalli Engineering</p> <p>All pull requests must include an entry in CHANGELOG.md or will not be merged.  A changelog entry allows the community to follow the changes to the book.</p> <p>Each pull request will have a number of CI workflows run against the contribution, checking the format of the content and if a changelog entry has been provided.</p> <p>Please keep pull requests small and focused, as they are much quicker to review and easier to accept.  Ideally PR's should be for a specific page or at most a section.</p> <p>A PR with a list of changes across different sections will be closed without merging as these take considerable time to review.</p> <p>Issues such as grammar improvements are typically a sign of a rushed section that requires a rewrite, so a pull request to fix a typeographic error will probably not be merged.  Raise an issue, or post a thread in the  Clojurians Slack #practicall channel</p>"},{"location":"introduction/contributing/#thank-you-to-everyone-that-has-contributed","title":"Thank you to everyone that has contributed","text":"<p>A huge thank you to Rich Hickey and the team at Cognitect for creating and continually guiding the Clojure language.  Special thank you to Alex Miller who has provided excellent advice on working with Clojure and the CLI tooling.</p> <p>The Clojure community has been highly supportive of everyone using Clojure and I'd like to thank everyone for the feedback and contributions.  I would also like to thank everyone that has joined in with the London Clojurins community, ClojureBridgeLondon, Clojurians Slack community, Clojurians Zulip community and Clojureverse community.</p> <p>Thank you to everyone who sponsors the Practicalli websites and videos and for the Clojurists Together sponsorship, it helps me continue the work at a much faster pace.</p> <p>Special thanks to Bruce Durling for getting me into Cloure in the first place.</p> <p></p>"},{"location":"introduction/writing-tips/","title":"Writing tips for MkDocs","text":"<p>Making the docs more engaging using the mkdocs-material theme reference guide</p> Configuring Colors <p>Material for MkDocs - Changing the colors lists the primary and accent colors available.</p> <p>HSL Color Picker for codes to modify the theme style, overriding colors in <code>docs/assets/stylesheets/extra.css</code></p>"},{"location":"introduction/writing-tips/#hypertext-links","title":"Hypertext links","text":"<p>Links open in the same browser window/tab by default.</p> <p>Add <code>{target=_blank}</code> to the end of a link to configure opening in a new tab</p> <pre><code>[link text](url){target=_blank}\n</code></pre>"},{"location":"introduction/writing-tips/#buttons","title":"Buttons","text":"<p>Convert any link into a button by adding <code>{.md-button}</code> class names to end of the markdown for a link, which uses <code>.md-button-primary</code> by default.  Include <code>target=_blank</code> for buttons with links to external sites.</p> <pre><code>[link text](http://practical.li/blog){.md-button target=_blank}\n</code></pre> <p>Or specify a different class</p> <pre><code>[link text](http://practical.li/blog){.md-button .md-button-primary}\n</code></pre> <p>Add an icon to the button</p> <p> Practicalli Issues  Practicalli Blog</p> <pre><code>[:fontawesome-brands-github: Practicalli Issues](http://practical.li/blog){ .md-button .md-button-primary }\n[:octicons-heart-fill-24: Practicalli Blog](http://practical.li/blog){ .md-button .md-button-primary }\n</code></pre> <p>Search all supported icons</p>"},{"location":"introduction/writing-tips/#youtube-video","title":"YouTube video","text":"<p>Use an iframe element to include a YouTube video, wrapping in a paragraph tag with center alignment to place the video in a centered horizontal position</p> <pre><code>&lt;p style=\"text-align:center\"&gt;\n&lt;iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/rQ802kSaip4\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen&gt;&lt;/iframe&gt;\n&lt;/p&gt;\n</code></pre> <p>mkdocs material does not have direct support for adding a YouTube video via markdown.</p>"},{"location":"introduction/writing-tips/#admonitions","title":"Admonitions","text":"<p>Supported admonition types</p> <p>Note</p> <p>Use <code>!!!</code> followed by <code>NOTE</code></p> <p>Adding a title</p> <p>Use <code>!!!</code> followed by <code>NOTE</code> and a <code>\"title in double quotes\"</code></p> <p>Shh, no title bar just the text... Use <code>!!!</code> followed by <code>NOTE</code> and a <code>\"\"</code> empty double quotes</p> <p>Abstract</p> <p>Use <code>!!!</code> followed by <code>ABSTRACT</code></p> <p>Info</p> <p>Use <code>!!!</code> followed by <code>INFO</code></p> <p>Tip</p> <p>Use <code>!!!</code> followed by <code>TIP</code></p> <p>Success</p> <p>Use <code>!!!</code> followed by <code>SUCCESS</code></p> <p>Question</p> <p>Use <code>!!!</code> followed by <code>QUESTION</code></p> <p>Warning</p> <p>Use <code>!!!</code> followed by <code>WARNING</code></p> <p>Failure</p> <p>Use <code>!!!</code> followed by <code>FAILURE</code></p> <p>Danger</p> <p>Use <code>!!!</code> followed by <code>DANGER</code></p> <p>Bug</p> <p>Use <code>!!!</code> followed by <code>BUG</code></p> <p>Example</p> <p>Use <code>!!!</code> followed by <code>EXAMPLE</code></p> <p>Quote</p> <p>Use <code>!!!</code> followed by <code>QUOTE</code></p>"},{"location":"introduction/writing-tips/#collapsing-admonitions","title":"Collapsing admonitions","text":"Note <p>Collapse those admonitions using <code>???</code> instead of <code>!!!</code></p> Replace with a title <p>Use <code>???</code> followed by <code>NOTE</code> and a <code>\"title in double quotes\"</code></p> Expanded by default <p>Use <code>???+</code>, note the <code>+</code> character,  followed by <code>NOTE</code> and a <code>\"title in double quotes\"</code></p>"},{"location":"introduction/writing-tips/#inline-blocks","title":"Inline blocks","text":"<p>Inline blocks of text to make a very specific callout within text</p> <p>Info</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Adding something to then end of text is probably my favourite</p> <p>Info</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p>"},{"location":"introduction/writing-tips/#code-blocks","title":"Code blocks","text":"<p>Code blocks include a copy icon automatically</p> <p>Syntax highlighting in code blocks</p> <pre><code>(defn my-function  ; Write a simple function\n  \"With a lovely doc-string\"\n  [arguments]\n  (map inc [1 2 3]))\n</code></pre> <p>Give the code block a title using <code>title=\"\"</code> after the backtics and language name</p> src/practicalli/gameboard.clj<pre><code>(defn my-function\n  \"With a lovely doc-string\"\n  [arguments]\n  (map inc [1 2 3]))\n</code></pre> <p>We all like line numbers, especially when you can set the starting line</p> src/practicalli/gameboard.clj<pre><code>(defn my-function\n  \"With a lovely doc-string\"\n  [arguments]\n  (map inc [1 2 3]))\n</code></pre> <p>Add <code>linenums=42</code> to start line numbers from 42 onward</p> <pre><code>clojure linenums=\"42\" title=\"src/practicalli/gameboard.clj\"\n</code></pre>"},{"location":"introduction/writing-tips/#annotations","title":"Annotations","text":"<p>Annotations in a code block help to highlight important aspects.  Use the comment character for the language followed by a space and a number in brackets</p> <p>For example, in a shell code block, use <code># (1)</code> where 1 is the number of the annotation</p> <p>Use a number after the code block to add the text for the annotation, e.g. <code>1.</code>. Ensure there is a space between the code block and the annotation text.</p> <pre><code>ls -la $HOME/Downloads  # (1)\n</code></pre> <ol> <li> I'm a code annotation! I can contain <code>code</code>, formatted text, images, ... basically anything that can be written in Markdown.</li> </ol> <p>Code blocks with annotation, add <code>!</code> after the annotation number to suppress the <code>#</code> character</p> <pre><code>(defn helper-function\n  \"Doc-string with description of function purpose\" ; (1)!\n  [data]\n  (merge {:fish 1} data)\n  )\n</code></pre> <ol> <li>Always include a doc-string in every function to describe the purpose of that function, identifying why it was added and what its value is.</li> </ol> <p>GitHub action example with multiple annotations</p> <pre><code>name: ci # (1)!\non:\n  push:\n    branches:\n      - master # (2)!\n      - main\npermissions:\n  contents: write\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - uses: actions/setup-python@v4\n        with:\n          python-version: 3.x\n      - run: pip install mkdocs-material # (3)!\n      - run: mkdocs gh-deploy --force\n</code></pre> <ol> <li> <p>You can change the name to your liking.</p> </li> <li> <p>At some point, GitHub renamed <code>master</code> to <code>main</code>. If your default branch     is named <code>master</code>, you can safely remove <code>main</code>, vice versa.</p> </li> <li> <p>This is the place to install further [MkDocs plugins] or Markdown     extensions with <code>pip</code> to be used during the build:</p> <pre><code>pip install \\\n  mkdocs-material \\\n  mkdocs-awesome-pages-plugin \\\n  ...\n</code></pre> </li> </ol>"},{"location":"introduction/writing-tips/#highlight-lines-in-code-blocks","title":"Highlight lines in code blocks","text":"<p>Add highlight line meta data to a code block after the opening backticks and code block language.</p> <p><code>hl_lines=\"2\"</code> highlights line 2 in the codeblock</p> <pre><code>(defn my-function\n  \"With a lovely doc-string\"\n  [arguments]\n  (map\n   inc\n   [1 2 3]))\n</code></pre>"},{"location":"introduction/writing-tips/#embed-external-files","title":"Embed external files","text":"<p><code>--8&lt;--</code> in a code block inserts code from a source code file or other text file</p> <p>Specify a local file from the root of the book project (the directory containing mkdocs.yml)</p> Scheduled Version Check GitHub Workflow from source code file scheduled version check<pre><code>\n</code></pre> Practicalli Project Templates Emacs project configuration - .dir-locals.el<pre><code>((clojure-mode . ((cider-preferred-build-tool . clojure-cli)\n                  (cider-clojure-cli-aliases . \":test/env:dev/reloaded\"))))\n</code></pre> <p>Code example reuse</p> <p>Use an embedded local or external file (URL) when the same content is required in more than one place in the book.</p> <p>An effective way of sharing code and configuration mutliple times in a book or across multiple books.</p>"},{"location":"introduction/writing-tips/#content-tabs","title":"Content tabs","text":"<p>Create in page tabs that can also be</p> <p>Setting up a project</p> Clojure CLILeiningen <pre><code>clojure -T:project/new :template app :name practicalli/gameboard\n</code></pre> <pre><code>lein new app practicalli/gameboard\n</code></pre> <p>Or nest the content tabs in an admonition</p> <p>Run a terminal REPL</p> Clojure CLILeiningen <pre><code>clojure -T:repl/rebel\n</code></pre> <pre><code>lein repl\n</code></pre>"},{"location":"introduction/writing-tips/#diagrams","title":"Diagrams","text":"<p>Neat flow diagrams</p> <p>Diagrams - Material for MkDocs</p> <pre><code>graph LR\n  A[Start] --&gt; B{Error?};\n  B --&gt;|Yes| C[Hmm...];\n  C --&gt; D[Debug];\n  D --&gt; B;\n  B ----&gt;|No| E[Yay!];</code></pre> <p>UML Sequence Diagrams</p> <pre><code>sequenceDiagram\n  Alice-&gt;&gt;John: Hello John, how are you?\n  loop Healthcheck\n      John-&gt;&gt;John: Fight against hypochondria\n  end\n  Note right of John: Rational thoughts!\n  John--&gt;&gt;Alice: Great!\n  John-&gt;&gt;Bob: How about you?\n  Bob--&gt;&gt;John: Jolly good!</code></pre> <p>state transition diagrams</p> <pre><code>stateDiagram-v2\n  state fork_state &lt;&lt;fork&gt;&gt;\n    [*] --&gt; fork_state\n    fork_state --&gt; State2\n    fork_state --&gt; State3\n\n    state join_state &lt;&lt;join&gt;&gt;\n    State2 --&gt; join_state\n    State3 --&gt; join_state\n    join_state --&gt; State4\n    State4 --&gt; [*]</code></pre> <p>Class diagrams - not needed for Clojure</p> <p>Entity relationship diagrams are handy though</p> <pre><code>erDiagram\n  CUSTOMER ||--o{ ORDER : places\n  ORDER ||--|{ LINE-ITEM : contains\n  LINE-ITEM {\n    customer-name string\n    unit-price int\n  }\n  CUSTOMER }|..|{ DELIVERY-ADDRESS : uses</code></pre>"},{"location":"introduction/writing-tips/#keyboard-keys","title":"Keyboard keys","text":"<p>Represent key bindings with Keyboard keys. Each number and alphabet character has their own key.</p> <ul> <li>1 <code>++1++</code> for numbers</li> <li>l <code>++\"l\"++</code> for lowercase character</li> <li>U <code>++u++</code> for uppercase character or <code>++\"U\"++</code> for consistency</li> </ul> <p>Punctionation keys use their name</p> <ul> <li>Space <code>++spc++</code></li> <li>, <code>++comma++</code></li> <li>Left <code>++arrow-left++</code></li> </ul> <p>For key sequences, place a space between each keyboard character</p> <ul> <li>Space g s <code>++spc++ ++\"g\"++ ++\"s\"++</code></li> </ul> <p>For key combinations, use join they key identifies with a <code>+</code></p> <ul> <li>Meta+X <code>++meta+x++</code></li> <li>Ctrl+Alt+Del <code>++ctrl+alt+del++</code></li> </ul> <p>MkDocs keyboard keys reference</p>"},{"location":"introduction/writing-tips/#images","title":"Images","text":"<p>Markdown images can be appended with material tags to set the size of the image, whether to appear on light or dark theme and support lazy image loading in browsers</p> SizeLazy LoadingAlignTheme SpecificAll Image Attributes <p><code>{style=\"height:150px;width:150px\"}</code> specifies the image size <pre><code>![Kitty Logo](https://raw.githubusercontent.com/practicalli/graphic-design/live/icons/kitty-light.png#only-dark){style=\"height:150px;width:150px\"}\n</code></pre></p> <p></p> <p><code>{loading=lazy}</code> specifies an image should lazily load in the browser <pre><code>![Kitty Logo](https://raw.githubusercontent.com/practicalli/graphic-design/live/icons/kitty-light.png){loading=lazy}\n</code></pre></p> <p><code>{aligh=left}</code> or <code>{aligh=right}</code> specifies the page alignment of an image. <pre><code>![Kitty Logo](https://raw.githubusercontent.com/practicalli/graphic-design/live/icons/kitty-light.png#only-dark){align=right}\n![Kitty Logo](https://raw.githubusercontent.com/practicalli/graphic-design/live/icons/kitty-dark.png#only-light){align=right}\n</code></pre></p> <p>  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p><code>![Kitty Logo](image/kitty-light.png#only-dark)</code> or <code>![Kitty Logo](image/kitty-light.png#only-light)</code>  specifies the theme the image should be shown, allowing different versions of images to be shown based on the theme. <pre><code>![Kitty Logo](https://raw.githubusercontent.com/practicalli/graphic-design/live/icons/kitty-light.png#only-dark){style=\"height:150px;width:150px\"}\n![Kitty Logo](https://raw.githubusercontent.com/practicalli/graphic-design/live/icons/kitty-dark.png#only-light){style=\"height:150px;width:150px\"}\n</code></pre> Use the theme toggle in the top nav bar to see the icon change between light and dark.  </p> <p>Requires the color pallet toggle</p> <p>Alight right, lazy load and set image to 150x150</p> <pre><code>![Kitty Logo](https://raw.githubusercontent.com/practicalli/graphic-design/live/icons/kitty-light.png#only-dark){align=right loading=lazy style=\"height:64px;width:64px\"}\n![Kitty Logo](https://raw.githubusercontent.com/practicalli/graphic-design/live/icons/kitty-dark.png#only-light){align=right loading=lazy style=\"height:64px;width:64px\"}\n</code></pre> <p>  Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p>"},{"location":"introduction/writing-tips/#lists","title":"Lists","text":"<p>Task lists</p> <ul> <li> Lorem ipsum dolor sit amet, consectetur adipiscing elit</li> <li> Vestibulum convallis sit amet nisi a tincidunt<ul> <li> In hac habitasse platea dictumst</li> <li> In scelerisque nibh non dolor mollis congue sed et metus</li> <li> Praesent sed risus massa</li> </ul> </li> <li> Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque</li> </ul> <p>Task List example</p> <pre><code>- [x] Lorem ipsum dolor sit amet, consectetur adipiscing elit\n- [ ] Vestibulum convallis sit amet nisi a tincidunt\n    * [x] In hac habitasse platea dictumst\n    * [x] In scelerisque nibh non dolor mollis congue sed et metus\n    * [ ] Praesent sed risus massa\n- [ ] Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque\n</code></pre>"},{"location":"introduction/writing-tips/#tooltips","title":"Tooltips","text":"<p>The humble tool tip</p> <p>Hover me</p> <p>with references</p> <p>Hover me</p> <p>Icon tool tip with a title</p> <p></p>"},{"location":"introduction/writing-tips/#abreviations","title":"Abreviations","text":"<p>The HTML specification is maintained by the W3C.</p> <p>[HTML]: Hyper Text Markup Language [W3C]: World Wide Web Consortium</p>"},{"location":"introduction/writing-tips/#magic-links","title":"Magic links","text":"<p>MagicLink can auto-link HTML, FTP, and email links. It can auto-convert repository links (GitHub, GitLab, and Bitbucket) and display them in a more concise, shorthand format.</p> <p>Email Practicalli</p> <p>Practicalli Neovim</p>"},{"location":"os/","title":"Operating Systems","text":"<p>Quote</p> <p>Without an operating system, the computer is an expensive piece of electronics.</p> <p>Guides to install and configure operating systems to support software development, graphic design and video editing.</p>"},{"location":"os/#linux-distributions","title":"Linux Distributions","text":"<p>A Linux distribution is an assembly of software packaged to work together seemlessly (or atleast avoid conflicts).</p> <p>There are a wide range of Linux distributions to choose from, although Practicalli recommends Ubuntu or Debian Linux distributions.</p> <p>Arch Linux has solid community support should a more involved operating system experience be desirable.</p> <p>Regolith Desktop Debian Linux Ubuntu Desktop</p> <p>Practicalli recommends Linux</p> <p>Practicalli recommends use of a Linux distribution to provide a stable and developer focused environment to work in.  The vast majority is freely available without cost and typically of high quality thanks to so many collaborators and users.</p> <p>Linux also has a quite minimal hardware resource use and has very widespread support for new and classic hardware.</p> <p>Unix/BSD based systems can also offer a good developer experience, e.g. MacOSX.</p> MacOS <p>MacOS is a BSD based operating system so has similar command line experience.  Coupled with Homebrew, a wide selection of developer focused packages are freely available.</p> <p>MacOS is often used by commercial companies (especially FinTech) as commercial software is used to administrate the operting system, usually by a remote System Adminstration team.</p>"},{"location":"os/#command-line-shell","title":"Command Line Shell","text":"<ul> <li>bash</li> <li>zsh</li> <li>fish</li> <li>Kitty Terminal</li> </ul>"},{"location":"os/#linux-desktop","title":"Linux Desktop","text":"<ul> <li>i3 tiling window manager</li> <li> Hyprland Wayland compositor and nwg-shell desktop for wayland and sway</li> </ul>"},{"location":"os/#software-development-tools","title":"Software development tools","text":"<ul> <li>Clojure CLI</li> <li>node.js and npm</li> <li>Python</li> </ul>"},{"location":"os/#creative-tools","title":"Creative tools","text":"<ul> <li>Inkscape 2D vector graphics (SVG)</li> <li>Blender 3D modelling, animation and video editing</li> <li>Audacity sound editing</li> <li>FFMpeg audio-video command line tools</li> <li>Opus audio codec</li> </ul>"},{"location":"os/cron/","title":"Cron scheduled tasks","text":"<p><code>cron</code> is daemon that automatically runs a script or command at predefined time. cron is started automatically from /etc/init.d on entering multi-user runlevels.</p> <p><code>crontab</code> is used to create and update the schedule of tasks.</p> Linux Daemon <p>Daemon is a Linux background system process</p> Scheduled GitHub workflow <p>Use a <code>schedule:</code> directive to add <code>cron:</code> events that automatically run on the scheduled time</p> <p>GitHub Docs - Schedule </p>"},{"location":"os/cron/#crontab","title":"Crontab","text":"<p>The crontab is used to automate all types of tasks on Linux systems.</p> <ul> <li>Cron Concepts</li> <li>Setup Crontab Access for a User Account</li> <li>Handle errors with cronjobs</li> <li>Creating cronjobs</li> </ul> <p>Difference between Cron, Crontab and Cron Job</p> <p>Element Linux Name Meaning</p> <ul> <li>crontab:  rows in a table for each cron job, each \u2018*\u2019 asterisk represents a segment of time and a corresponding column in each row.</li> <li>Cron Job: specific task to be performed described in a row, paired with its designated time id</li> </ul> <pre><code>ps aux | grep crond\n</code></pre> <p>This command will search current processes for all users and return any instances of \u2018crond\u2019.</p> <pre><code>ps ux | grep crond\npracticalli  8942  0.0  0.0  18612   840 pts/0    S+   02:16   0:00 grep --color=auto crond\n</code></pre> <p>The daemon is running for the practicalli user account.</p>"},{"location":"os/cron/#crontab-syntax","title":"Crontab syntax","text":"<p>crontab [options]</p> <ul> <li> <ul> <li> <ul> <li> <ul> <li> <ul> <li>updatedb OR</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li> <ul> <li> <ul> <li> <ul> <li> <ul> <li> <p>| | | | | | | | | |-&gt; Day of the week (0-6) | | | |---&gt; Month of the year (1-12) | | |-----&gt; Day of the month (1-31) | |-------&gt; Hour (0-23) |---------&gt; Minute (0-59)</p> <p>List the crontab entries for the current user account.</p> <pre><code>crontab -l\n</code></pre> <p>Edit entries in the crontab</p> <pre><code>crontab -e\n</code></pre> <p>Select a tool to edit the crontab for the first time</p> <pre><code>no crontab for practicalli - using an empty one\n\nSelect an editor.  To change later, run 'select-editor'.\n  1. /bin/nano        &lt;---- easiest\n  2. /usr/bin/vim.basic\n  3. /usr/bin/kak\n  4. /usr/bin/vim.tiny\n  5. /usr/bin/code\n  6. /bin/ed\n\nChoose 1-6 [1]: \n</code></pre>"},{"location":"os/cron/#examples","title":"Examples","text":"<p>Redirect cron output to a file</p> <pre><code>* * * * * sh /path/to/script.sh &amp;&gt; log_file.log\n</code></pre> <p>log errors</p> <pre><code>* * * * * sh /path/to/script.sh &gt;&gt; /tmp/cron.txt 2&gt;&amp;1\n</code></pre> <p>drop out debug logs</p> <pre><code>* * * * * /bin/bash -x /path/to/script.sh &gt;&gt; /tmp/cron.txt 2&gt;&amp;1\n</code></pre>"},{"location":"os/cron/#tips","title":"Tips","text":"<ul> <li>echo the date and time at start of script</li> </ul>"},{"location":"os/debian-linux/","title":"Debian Linux","text":"<p>Debian Linux provides support for the widest range of hardware of any Linux distribution.</p> <p>The Debian project established the high-quality <code>.deb</code> package system which ensures all packages are consitenly defined and manage dependencies.</p>"},{"location":"os/debian-linux/#install","title":"Install","text":"<p>Debian provides ISO images which can be burned onto compact disks or USB memory sticks.</p> <p>The net install image is small and quick to download, containing only the essential packages to run an operating system.</p> <p>Copy the Debian Linux ISO image to a USB of 1GB size or larger</p> <p>Create USB install disk from Debian ISO file</p> <pre><code>cp debian.iso /dev/sda\n</code></pre> Creating USB install disks with non-Debian images <p>Find the name of the USB stick <pre><code>ls -l /dev/disk/by-id/usb-*\n</code></pre></p> <p>Copy the image using the name of the USB Stick <pre><code>dd bs=4M if=path/to/filename.iso of=/dev/disk/by-id/usb-My_flash_drive conv=fsync oflag=direct status=progress\n</code></pre> Arch Linux: ISO image command line utilities</p>"},{"location":"os/debian-linux/#post-install","title":"Post Install","text":""},{"location":"os/debian-linux/#root-vs-sudo","title":"root vs sudo","text":"<p>Debian recommends using the root account to administer the system, rather than using <code>sudo</code> as with Ubuntu</p> <p><code>su -</code> command in a terminal changes to the root user, upon entering a successful root password at the prompt</p> <pre><code>su -\n</code></pre> <p>root password is set during install of Debian</p> <p>Dedicated terminal for root account</p> <p>Open a terminal application specifically to use the root account when carrying out significant maintenance, e.g. installing many packages during the post install.</p>"},{"location":"os/debian-linux/#set-xdg-freedesktop-locations","title":"Set XDG freedesktop locations","text":"ZshBash <pre><code># Set XDG_CONFIG_HOME for clean management of configuration files\nexport XDG_CONFIG_HOME=\"${XDG_CONFIG_HOME:=$HOME/.config}\"\nexport XDG_DATA_HOME=\"${XDG_DATA_HOME:=$HOME/.local/share}\"\nexport XDG_STATE_HOME=\"${XDG_STATE_HOME:=$HOME/.local/state}\"\nexport XDG_CACHE_HOME=\"${XDG_CACHE_HOME:=$HOME/.cache}\"\nexport ZDOTDIR=\"${ZDOTDIR:=$XDG_CONFIG_HOME/zsh}\"\n</code></pre>"},{"location":"os/debian-linux/#git-install","title":"Git install","text":"<pre><code>apt install git\n</code></pre>"},{"location":"os/debian-linux/#clone-practicallidotfiles","title":"Clone practicalli/dotfiles","text":"<p>clone to projects/practicalli/dotfiles</p>"},{"location":"os/debian-linux/#git-configure","title":"Git configure","text":"<p>link .config/git to Practicall dotfiles/git directory</p> <pre><code>ln -s ~/projects/practicalli/dotfiles/git ~/.config/git\n</code></pre> <p>Update identity-practicalli-john file with annonymous email address from GitHub settings &gt; Emails section</p>"},{"location":"os/debian-linux/#kitty-terminal","title":"Kitty Terminal","text":"<p>Debian packages</p> <ul> <li>kitty</li> <li>kitty-doc</li> <li>kitty-shell-integration</li> <li>kitty-terminfo</li> <li><code>fonts-firacode</code> use by Practicalli Kitty terminal</li> </ul> <pre><code>apt install kitty kitty-doc kitty-shell-integration kitty-terminfo fonts-firacode\n</code></pre> <p>Link to practicalli/dotfiles/kitty in .config directory</p> <pre><code>ln -s ~/projects/practicalli/dotfiles/kitty ~/.config/kitty\n</code></pre>"},{"location":"os/debian-linux/#zsh","title":"Zsh","text":"<p>Install zsh package</p> <pre><code>apt install zsh\n</code></pre>"},{"location":"os/debian-linux/#zsh-configuration","title":"Zsh Configuration","text":"<p>Prezto and OhMyZsh are community created configurations that provide a rich experience with very little work.</p> <p>Prezto recommended</p> <p>Practicalli recommends Prezto as it has excellent completion capabilities, especially wish fish mode enabled.</p> <p>OhMyZsh is used within Termux as Prezto has not worked correctly in my experiences.</p> Prezto <p>Prezto provides sane defaults, aliases, functions, auto completion, and prompt themes.</p> <p>As the user account, <code>zsh</code> command to change to zsh from bash shell</p> <p>check if XDG_CONFIG_HOME is set, if not set to $HOME/.config on command line (practicalli dotfiles has a zshenv that does this later in process)</p> <p>clone prezto to XDG location</p> <p>Clone Prezto to XDG_CONFIG_HOME</p> <pre><code>git clone --recursive https://github.com/sorin-ionescu/prezto.git \"${ZDOTDIR:-${XDG_CONFIG_HOME:-$HOME/.config}/zsh}/.zprezto\"\n</code></pre> <pre><code>export XDG_CONFIG_HOME=\"${XDG_CONFIG_HOME:=$HOME/.config}\"\n[[ -d $XDG_CONFIG_HOME/zsh ]] &amp;&amp; export ZDOTDIR=\"$XDG_CONFIG_HOME/zsh\"\nsource \"$ZDOTDIR/.zshenv\"\n</code></pre> <p>Generate symbolic links to the zsh configuration files residing in the prezto directory</p> <pre><code>setopt EXTENDED_GLOB\nfor rcfile in \"${ZDOTDIR:-$HOME}\"/.zprezto/runcoms/^README.md(.N); do\n  ln -s \"$rcfile\" \"${ZDOTDIR:-$HOME}/.${rcfile:t}\"\ndone\n</code></pre> <p>Practicalli creates custom configuration files to keep updating Prezto a simple Git pull.</p> <ul> <li><code>.zprezto</code> practicalli customisations for Prezto</li> <li><code>.zprofile</code> includes .local/bin on execution path</li> <li><code>.zshrc</code> aliases for neovim configurations (astro, practicalli</li> <li><code>.zshenv</code> configures XDG locations</li> </ul> <pre><code>.zprezto\n.p10k.zsh\n.zlogin -&gt; /home/practicalli/.config/zsh/.zprezto/runcoms/zlogin\n.zlogout -&gt; /home/practicalli/.config/zsh/.zprezto/runcoms/zlogout\n.zpreztorc -&gt; /home/practicalli/projects/practicalli/dotfiles/zsh/.zpreztorc\n.zprofile -&gt; /home/practicalli/.config/zsh/.zprezto/runcoms/zprofile\n.zshenv -&gt; /home/practicalli/projects/practicalli/dotfiles/zsh/.zshenv\n.zsh_history\n.zshrc -&gt; /home/practicalli/projects/practicalli/dotfiles/zsh/.zshrc\n</code></pre> <p>Change the default shell</p> <pre><code>chsh -s /usr/bin/zsh\n</code></pre>"},{"location":"os/debian-linux/#neovim","title":"Neovim","text":"<p>Practicalli Neovim install</p> <p>See the install guide in Practicalli Neovim book</p> <p>Required packages</p> <ul> <li><code>curl</code> - suggested by neovim checkhealth</li> <li><code>gdu</code></li> <li><code>xclip</code> provider for accessing operating system clipboard from Neovim</li> <li><code>luarocks</code> required for some mason installed tools</li> <li><code>lazygit</code> terminal UI git client used by AstroNvim</li> </ul> <pre><code>apt install curl gdu lazygit luarocks xclip\n</code></pre> <p><code>btm</code> is suggested but not available as a Debian packages</p>"},{"location":"os/debian-linux/#nodejs","title":"Nodejs","text":"<p>Nodejs is required for many of the linters installed by Mason.</p> <p><code>node</code> and <code>npm</code> are available as packages, although the <code>npm</code> package has a great many dependencies (many of which seem redundant).</p> <p>Download from nodejs website</p> <p>Extract to <code>~/.local/apps/nodejs</code></p> <p>Create a symbolic link to make it simpler to use alternate versions of node and npm</p> <pre><code>ln -s ~/.local/apps/nodejs/node-v18.17.1-linux-x64/ ~/.local/apps/nodejs/current\n</code></pre> <p>Create symbolic links for <code>node</code> and <code>npm</code> to add them to the OS excecution path</p> <pre><code>ln -s ~/.local/apps/nodejs/current/bin/node ~/.local/bin/node &amp;&amp;\nln -s ~/.local/apps/nodejs/current/bin/npm ~/.local/bin/npm &amp;&amp;\n</code></pre> <p>If a different version of nodejs is installed, then all that should be required is deleting and recreating the <code>current</code> symbolic link in <code>~/.local/apps/nodejs/</code> to point to the desired version.</p>"},{"location":"os/debian-linux/#releases","title":"Releases","text":"<p>Download latest appimage for neovim from Neovim repository releases page</p> <p>Make Neovim executable</p> <pre><code>chmod u+x nvim.appimage\n</code></pre> <p>Add Neovim to shell execution path</p> <pre><code>mv nvim.appimage ~/.local/bin &amp;&amp; \\\nln -s ~/.local/bin/nvim.appimage ~/.local/bin/nvim\n</code></pre> <p>Edit <code>.config/zsh/.zprofile</code> and add <code>$HOME/.local/bin</code> to paths to search for executables</p> <p>Set the list of directories that Zsh searches for programs.</p> <p>Zsh paths searched for executable files</p> <pre><code>path=(\n  $HOME/.local/{,s}bin(N)\n  /opt/{homebrew,local}/{,s}bin(N)\n  /usr/local/{,s}bin(N)\n  $path\n)\n</code></pre> <p>Use the <code>source</code> command to load the changes in <code>.zprofile</code> to  update the path and include the <code>.local/bin</code> path</p> <p>Load configuration into current shell session</p> <pre><code>source .config/zsh/.zprofile\n</code></pre>"},{"location":"os/debian-linux/#configure-nvim","title":"Configure nvim","text":"<p>Clone astronvim configuration and Practicalli Astronvim configuration</p> <p>Add aliases to <code>.zshrc</code> to call different neovim configurations</p> <p>Shell Aliases for neovim multiple configurations</p> <pre><code>alias astro=\"NVIM_APPNAME=astronvim nvim\"\nalias lazyvim=\"NVIM_APPNAME=lazyvim nvim\"\nalias practicalli-redux=\"NVIM_APPNAME=neovim-config-redux nvim\"\n</code></pre> <p>Practicalli Neovim - Multiple Configurations</p>"},{"location":"os/debian-linux/#date-and-time","title":"Date and Time","text":"<p>The <code>date</code> command on Linux systems is used to show the system date or set a specific date and time. The system date can only be changed by the root account or accounts in the <code>sudo</code> group.</p> <p>When using the Gnome desktop the system date is automatically managed, keeping the date and time current.</p> <p>The <code>timedatectl</code> command is used to control automatic updating of the system time.  This must be disabled to set the date and time to something other than the current.</p> <p>Disable automatic date-time</p> <pre><code>timedatectl set-ntp 0\n</code></pre> <p>When the timedateclt is disabled, then the <code>date</code> command can be used to set a specific date and or time.</p> <p>Set the date and time</p> <pre><code> date -s '2024-09-16 21:32:00'\n</code></pre> <p><code>date</code> command will show the current date, confirming that the OS system date was changed.</p> <p>Enable automatic date-time</p> <pre><code>timedatectl set-ntp 1\n</code></pre> <p>Linux used to use the ntp service which is available via the Debian <code>ntp</code> package, but not used by Gnome desktop</p>"},{"location":"os/debian-linux/#add-sid-packages-on-testing","title":"Add Sid packages on Testing","text":"<p>Packages can be installed from <code>sid</code> (unstable) when the <code>testing</code> version of Debian Linux is installed, referred to as \"Testing-Unstable Mix\".</p> <p>Configure apt to ensure a testing system stays on testing, without apt upgrading every package to the unstable version.</p> <p>Define testing as the default Debian release</p> <p>Set Default Release as testing with security updates</p> /etc/apt/apt.conf.d/20-tum.conf<pre><code>APT::Default-Release \"/^testing(|-security|-updates)$/\";\n</code></pre> <p>In the Apt sources.list configuration, copy the main testing line and change the version to unstable</p> <p>Add unstable to package sources</p> /etc/apt/sources.list<pre><code># Testing\ndeb &lt;http://deb.debian.org/debian/&gt; trixie main contrib non-free-firmware non-free\ndeb-src &lt;http://deb.debian.org/debian/&gt; trixie main contrib non-free-firmware non-free\ndeb &lt;http://deb.debian.org/debian/&gt; trixie-updates main contrib non-free-firmware non-free\ndeb &lt;http://deb.debian.org/debian/&gt; trixie-backports main contrib non-free-firmware non-free\n\ndeb http://security.debian.org/debian-security trixie-security main contrib non-free-firmware\ndeb-src http://security.debian.org/debian-security trixie-security main contrib non-free-firmware\n\n# Sid\n# deb http://deb.debian.org/debian/ unstable main contrib non-free-firmware non-free\n</code></pre> <p>Run apt update to refresh the cache. Use <code>apt -t unstable install &lt;package-name&gt;</code> to install package foo from unstable rather than testing.</p> <p>Install specific package from Sid</p> <p>```shell apt -t unstable install hyprland</p>"},{"location":"os/debian-linux/#debian-tracker","title":"Debian Tracker","text":"<p>Tracker service indexes many types of files to enable discovery of files by other Gnome services and applications.</p> <ul> <li>desktop search</li> <li>Tag database for keyword tagging</li> <li>Extensible metadata database to add custom metadata to files, e.g. rhythmbox, gedit, etc.</li> <li>Store First Class Objects and the Gnome 3.0 Model</li> </ul> <p>NOTE: when actively using Gnome desktop and Gnome apps, disabling the tracker may reduce functionality</p>"},{"location":"os/debian-linux/#disable-the-tracker-service","title":"Disable the tracker service","text":"<p>The tracker service can be a significant drain on computer resources as it indexes files, especially when there have been a log of changes or for a newly installed system.</p> <p>The tracker has many dependencies, so its not easy to remove the <code>tracker-miner-fs-3</code> package when actively using the Gnome desktop.</p> <p>The recommended approach is to edit the <code>.desktop</code> files and add <code>Hidden=true</code> at the end of each tracker related file and reboot the operating system.</p> /etc/xdg/autostart/tracker-miner-fs-3.desktop<pre><code>[Desktop Entry]\nName=Tracker File System Miner\nComment=Crawls and processes files on the file system\nExec=/usr/libexec/tracker-miner-fs-3\nTerminal=false\nType=Application\nCategories=Utility;\nX-GNOME-Autostart-enabled=false\nX-GNOME-HiddenUnderSystemd=false\n# X-KDE-autostart-after=panel\nX-KDE-StartupNotify=false\nX-KDE-UniqueApplet=true\nNoDisplay=true\nOnlyShowIn=GNOME;KDE;XFCE;X-IVI;Unity;\nX-systemd-skip=true\nHidden=true\n</code></pre> <p>If adding \"Hiddent=true\" is not sufficient, then disable the services for all users by setting them to <code>/dev/null</code> using the <code>systemctl</code> command.</p> <pre><code>sudo systemctl --global mask tracker-miner-fs-3.service\nsudo systemctl --global mask tracker-xdg-portal-3.service\n</code></pre> <p>Remove the database of indexed files from each user account on the system</p> <p><pre><code>rm -rf $HOME/.cache/tracker*\n</code></pre> ```</p>"},{"location":"os/macos/","title":"Mac OS","text":"<p>A BSD based Unix system with commercial Graphic Desktop shell created by Apple.</p> <pre><code>This section relies on commercial experiences when Practicalli was supplied with MacOSX and supporting hardware.\n\nOtherwise, testing relies on the community for confirmation and corrections.\n</code></pre> <p>Use International English keyboard for sensible key locations</p> <p>Practicalli highly recommends the 'International English' keyboard on Macbook laptops as this has sensible locations for keys, especially the <code>#</code> key. Or use the  Model 100 or Atreus keyboards from Keyboard.io</p> <p>Practicalli has limited access to MacOS</p> <p>Practicalli only has occasional access to Mac hardware and macOS, typically during some commercial engagements.  Any updates or corrections to this section are most welcome</p>"},{"location":"os/macos/#window-tiling-manager","title":"Window Tiling manager","text":"<p>Amethyst provides an automatic tiling window manager for MacOS.</p> <p>Enable Ctrl 1 ... 5 in the MacOS keybinding settings to control switching between the first 5 workspaces. Place the most commonly used apps on each of these workspaces to quickly switch between them.</p> <p> Amethyst Tiling window management - MacOSX</p>"},{"location":"os/macos/#homebrew","title":"Homebrew","text":"<p>Open Source development tools are available via homebrew, a community managed distribution of a wide range of tools.</p> <p>Install Homebrew</p> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre>"},{"location":"os/macos/#clojure","title":"Clojure","text":"<p>OpenJDK required for Clojure</p> <p>Install OpenJDK from Adoptium or search for <code>openjdk@21</code> on Homebrew</p> <p>The Clojure maintainers provide a Homebrew Tap for Clojure which is recommeded over that provide by the Homebrew maintainers</p> <p>Install Clojure environment</p> <pre><code>brew install clojure/tools/clojure\n</code></pre> <p><code>bash</code> and <code>curl</code> are required, as is <code>rlwrap</code> if using the <code>clj</code> wrapper.</p>"},{"location":"os/ubuntu-linux/","title":"Ubuntu Linux","text":"<p>Ubuntu Linux is a Debian Linux based distribution providing additional Kernel patches, Desktop management and themes and a wider range of packages provided in the distribution ISO (including non-free software).</p> <p>Ubuntu Desktop is aimed at a rich user experience.</p> <p>Server ISO provides additional options when installing Ubuntu, especially RAID support.</p>"},{"location":"os/ubuntu-linux/#post-install-steps","title":"Post Install steps","text":"<ul> <li>XDG Environment Variables</li> <li>Git</li> <li>Practicalli Dotfiles</li> <li>Kitty Terminal</li> <li>Zsh and power10k themes</li> <li>OpenJDK</li> <li>Clojure CLI</li> <li>Emacs</li> <li>Neovim</li> </ul>"},{"location":"os/command-line/","title":"Command Line","text":"<p>Terminal apps, shells, configuration tips, etc.</p>"},{"location":"os/command-line/kitty-terminal/","title":"Kitty terminal","text":"<p>Kitty Terminal is a fast, feature-rich, GPU based terminal emulator providing additional features via<code>+kitten</code> extensions.</p> <p>Practicalli uses Kitty as the terminal application on Linux and MacOS operating systems.  Kitty is used for running Neovim and all command line tools.</p>"},{"location":"os/command-line/kitty-terminal/#install","title":"Install","text":"Debian PackagesHomebrewGitHub ReleaseLinux Script <p>Install Kitty via the Debian package manager.</p> <pre><code>sudo apt install kitty\n</code></pre> <p>Install the kitty homebrew package</p> <pre><code>brew install --cask kitty\n</code></pre> <p>Download kitty from GitHub releases or use one of the package managers for your operating system.</p> <p>Use the installer script to install in <code>~/.local/kitty.app on Linux</code> and <code>/Applications/kitty.app</code> on macOS.</p> <p>Install script for Linux &amp; MacOSX</p> <pre><code>curl -L https://sw.kovidgoyal.net/kitty/installer.sh | sh /dev/stdin \\\n dest=~/.local/apps/kitty/kitty-&lt;version&gt;\n</code></pre> <p>Add kitty and kitten to the operating system path by creating symbolic links</p> <p>Install script for Linux &amp; MacOSX</p> <pre><code>ln -sf ~/.local/apps/kitty/kitty-&lt;version&gt;/bin/kitty \\\n~/.local/apps/kitty/kitty-&lt;version&gt;/bin/kitten \\\n~/.local/bin/\n</code></pre> <p>Include kitty in desktop launchers using the kitty.desktop file</p> <p>Symbolic link for Kitty desktop application</p> <pre><code>ln -s ~/.local/apps/kitty/kitty-&lt;version&gt;/share/applications/kitty.desktop \\\n~/.local/share/applications/kitty.desktop\n</code></pre> <p>Open text files and images in kitty via the file manager</p> <p>Symbolic link for Kitty file manager support</p> <pre><code>ln -s ~/.local/apps/kitty/kitty-&lt;version&gt;/share/applications/kitty-open.desktop \\\n~/.local/share/applications/kitty-open.desktop\n</code></pre>"},{"location":"os/command-line/kitty-terminal/#configuration","title":"Configuration","text":"<p>Copy the installed configuration to make personal changes, or start a new configuration file in <code>~/.config/kitty/kitty.conf</code></p> Practicalli Kitty ConfigCustom Configuration <p>Practicalli Kitty configuration is included in the Practicalli Dotfiles repository.  Clone the repository</p> <p>Clone Practicalli Dotfiles</p> <p><pre><code>git clone git@github.com:practicalli/dotfiles.git\n</code></pre> Create a symbolic link to the kitty configuration from the <code>$HOME/.config</code> directory <pre><code>ln -s ~/projects/practicalli/dotfiles/kitty  ~/.config/kitty\n</code></pre></p> Practicalli Kitty configuration ~/.config/kitty/kitty.conf<pre><code># ---------------------------------------------------------\n# Practicalli Kitty terminal theme\n#\n# Configuration using GitHub theme with light and dark options\n# using FiraCode font and NerdFont symbol mappings for icon support\n# for powerline10k and web-devicons in Neovim\n# ---------------------------------------------------------\n\n\n# ---------------------------------------------------------\n# Colorscheme / Icons\n\n# `kitty +kitten themes` to browse available themes and apply one\n# `kitty +kitten themes theme-name` to change theme directly\n# Favorite themes include:\n# kitty +kitten themes Gruvbox Material Dark Hard\n# kitty +kitten themes Gruvbox Material Light Hard\n# Shell Aliases: kitty-dark &amp; kitty-light\n\n# BEGIN_KITTY_THEME\n# Gruvbox Material Dark Hard\ninclude current-theme.conf\n# END_KITTY_THEME\n\n# ---------------------------------------------------------\n\n# ---------------------------------------------------------\n# Key bindings\n\n# Open new tab in current window\nmap ctrl+shift+t launch --cwd=current --type=tab\n\n# ---------------------------------------------------------\n\n\n# ---------------------------------------------------------\n# Terminal app visuals\n# - OS specific include: linux, macosx, bsd\n\ninclude ${KITTY_OS}.conf\n\n\n# ---------------------------------------------------------\n# Tab styles\n# fade slant separator powerline custom hidden\ntab_bar_style powerline\ntab_bar_align left\ntab_powerline_style angled\n\n# ---------------------------------------------------------\n\n\n# ---------------------------------------------------------\n# Fonts\n\n# BEGIN_KITTY_FONTS\nfont_family      family=\"FiraCode Nerd Font\"\nbold_font        auto\nitalic_font      auto\nbold_italic_font auto\n# END_KITTY_FONTS\n\nfont_size 12\n\n# adjust_line_height  0\n# adjust_column_width 0\n# adjust_baseline 0\n# ---------------------------------------------------------\n\n\n# ---------------------------------------------------------\n#  Neovim zen-mode-nvim\n#  - change the font size on kitty when in zen mode\nallow_remote_control socket-only\nlisten_on unix:/tmp/kitty\n# ---------------------------------------------------------\n\n# Scrollback\n# Default less command\nscrollback_pager less --chop-long-lines --RAW-CONTROL-CHARS +INPUT_LINE_NUMBER\n# Neovim\nscrollback_pager astro\n</code></pre> Linux specific configurations <p>Linux specific configuration is automatically loaded if OS detected as Linux Ctrl Shift t to open a new tab in kitty using the working directory of the current tab. <pre><code># ---------------------------------------------------------\n# Practicalli Kitty terminal - Linux configuration\n#\n# Configuration specific to using Linux operating system,\n# especially with a tiling window manager without decorations\n# e.g. Regoligth Desktop\n# ---------------------------------------------------------\n\n\n# ---------------------------------------------------------\n# Key bindings\n\n# Open new tab in current window\nmap ctrl+shift+t launch --cwd=current --type=tab\n\n# ---------------------------------------------------------\n\n\n# ---------------------------------------------------------\n# Terminal app visuals\n# - OS specific\n\nenable_audio_bell no\n# visual_bell_color none\n\n# Hide window menu bar (e.g for Gnome)\nhide_window_decorations yes\n\n# Transparent terminal\n#\n# Very transparent\n# background_opacity 0.64\n# Subtle transparency\n# background_opacity 0.92\n# ---------------------------------------------------------\n</code></pre></p> <p>Create your own custom Kitty configuration using the example configuration provided by Kitty terminal.</p> <p>Copy Default Kitty Config</p> <pre><code>cp /usr/share/doc/kitty/examples/kitty.conf ~/.config/kitty/\n</code></pre>"},{"location":"os/command-line/kitty-terminal/#fonts","title":"Fonts","text":"<p>Set the font family in the <code>kitty.conf</code> configuration file</p> <p>Configure Kitty Font from Practicalli Dotfiles</p> $HOME/.config/kitty/kitty.conf<pre><code>font_family      family=\"FiraCode Nerd Font\"\nbold_font        auto\nitalic_font      auto\nbold_italic_font auto\nfont_size        16\n</code></pre> <p>Nerd Fonts provide symbols and icons to Neovim and other terminal tools that support graphics, e.g Powerline10k command prompt themes</p> <p>Nerd Fonts already included in Kitty Terminal</p> <p>Nerd Fonts do not need to be installed when using Kitty Terminal as they are already available.</p> Not Required - Manually Install Nerdfonts <p>Manually installing fonts is not required, however, for completeness this is a previous approach that was used:</p> <p>Download a Nerd Font and configure the font in <code>kitty.conf</code></p> <p>Or download the <code>Symbols Nerd Font</code> font package for use with any font and add the Nerd Font symbols to Kitty via a symbol map configuration.</p> <p>Include Nerdfont Symbols only</p> $HOME/.config/kitty.conf<pre><code># Icons from NerdFont (install Nerdfont symbols only theme)\n# include ./nerdfont-icons.conf\n</code></pre> <p>Kitty Configuration for Nerd Fonts symbol map</p> $HOME/.config/kitty/nerdfont-icons.conf<pre><code># ---------------------------------------------------------\n# NerdFont icons via symbol maps\n#\n# Kitty recommends mapping symbols rather than using patched fonts\n#\n# Download symbols only font from\n# https://github.com/ryanoasis/nerd-fonts/blob/master/src/glyphs/Symbols-2048-em%20Nerd%20Font%20Complete.ttf\n#\n# List available fonts with:\n#   kitty +list-fonts\n#\n# Troubleshoot missing/incorrect characters with:\n#   kitty --debug-font-fallback\n#\n# Reference: https://erwin.co/kitty-and-nerd-fonts/\n\n# Symbols Nerd Font - complete symbol_map\n\n# \"Nerd Fonts - Pomicons\"\nsymbol_map  U+E000-U+E00D Symbols Nerd Font\n\n# \"Nerd Fonts - Powerline\"\nsymbol_map U+e0a0-U+e0a2,U+e0b0-U+e0b3 Symbols Nerd Font\n\n# \"Nerd Fonts - Powerline Extra\"\nsymbol_map U+e0a3-U+e0a3,U+e0b4-U+e0c8,U+e0cc-U+e0d2,U+e0d4-U+e0d4 Symbols Nerd Font\n\n# \"Nerd Fonts - Symbols original\"\nsymbol_map U+e5fa-U+e62b Symbols Nerd Font\n\n# \"Nerd Fonts - Devicons\"\nsymbol_map U+e700-U+e7c5 Symbols Nerd Font\n\n# \"Nerd Fonts - Font awesome\"\nsymbol_map U+f000-U+f2e0 Symbols Nerd Font\n\n# \"Nerd Fonts - Font awesome extension\"\nsymbol_map U+e200-U+e2a9 Symbols Nerd Font\n\n# \"Nerd Fonts - Octicons\"\nsymbol_map U+f400-U+f4a8,U+2665-U+2665,U+26A1-U+26A1,U+f27c-U+f27c Symbols Nerd Font\n\n# \"Nerd Fonts - Font Linux\"\nsymbol_map U+F300-U+F313 Symbols Nerd Font\n\n#  Nerd Fonts - Font Power Symbols\"\nsymbol_map U+23fb-U+23fe,U+2b58-U+2b58 Symbols Nerd Font\n\n#  \"Nerd Fonts - Material Design Icons\"\nsymbol_map U+f500-U+fd46 Symbols Nerd Font\n\n# \"Nerd Fonts - Weather Icons\"\nsymbol_map U+e300-U+e3eb Symbols Nerd Font\n\n# Misc Code Point Fixes\nsymbol_map U+21B5,U+25B8,U+2605,U+2630,U+2632,U+2714,U+E0A3,U+E615,U+E62B Symbols Nerd Font\n\n# ---------------------------------------------------------\n</code></pre>"},{"location":"os/command-line/kitty-terminal/#common-key-mappings","title":"Common Key mappings","text":"<p>Terminal session management.</p> <p>Ctrl+Shift+t to create a new session in a tab window</p> <p>Ctrl+Shift+Left or Right to switch between tabs to the left or right</p> <p>Ctrl+Shift+q to close a window</p> <p>Ctrl+Shift+Enter to create a new session in split window</p> <p>Ctrl+Shift+[ or ] to switch between window splits</p> <p>Ctrl+Shift+w to close a window</p> <p>Other common commands include:</p> <p>Ctrl+Shift+= to increase the font size without restarting kitty</p> <p>Ctrl+Shift+- to increase the font size without restarting kitty</p> <p>Ctrl+Shift+F11 to toggle kitty full-screen</p> <p>Ctrl+Shift+c copy from kitty terminal to clipboard</p> <p>Ctrl+Shift+x copy from kitty terminal to clipboard</p> <p>Ctrl+Shift+v paste into to kitty terminal from clipboard</p> <p>Ctrl+Shift+s paste into to kitty terminal from clipboard</p>"},{"location":"os/command-line/kitty-terminal/#kitten-features","title":"Kitten features","text":"<p>kittens provide additional features.  Recommended features include:</p> <ul> <li>Theme kitten - in-terminal theme browser and selector</li> <li>diff - fast, side-by-side diff for the terminal with syntax highlighting</li> <li>Clipboard - Copy/paste to the clipboard from shell scripts, even over SSH</li> <li>SSH - SSH with automatic shell integration, connection re-use for low latency and easy cloning of local shell and editor configuration to the remote host</li> </ul>"},{"location":"os/command-line/kitty-terminal/#themes","title":"Themes","text":"<p>Theme kitten provides a simple way to browse available themes and select a theme for use</p> <p>Browse available themes and apply one, or Ctrl+c to cancel</p> <pre><code>kitty +kitten themes\n</code></pre> <p>Change themes automatically to the given theme name (the theme must exist)</p> <pre><code>kitty +kitten theme theme-name\n</code></pre> Themes used by Practicalli <p>Practicalli uses the Gruvbox Material Light Soft as the light theme <pre><code>kitty +kitten themes Gruvbox Material Light Soft\n</code></pre> Practicalli uses Gruvbox Material Dark Soft as the dark theme <pre><code>kitty +kitten themes Gruvbox Material Dark Soft\n</code></pre></p> <p>The first time the theme kitten is run the following config is added to the <code>~.config/kitty/kitty.conf</code> file and the chosen theme configuration written to the  <code>~/.config/kitty/current-theme.conf</code> file</p> <p>This configuration shows the name of the theme, which is also in the top of the <code>current-theme.conf</code> file</p> <pre><code># BEGIN_KITTY_THEME\n# GitHub Dark\ninclude current-theme.conf\n# END_KITTY_THEME\n</code></pre> <p>Icon support from Nerd fonts, download and include the configuration to show icons in terminal based editors (e.g. Neovim, Emacs, etc.)</p> <pre><code>include ./nerdfont-icons.conf\n</code></pre>"},{"location":"os/command-line/kitty-terminal/#diff","title":"Diff","text":"<p>The Diff kitten provides a fast way to compare files, although there is no support for merging changes.</p> <p>Kitty supports diff of image files, showing the two images side by side.</p> <pre><code>kitty +kitten diff file1 file2\n</code></pre> <p>, or &gt; jumps to next diff match</p> <p>. or &lt;  jumps to previous diff match</p> <ul> <li>All keyboard controls</li> </ul>"},{"location":"os/command-line/kitty-terminal/#ssh","title":"SSH","text":"<p>The <code>+kitten</code> option ensures the remote environment is configured correctly for the Kitty terminal</p> <pre><code>kitty +kitten ssh hostname\n</code></pre>"},{"location":"os/command-line/kitty-terminal/#image-viewer","title":"Image viewer","text":"<p>View images with kitty</p> <pre><code>kitty +kitten icat /path/to/image\n</code></pre> <p>create an alias in your shell configuration file, e.g. <code>shell-aliases</code></p> <p>View images with </p> <pre><code>alias icat=\"kitten icat\"\n</code></pre> <p>Kitty kitten icat</p>"},{"location":"os/command-line/warp/","title":"Warp terminal","text":"<p>Section just starting - Warp terminal being evaluated</p> <p>Practicalli uses kitty terminal currently, but has started to evaluate Warp terminal.  Any feedback on content recommendations most welcome.</p> <p>A terminal app with a focus on an enhanced programming experience, connecting to Warp AI for convienient help right within the command line.</p> <p>Warp Terminal</p> <p>Warp Terminal Docs</p> <p> </p>"},{"location":"os/command-line/warp/#customisation","title":"Customisation","text":"<p>Warp offers customisation options when first starting.</p> <ul> <li>terminal theme</li> <li> <p>prompt theme (warp or any detected theme, e.g. powerline10k)</p> </li> <li> <p> Vim key bindings</p> </li> </ul>"},{"location":"os/command-line/warp/#notebooks","title":"Notebooks","text":"<p>Code blocks do not support Clojure syntax</p> <p>Code blocks do not support Clojure code syntax highlighting.</p> <p>Languages currently supported are: <code>shell</code>, <code>JSON</code>, <code>C++</code>, <code>Go</code>, <code>Java</code>, <code>JavaScript</code></p>"},{"location":"os/linux/hyprland/","title":"Hyprland Tiling Compositor","text":"<p>Hyprland is an elegant and high performance Tiling window compositor for Wayland on Linux.</p> <p>Hyprland is not a full desktop environment, unlike KDE or Gnome.  Additional tools can be installed to make a complete desktop environment.</p> <p>Try Hyprland with the NWG Live ISO</p> <p>NWG Live ISO provides a simple way to try Hyprland and the set of tools provided by NWG-Shell to give a Desktop Environment experience based on Gnome.</p>"},{"location":"os/linux/hyprland/#install","title":"Install","text":"<p>Hyprland is a quickly evolving project and issues may occur</p> NWG Live ISODebian LinuxArch Linux <p>NWG-ISO is a simple way to try out hyprland and the NWG Shell tools, providing an experience close to a Desktop Environment like Gnome or KDE.</p> <p>NWG Live ISO will run from memory.  Once running, the Live ISO can also be used to install Hyprland &amp; NWG tools on the permanent storage of the computer.</p> <p></p> <p></p> <p>Create USB Startup</p> <pre><code>dd bs=4M if=nwg-live-2024.10.01-x86_64.iso of=/dev/disk/by-id/usb-Generic_Flash_Disk_&lt;id&gt;-0:0 conv=fsync oflag=direct status=progress\n</code></pre> <p>Arch Linux ISO command line untilities</p> <p>Practicalli Journal - notes on Hyprland with Debian Linux</p> <p>Hyprland on Debian - scripts to review </p> <p>hyprland on debian script</p> <p>The hyprland packages are on version 0.41 at time of writing, the current release is 0.45 and has significant changes</p> <p>Install a minimal Arch Linux OS from the ISO and setup Arch with Hyprland using the <code>archinstall</code></p> <p>Install the <code>nwg-shell</code> package to add tools for a Desktop Environment experience on top of Gnome.</p> <pre><code>pacman -S nwg-shell\n</code></pre> <p>Run the nwg-shell install script to configure the NWG tools <pre><code>nwg-shell-installer -w -hypr\n</code></pre></p> <p>Set nwg-hello as the greeter (login screen) in <code>/etc/greetd/config.toml</code></p> /etc/greetd/config.toml<pre><code>[default_session]\ncommand = \"Hyprland -c /etc/nwg-hello/hyprland.conf\"\n</code></pre> <p>Arch Linux greetd.service</p>"},{"location":"os/linux/regolith/","title":"Regolith Desktop","text":"<p>Regolith 3 is a keyboard focused tiling window manager on top of Ubuntu or Debian, supporting X11 and Wayland (sway).</p> <p>Excellent defaults make using Regolith very quick to get started and customisation is via a simple <code>~/.config/regolith3/Xresources</code> file.</p> <p>Install Guide Basic Use Configuration guide</p>"},{"location":"os/linux/regolith/#install","title":"Install","text":"<p>Use a Reglolith ISO image if installing the Operating System anew.</p> <p>Or add the Regolith public key and package repository to an existing Ubuntu or Debian Linux operating system.</p> <p>Regolith 3 Install Guide</p>"},{"location":"os/linux/regolith/#post-install","title":"Post Install","text":"<p>Regolith 3 Configuration guide</p> <p>Edit <code>~/.config/regolith3/Xresources</code> to customise Regolith (version 3)</p> Practicalli Xresources configuration <pre><code>!! Postion of status bar\nwm.bar.position: top\n\n!! Theme\ngnome.wm.theme: Gruvbox\n\n!! Workspace Icons\n!! Icons to represent purpose of each workspace:\n!! issues, terminal, Clojure, db, chat, settings, music, meetings/screencats, browser\nwm.workspace.01.name: 1:&lt;span&gt; &lt;/span&gt;&lt;span font_desc='FontAwesome 14'&gt;1 &lt;/span&gt;&lt;span foreground='#4455ff'&gt;\ue65b&lt;/span&gt;&lt;span&gt; &lt;/span&gt;\nwm.workspace.02.name: 2:&lt;span&gt; &lt;/span&gt;&lt;span font_desc='FontAwesome 14'&gt;2 &lt;/span&gt;&lt;span foreground='#F7F7F7'&gt;\uf120&lt;/span&gt;&lt;span&gt; &lt;/span&gt;\nwm.workspace.03.name: 3:&lt;span&gt; &lt;/span&gt;&lt;span font_desc='FontAwesome 14'&gt;3 &lt;/span&gt;&lt;span foreground='#339e35'&gt;\ue66a&lt;/span&gt;&lt;span&gt; &lt;/span&gt;\nwm.workspace.04.name: 4:&lt;span&gt; &lt;/span&gt;&lt;span font_desc='FontAwesome 14'&gt;4 &lt;/span&gt;&lt;span foreground='#f99b11'&gt;\ue706&lt;/span&gt;&lt;span&gt; &lt;/span&gt;\nwm.workspace.05.name: 5:&lt;span&gt; &lt;/span&gt;&lt;span font_desc='FontAwesome 14'&gt;5 &lt;/span&gt;&lt;span foreground='#bf1e0f'&gt;\uf1d7&lt;/span&gt;&lt;span&gt; &lt;/span&gt;\nwm.workspace.06.name: 6:&lt;span&gt; &lt;/span&gt;&lt;span font_desc='FontAwesome 14'&gt;6 &lt;/span&gt;&lt;span foreground='#79ace6'&gt;\ue66a&lt;/span&gt;&lt;span&gt; &lt;/span&gt;\nwm.workspace.07.name: 7:&lt;span&gt; &lt;/span&gt;&lt;span font_desc='FontAwesome 14'&gt;7 &lt;/span&gt;&lt;span foreground='#d489ff'&gt;\uf03e&lt;/span&gt;&lt;span&gt; &lt;/span&gt;\nwm.workspace.08.name: 8:&lt;span&gt; &lt;/span&gt;&lt;span font_desc='FontAwesome 14'&gt;8 &lt;/span&gt;&lt;span foreground='#F7F7F7'&gt;\uf025&lt;/span&gt;&lt;span&gt; &lt;/span&gt;\nwm.workspace.09.name: 9:&lt;span&gt; &lt;/span&gt;&lt;span font_desc='FontAwesome 14'&gt;9 &lt;/span&gt;&lt;span foreground='#F7F7F7'&gt;\uf130&lt;/span&gt;&lt;span&gt; &lt;/span&gt;\nwm.workspace.10.name: 10:&lt;span&gt; &lt;/span&gt;&lt;span font_desc='FontAwesome 14'&gt;10 &lt;/span&gt;&lt;span foreground='#ee8424'&gt;\uf269&lt;/span&gt;&lt;span&gt; &lt;/span&gt;\n</code></pre>"},{"location":"os/linux/regolith/#default-terminal","title":"Default terminal","text":"<p>Set the default terminal to run when pressing Super Enter</p> <p>Open a terminal and use the update-alternatives command to list the available terminal programms.  Install the terminal app if not listed.</p> Adminstrator access required to set default terminal <p><code>sudo</code> is used as adminstrator rights are required to change the default terminal.  The list of available terminals can be seen without admin rights, although cannot be changed.</p> <pre><code>sudo update-alternatives --config x-terminal-emulator\n</code></pre> <p>Press the corresponding number key to select the desired terminal, e.g. 2 to select Kitty terminal</p> <pre><code>There are 3 choices for the alternative x-terminal-emulator (providing /usr/bin/x-terminal-emulator).\n\n  Selection    Path                             Priority   Status\n------------------------------------------------------------\n* 0            /usr/bin/gnome-terminal.wrapper   40        auto mode\n  1            /usr/bin/gnome-terminal.wrapper   40        manual mode\n  2            /usr/bin/kitty                    20        manual mode\n  3            /usr/bin/urxvt                    20        manual mode\n</code></pre> <p>Practicalli recommends Kitty</p> <p>Kitty terminal is a very easy to use and configure terminal application with great support for running Neovim and other terminal applications.</p>"},{"location":"os/linux/regolith/#theme","title":"Theme","text":"<p>List the available Regolith look packages to see the available themes</p> <pre><code>apt list | grep regolith-look-\n</code></pre> <p>Install the desired theme using the <code>regolith-look</code> command</p> <pre><code>regolith-look set grubox\n</code></pre> <p>Regolith Desktop should update with the new theme settings.  Use <code>regolith-look refresh</code> to force an update to the new theme settings or if further changes are made to the configuration.</p>"},{"location":"os/linux/regolith/#background","title":"Background","text":"<p>Use the Settings app to select from the available backgrounds (if the theme doent set the desired wallpaper)</p>"},{"location":"os/shell/","title":"Operating System Shell","text":"<p>a shell is a computer program that exposes an operating system's services to a human user or other programs. In general, operating system shells use either a command-line interface (CLI) or graphical user interface (GUI), depending on a computer's role and particular operation. It is named a shell because it is the outermost layer around the operating system.[1][2]</p> <p>Command-line shells require the user to be familiar with commands and their calling syntax, and to understand concepts about the shell-specific scripting language (for example, bash), while graphical shells place a low burden on beginning computer users and are characterized as being easy to use, yet most GUI-enabled operating systems also provide CLI shells, normally for performing advanced tasks.</p> <p>Wikipedia: Shell - Computing</p>"},{"location":"os/shell/#command-line-shell","title":"Command Line Shell","text":"<ul> <li>bash</li> <li>zsh</li> </ul>"},{"location":"os/shell/#aliases","title":"Aliases","text":"<p>Define aliases to optomise commands and create useful default flags when calling commands</p> <p>Use a <code>shell-aliases</code> file to define aliases to be used with any command line shell.</p> <p>Shell Aliases</p> <pre><code># Shell aliases shared across all shells (zsh, bash)\n\n# Neovim Aliases for multiple configurations\nalias astro=\"NVIM_APPNAME=astronvim nvim\"\n\n# Neovide alias with AstroNvim configuration\nalias neovide=\"NVIM_APPNAME=astronvim neovide\"\n\n# Shell history\n# edit entire history\nalias edit-shell-history=\"fc -W; astro \\\"$HISTFILE\\\"; fc -R\"\n\n# edit previous command in history\nalias edit-last-command=\"fc -e astro -1\"\n</code></pre> <p>Source the shell aliases from the shell configuration files</p> ZshBash <p>Source Shell Aliases for Zsh</p> .config/zsh/.zshrc<pre><code># Source Shell Aliases\n[[ ! -f ~/.config/shell-aliases ]] || source ~/.config/shell-aliases\n</code></pre> <p>Source Shell Aliases for bash</p> .bashrc<pre><code># Source Shell Aliases\nif [ -f $HOME/.config/shell-aliases ]; then\n    source $HOME/.config/shell-aliases \nfi\n</code></pre>"},{"location":"os/shell/#gui-shell","title":"GUI Shell","text":"<p>Gnome, KDE, Regolith are examples of desktop shells.</p>"},{"location":"os/shell/bash/","title":"Bourne Again SHell (BASH)","text":"<p>Bash is a very common shell environment for Linux and is typically the default shell.</p>"},{"location":"os/shell/bash/#ohmybash","title":"Ohmybash","text":""},{"location":"os/shell/fish/","title":"Fish Shell","text":""},{"location":"os/shell/zsh/","title":"Z Shell","text":"<p>Z Shell (zsh) is an advanced shell environment and shell scripting language for Linux/Unix systems.</p> <p>Wikipedia - Z Shell</p>"},{"location":"os/shell/zsh/#prezto","title":"Prezto","text":""},{"location":"os/shell/zsh/#ohmyzsh","title":"OhmyZsh","text":""},{"location":"os/shell/zsh/#command-history","title":"Command History","text":"<p>Up and Down navigate through the history of commands.</p>"},{"location":"os/shell/zsh/#edit-last-command","title":"Edit last command","text":"<p>Edit the previous command in the history if you know what it should be, e.g. if you typed got status instead of git statusthe you could run the command</p> <p>Edit last command</p> <pre><code>fc -e astro -1\n</code></pre> <p>Replace <code>astro</code> with either <code>emacsclient</code>, <code>nvim</code>, <code>vim</code>, nano or any preferred text editor.</p> Edit last command <p>Edit the <code>got status</code>to <code>git status</code> and save &amp; exit. Now the previous command is <code>git status</code>.</p> <p>Change the <code>-1</code> to if the command is further back in history (a known location)</p>"},{"location":"os/shell/zsh/#edit-command-history","title":"Edit command history","text":"<p>Edit the whole command history as a file, allowing easy removal of unwanted or duplicate commands using search and replace and other editing tools.</p> <p>Edit entire history</p> <p>```shell fc -W; astro \"$HISTFILE\"; fc -R ````</p> <p>Replace <code>astro</code> with either <code>emacsclient</code>, <code>nvim</code>, <code>vim</code>, nano or any preferred text editor.</p>"},{"location":"os/shell/zsh/#shell-aliases-for-commands","title":"Shell aliases for commands","text":"<p>Add aliases to <code>$XDG_CONFIG_HOME/shell-aliases</code> or your preferred location of shell alias definitions.</p> <p>Shell Alias</p> <pre><code># Shell history\n# edit entire history\nalias zsh-edit-history=\"fc -W; astro \\\"$HISTFILE\\\"; fc -R\"\n\n# edit previous command in history\nalias zsh-edit-last-command=\"fc -e astro -1\"\n</code></pre>"},{"location":"os/shell/zsh/#alternatives","title":"Alternatives","text":"<ul> <li>zsh-hist plugin</li> </ul>"},{"location":"os/shell/zsh/#references","title":"References","text":"<ul> <li>Edit Zsh history - Stack Exchange</li> <li>zsh history setup</li> </ul>"},{"location":"people/","title":"Notable People","text":"<p>Notable people from computer science and software engineering</p>"},{"location":"people/#people","title":"People","text":"<p>Doug Englebart - the mother of all demos</p> <p>Ted Nelson - xanadu repository</p>"},{"location":"persistence/","title":"Persistence","text":"<p>Storing data on long term storage</p>"},{"location":"persistence/#relational-databases","title":"Relational Databases","text":"<p>Relational Database (RDBMS) and tools</p> <ul> <li>Postgresql</li> <li>H2</li> <li>MariaDB</li> </ul>"},{"location":"persistence/#document-databases","title":"Document databases","text":"<ul> <li>MongoDB</li> <li>XTDB (although this seems to be more of an SQL store now)</li> </ul>"},{"location":"persistence/#key-value-stores","title":"Key Value Stores","text":"<p>Key Value stores are often used for caches or data that has relatively simple structure.</p> <ul> <li>Redis</li> <li>AWS DynamoDB</li> </ul>"},{"location":"persistence/rdbms/","title":"Relational Database Management Systems","text":""},{"location":"persistence/rdbms/#servers","title":"Servers","text":"<p>An</p>"},{"location":"persistence/rdbms/#clients","title":"Clients","text":"<p>An RDMS client is anything that connects to an RDMS server, e.g. Database tool, command line, programming languaguage code.</p> <p> RDMS CLient</p>"},{"location":"persistence/rdbms/#languages","title":"Languages","text":"<p> Structured Query Language (SQL) overview</p> <p> SQL</p>"},{"location":"persistence/rdbms/#sql-guides","title":"SQL Guides","text":"<ul> <li> W3 Schools - SQL Introduction</li> <li> AWS - What Is SQL</li> <li> Codecademy - Learn SQL</li> </ul>"},{"location":"persistence/rdbms/client/","title":"RDBMS Clients","text":"<p>Connect to a relational database instance and manage the structure and data within a database.</p>"},{"location":"persistence/rdbms/client/#recommended-clients","title":"Recommended Clients","text":"<ul> <li>Command Line</li> <li>DBeaver - desktop application</li> <li>pgAdmin - desktop application and web app</li> </ul>"},{"location":"persistence/rdbms/client/#manage-databases","title":"Manage Databases","text":"<p>Create Database \u2013 create a new database using CREATE DATABASE statement.</p> <p>Alter Database \u2013 modify the features of an existing database using the ALTER DATABASE statement.</p> <p>Rename Database \u2013 change the name of the database to a new one.</p> <p>Drop Database \u2013 removes a database permanently using the DROP DATABASE statement.</p> <p>Copy a Database \u2013 copy a database within a database server or from a server to another.</p> <p>Get Database Object Sizes \u2013 introduce you to various handy functions to get the size of a database, a table, and indexes.</p>"},{"location":"persistence/rdbms/client/#manage-schema","title":"Manage Schema","text":"<p>Schema \u2013 introduce the schema concept and explains how the schema search path works in PostgreSQL.</p> <p>Create Schema \u2013 show you how to create a new schema in a database.</p> <p>Alter Schema \u2013 rename a schema or changes its owner to the new one.</p> <p>Drop schema \u2013 delete one or more schemas with their objects from a database.</p>"},{"location":"persistence/rdbms/client/#managing-tablespaces","title":"Managing Tablespaces","text":"<p>PostgreSQL tablespaces allow you to control how data stored in the file system. The tablespaces are very useful in many cases such as managing large tables and improving database performance.</p> <p>Creating Tablespaces \u2013 introduce you to PostgreSQL tablespaces and shows you how to create tablespaces by using CREATE TABLESPACE statement.</p> <p>Changing Tablespaces \u2013 show you how to rename, change owner and set the parameter for a tablespace by using ALTER TABLESPACE statement.</p> <p>Delete Tablespaces \u2013 learn how to delete tablespaces by using DROP TABLESPACE statement.</p>"},{"location":"persistence/rdbms/client/#section-4-roles-privileges","title":"Section 4. Roles &amp; Privileges","text":"<p>PostgreSQL represents accounts as roles. Roles that can log in called login roles or users. Roles that contain other roles are called group roles. In this section, you will learn how to manage roles and groups effectively.</p> <p>Create role: introduce you to roles concept and show you how to create roles and groups by using the create role statement.</p> <p>Grant \u2013 show you how to grant privileges on database objects to a role.</p> <p>Revoke \u2013 guide you on revoking granted privileges on database objects from a role.</p> <p>Alter role \u2013 show you how to use the alter role statement to modify the attributes of roles, rename roles, and set the configuration parameters.</p> <p>Drop role \u2013 learn how to drop a role especially a role that has dependent objects.</p> <p>Role membership \u2013 learn how to create group roles to better manage role membership.</p> <p>List user roles \u2013 show you how to list all roles on the PostgreSQL server.</p>"},{"location":"persistence/rdbms/client/#backup-restore","title":"Backup &amp; Restore","text":"<p>PostgreSQL backup and restore tools including pg_dump, pg_dumpall, psql,  pg_restore and  pgAdmin to backup and restore databases.</p> <p>Backup \u2013 introduce you to practical ways to back up your databases by using PostgreSQL backup tool including pg_dump and pg_dumpall.</p> <p>Restore \u2013  show you various ways to restore PostgreSQL databases by using psql and pg_restore tools.</p> <p>Backup and restore useful for local development with Docker Compose and volumes</p>"},{"location":"persistence/rdbms/client/#general","title":"General","text":"<p>Reset Password \u2013 show you how to reset the forgotten password of the postgres user. psql Commands \u2013 give you the most common psql command to help you query data from PostgreSQL faster and more effectively. Describe Table \u2013 get information on a particular table. Show Databases \u2013 list all databases in the current database server Show Tables \u2013 show all tables in the current database.</p>"},{"location":"persistence/rdbms/client/cli/","title":"SQL Command Line Client","text":""},{"location":"persistence/rdbms/client/pgadmin/","title":"pgAdmin PostgreSQL Tools","text":"<p> pgAdmin</p> <p>pgAdmin is a feature rich Open Source administration and development platform for PostgreSQL</p> <p> pgAdmin Documentation</p>"},{"location":"persistence/rdbms/client/pgadmin/#references","title":"References","text":"<p>PostgreSQL Tutorial - Administration</p>"},{"location":"persistence/rdbms/server/","title":"RDBMS Server","text":"<p>Where the data lives.</p>"},{"location":"persistence/rdbms/server/#open-source-servers","title":"Open Source Servers","text":"<ul> <li>PostgreSQL</li> <li>H2</li> <li>MarinaDB</li> </ul>"},{"location":"persistence/rdbms/server/#features","title":"Features","text":""},{"location":"persistence/rdbms/server/#stored-procedures","title":"Stored Procedures","text":""},{"location":"persistence/rdbms/server/h2/","title":"H2","text":"<p>H2 is a very useful tool for rapid development as there is very little setup required.  H2 can be run in memory and data can also be saved to the local file space.</p> <p> Practicalli Clojure Web Services uses H2 to support a simple local development workflow.</p> <p>H2 is not recommended for production usage</p>"},{"location":"persistence/rdbms/server/mariadb/","title":"Maria DB","text":"<p>MariaDB is a community-developed, commercially supported fork of the MySQL relational database management system, intended to remain free and open-source software under the GNU General Public License.</p> <p>Development is led by some of the original developers of MySQL, who forked it due to concerns over its acquisition by Oracle Corporation in 2009.</p> <p>MariaDB Foundation</p> <p>MariaDB - Wikipedia</p> <p>MariaDB Primer</p>"},{"location":"persistence/rdbms/server/mariadb/#docker","title":"Docker","text":"<p>Docker Compose for MariaDB</p> compose.yaml<pre><code>services:\n  db:\n    image: mariadb\n    restart: always\n    environment:\n      # MARIADB_RANDOM_ROOT_PASSWORD: yes\n      MARIADB_ROOT_PASSWORD: practicalli\n      MARIADB_MYSQL_LOCALHOST_USER: clojure\n      MARIADB_DATABASE: service-db\n    healthcheck:\n      test: [ \"CMD\", \"healtcheck.sh --su-mysql --connect --innodb_initialized\" ]\n      timeout: 45s\n      interval: 10s\n      retries: 10\n\n  adminer:\n    image: adminer\n    restart: always\n    ports:\n      - 8080:8080\n</code></pre> <p>MariaDB Official Docker Image</p> <p>Install &amp; Use MariaDB via Docker</p> <p>MariaDB healthcheck script MariaDB healthcheck script - source</p>"},{"location":"persistence/rdbms/sql/","title":"Structured Query Language (SQL)","text":"<p>SQL is a domain-specific language designed for managing data held in a relational database management system (RDBMS), or for stream processing in a relational data stream management system (RDSMS).</p> <p>SQL data incorporates relations among entities and variables.</p> <p>Originally based upon relational algebra and tuple relational calculus, SQL consists of many types of statements,[6] which may be informally classed as sublanguages, commonly:</p> <ul> <li>data query language (DQL)</li> <li>data definition language (DDL) (schema creation and modification)</li> <li>data control language (DCL)</li> <li>data manipulation language (DML) (insert, update, and delete)</li> </ul> <p>SQL is a declarative language (4GL) which also includes procedural elements.</p> <p>SQL - wikipedia</p>"},{"location":"persistence/rdbms/sql/#sql-syntax","title":"SQL Syntax","text":"<p>The SQL language is subdivided into several language elements, including:</p> <ul> <li>Clauses, which are constituent components of statements and queries.</li> <li>Expressions, which can produce either scalar values, or tables consisting of columns and rows of data</li> <li>Predicates, which specify conditions that can be evaluated to SQL three-valued logic (3VL) (true/false/unknown) or Boolean truth values and are used to limit the effects of statements and queries, or to change program flow.</li> <li>Queries, which retrieve the data based on specific criteria. This is an important element of SQL.</li> <li>Statements, which may have a persistent effect on schemata and data, or may control transactions, program flow, connections, sessions, or diagnostics.</li> <li>SQL statements also include the semicolon (\";\") statement terminator. Though not required on every platform, it is defined as a standard part of the SQL grammar.</li> <li>Insignificant whitespace is generally ignored in SQL statements and queries, making it easier to format SQL code for readability.</li> </ul>"},{"location":"persistence/rdbms/sql/examples/","title":"SQL Examples","text":"<p>Useful SQL queries for doing interesting things. Each query should be accompanied by an explanation of its purpose Viewing the latest data View the last 100 rows in the database (assuming id is an ascending (or auto-incremental) value</p>"},{"location":"persistence/rdbms/sql/examples/#database-management","title":"Database management","text":""},{"location":"persistence/rdbms/sql/examples/#schema-management","title":"Schema management","text":""},{"location":"persistence/rdbms/sql/examples/#query-data","title":"Query data","text":"<pre><code>SELECT * FROM table ORDER BY id DESC LIMIT 100\n</code></pre> <p>Select the last 100 rows in a table (assuming id is an auto-incremental index) and order the results in ascending order</p> <pre><code>SELECT * FROM (\n  SELECT * FROM table ORDER BY id DESC LIMIT 100\n) sub\nORDER BY id ASC\n</code></pre>"},{"location":"practices/","title":"Engineering Practices","text":"<p>Engineering has many practices and principles defined over it many decades.</p> Section is work in progress"},{"location":"practices/#agile-software-development","title":"Agile software Development","text":"<ul> <li> Agile Manifesto (essentail reading)</li> <li>Extreme Programming</li> <li>Test Driven Development</li> <li>Technical Debt</li> </ul>"},{"location":"practices/#discussions","title":"Discussions","text":"<ul> <li> Standup</li> <li> Retrospective</li> <li> one-to-one meetings</li> <li> Six Thinking Hats</li> </ul>"},{"location":"practices/#lean-software-development","title":"Lean software development","text":"<ul> <li> Kanban - optomising work &amp; evolving change</li> <li> Behaviour Driven Development</li> </ul>"},{"location":"practices/#general-development","title":"General development","text":"<ul> <li>Root Cause Analysis</li> <li>Five whys</li> </ul>"},{"location":"practices/#general-project-management","title":"General Project Management","text":"<ul> <li>Rolling Wave Planning</li> <li>MoSCoW rules - Must have, Should have, Could have, Wont have</li> <li>Managing Bias</li> </ul>"},{"location":"practices/managing-bias/","title":"Managing Bias","text":"<p>Bais - judgements are systematically in a particular direction</p> <p>Noise - people with similar views arrive at very different conclusions</p> <p>noise must be considered a significant factor to fully understand when bias is being exerted.</p>"},{"location":"practices/managing-bias/#noise","title":"Noise","text":"<p>Noise can be recognised and measured without knowing anything about the subject or potential bias.</p> <p>Noise is important factor when considering bias as many of our conclusions are drawn from judgements whos answer is not fully known or even unknowable at that point in time.</p> <p>Movie studio exectutives make judgements about about a potential market without knowing the gross amout the film will actually make.  Analysing statitics can give a rough guide from past experiences, although that is not the same as actually knowing.</p> <p>Weather forcasts are very noisy as they try predict the future state of the highly complex global weather system that drives all local weather reporting.</p>"},{"location":"practices/managing-bias/#cognitive-bias-song","title":"Cognitive Bias Song","text":""},{"location":"practices/reports/","title":"Reports","text":"<p>When a report it required then the person creating that report should know what its value is and who is the audience.</p>"},{"location":"practices/reports/#identify-reports","title":"Identify reports","text":"<p>If repeating reports are required to be delivered, they should be identified as early as possible.</p> <p>Capture the value the report provides and the stakeholders that will consume the report.</p> <p>Ask those consuming each report what they really want to see and how often.</p> <p>Also consult as to the presentation of the information to ensure that the information can be consumed effectively.</p>"},{"location":"practices/reports/#identify-source-data","title":"Identify source data","text":"<p>What information is used to generate the report.</p> <p>Identify related information that could be useful to the stakeholders and provide examples to stakeholders for feedback.</p>"},{"location":"practices/reports/#optomise-creation","title":"Optomise Creation","text":"<p>Ensure that data capture and transformation for the report is as automated as possible, minimising the time it takes to generate each report.</p> <p>Optomise the report format to best suit the methods of consumption by the stakeholders.</p>"},{"location":"practices/root-cause-analysis/","title":"Root Cause Analysis","text":"<p>When there is a problem at work it can be difficult to get beyond immediate symptoms and neglect addressing the underlying causes. Tackling the symptoms rather than the cause results in fixing the same kinds of problems time and time again.  This is often referred to as \u201cfire-fighting\u201d.</p> <p>Root Cause Analysis (RCA) is a widely used technique that helps people get beyond the symptoms of a problem and reveal the, often hidden, and multiple root causes.</p> <p>RCA is built on the principle that causal relationships exist for all events. By understanding these we can move beyond the symptoms and address the root causes at source.</p> <p>Causes can be complex</p> <p>When multiple causes are contributing to the issue, consider which of those issues would make the most significant impact if addressed.</p>"},{"location":"practices/root-cause-analysis/#rca-processes","title":"RCA Processes","text":"<p>There are a variety of Root Cause Analysis processes to choose from, some are quick and easy, such as 5 Whys, others are longer and more complex such as FMEA and Ishikawa. The Sologic RCA methodology is quick to learn, logical and scalable.  It is also universal in that it can be applied to any problem, in any sector.</p> <ul> <li>Root Cause Analysis - Sologic</li> <li>Root Cause Analysis: A quick guide - ProjectManager</li> <li>What is a root cause analysis? - IBM</li> </ul> <p>Root Cause Analysis - Wikipedia</p> <ul> <li>Root Cause Analysis Explained: Definition, Examples, and Methods - Tableau</li> </ul>"},{"location":"practices/root-cause-analysis/#organisation-leadership","title":"Organisation Leadership","text":"<p>Root cause analysis: What it is &amp; how to perform one</p>"},{"location":"practices/tech-debt-notes/","title":"Tech debt notes","text":""},{"location":"practices/tech-debt-notes/#driving-factors","title":"Driving factors","text":"<p>Most technical debt has a root cause of time pressure or limited understanding.</p>"},{"location":"practices/tech-debt-notes/#time-constraints","title":"Time constraints","text":"<p>Rapid business changes increase presure on engineering teams to deliver, shift focus to delivering new feature with significantly reduced time to understand the impact of the changes.  Short term decisions can lead to extra work in the future when the wider picture is known.  </p> <p>Engineers under increased pressure increasingly seek short-cuts to their work in order to keep up.</p>"},{"location":"practices/tech-debt-notes/#business-or-market-change","title":"Business or market change","text":"<p>A significant shift in business goals, customer needs, or market environment can significantly affect development work and render it obsolete. Significant technical debt up to entire systems may be required to keep pace and stay relevant when plans pivot.  This can create legacy systems that no one is able or is motivated to work on.</p>"},{"location":"practices/tech-debt-notes/#limited-understanding-or-experience","title":"Limited understanding or experience","text":"<p>A lack of understanding of the business domain or technology used to create the solution leads to less optomal solutions that require rework.  Limited understanding of the technology used to build solutions can lead to sub-optimal code and design, requiring rework and greater troubleshooting.</p>"},{"location":"practices/tech-debt-notes/#limited-resources","title":"Limited resources","text":"<p>Technical debt requires exponentially more time to manage and work around. Teams need to factor in technical debt to planning work, including time spent aleviating the debt so it does not grow to unsustainable levels.</p>"},{"location":"practices/tech-debt-notes/#limited-sharing-of-knowledge","title":"Limited sharing of knowledge","text":"<p>Code that is undocumented and created without thought of readability and maintenance increased the effort required to use that code and build new features upon it.</p> <p>Issues such as regularly conflicting priorities, limited cross-team communication or poor quality are organisational debt, attributable by not investing time in how people can work effectively.</p> <ul> <li>limited understanding</li> <li>tools and workflows no longer fit for purpose</li> <li>technical architecutre constraints</li> </ul> <p>most often incured by decisions made due to increased pressure the organisation faced at the time the decision was made.</p> <p>Decisions made due to pressures on or from witin the organisation.  </p> <p>Pressures people put on themselved, sometimes unreasonably so.</p> <p>time pressures (assumed or actual) driving hurried decisions</p> <p>changing situations (change in requirements, startup pivit, tooling issues, process, people)</p> <p>communication challenges</p>"},{"location":"practices/tech-debt-notes/#aquiring-debt","title":"Aquiring debt","text":"<p>Technical debt is often driven by a need to balance business goals (and its customers) with the capability of the engineering teams to deliver (and remain productive in the longer term).  </p> <p>Over a the longer term, Software entropy is another form of technical debt where an engineering team is inexperienced or unable to invest time in actively evolving the overall system design, leading to a deteriating product that requires an increasing effort to maintain.</p> <p>In a broader sense, technical debt refers to many kinds of issues detrimental to effective delivery by technical teams, e.g. archtecutre, infrastructure, documentation, process, tooling, etc.  </p> <p>Some technical debt issues are also driven by organisational debt. TODO: add reference</p> <p>Technical debt over time</p> <ul> <li>design decisions early on lead to a system that can no longer scale or is otherwise limited</li> <li>inconsistent approach to code design and documentation limits the ability to extend and lenthens the time taken to maintain the system</li> <li>manually intensive opperations, e.g. running batch processes, data fixes, limit time available for new features</li> </ul>"},{"location":"practices/tech-debt-notes/#postive-technical-debt","title":"Postive technical debt","text":"<p>Example: Getting to market quickly with an essentail feature to start generating revenue.  Once revenue is generated then more time is available to create a maintainable system</p> <p>Aleviate: Capture important business and technical learnings from the project (during or straight after) reduces the rework to approximately a third of the effort, compared to starting again without oany prior knowledge.</p> <p>Planning the delivery of a longer-term solution and understanding the constraints that drive the switch-over from the short-term tactical solution time</p> <p>Context: Common in early-stage startups where tactical solutions are used to discover a sustainable market or where a company is moving quickly into a new or quite different area of business.</p> <p>Also described as active or intentional technical debt</p> <p>Making decisions without all the facts</p> <ul> <li>waiting for all the facts can miss valuable opportunities</li> <li>managing the level of risk around technical debt allows for experimentation and agility of the business without over-burdening the engineering teams ability to deliver</li> </ul>"},{"location":"practices/tech-debt-notes/#negative-techical-debt","title":"Negative techical debt","text":"<p>Example: writing code very quickly to satisfy unrealistic time pressures and creating code that is hard to work with and add more features. code tends to require significant rework (more than a simple refactor)</p> <p>The debt is compounded if the learning when creating the quick solution is not captured and available for the rework of the solution.  </p> <p>Without capturing the business &amp; technical experiences learned, rework is essentially starting from the beginning and will require learning everything again.</p> <p>Aleviate: Ensure time is planned to capture important information regarding requirements and implementation</p> <p>Context:</p> <p>Chaning deadlines and requirements can hamper effective communication and remove essential time for considering the correct approach.  </p> <ul> <li>building the wrong thing due to limited or incorrect understanding of the requirement</li> <li>insufficient time to consider the most appropriate time</li> </ul>"},{"location":"practices/tech-debt-notes/#aquiring-debt_1","title":"Aquiring debt","text":""},{"location":"practices/tech-debt-notes/#retrospective-assessment","title":"Retrospective assessment","text":"<p>When first starting to address technical debt</p> <ul> <li>journal tech debt as its is discovered and review at retrospectives to decide on actions</li> <li>capture potential issues in a document e.g. wiki (minimising interuption to flow)</li> <li>carve out time to expand on potential issues</li> <li>convert agreed upon as specific tickets, labelled as tech debt or part of an Epic style ticket to help understand the scale of debt</li> </ul>"},{"location":"practices/tech-debt-notes/#understand","title":"Understand","text":""},{"location":"practices/tech-debt-notes/#managing","title":"Managing","text":"<p>When reviewing a system, the principle of the technical debt is the cost of resolving all the idenfied technical debt</p> <p>ensuring the constraints that drove the need for debt are understood is paramount to managing and alieviating the debt in a timely manor.</p> <ul> <li>acknowledgement by all that technical debt exists and must be effectively managed</li> <li>agree on what technical debt effects the team and plan to aleviate the debt with limited value or most detrimental effect</li> <li>balance feature delivery with the ability to effectively deliver features by understanding the wider value proposition of new features to the business and a realistic view of the impact of technical debt.</li> <li>review ways of work to limit inadvertant debt, e.g. cover working practices and challenges in a retrospective with engineering team and product manager</li> </ul> <p>Specifically for the engineering team:</p> <ul> <li>automate tooling to support code quality and highlight potential technical debt, e.g. code analysis tools during development and automated quality checks in continuous integration</li> </ul>"},{"location":"practices/tech-debt-notes/#measuring","title":"Measuring","text":"<ul> <li>Bugs: issues raised as bugs as a basic indicator of quality. An increase in long-standing bugs indicates a software system in increasing trouble</li> <li>Churn: amout of rework on code indicates a limited understanding of the domain or requirements changing rapidly</li> <li>cycle time: working on the same feature indicates a limited understanding of the requirement or rapidly changing requirements</li> </ul>"},{"location":"practices/tech-debt-notes/#aleviate","title":"Aleviate","text":""},{"location":"practices/tech-debt-notes/#summary","title":"Summary","text":"<p>\"slow is smooth, smooth is fast\" matra encouraged by the author, John Stevenson.</p>"},{"location":"practices/tech-debt-notes/#maing-timely-decisions","title":"Maing timely decisions","text":"<p>Decisions made now can adversly affect future productivity. Making decisions too soon can be as impactful as making them too late</p> <p>Decisions that bind the team or organisation down a particular route, which becomes increasing challinging and costly to deviate from.</p> <p>Understanding the value of delivering features with respect to the timeframe in which those features are of value</p> <p>e.g. supporting a sales &amp; marketing team to roll out a campain in the run up to black friday.  If delivered after black friday, then of no value.</p> <p>If delivered to quickly, decisions may have been unneccessarily rushed and proper levels of though not been part of the process, increasing the risk of mistakes and errors going live.</p> <p>Understanding the value timeframe and the values within the campain ensures enough resources are dedicated to the campain in balance with the rest of the work required by the organisation</p>"},{"location":"practices/tech-debt-notes/#what-does-the-team-assume-is-tech-debt","title":"What does the team assume is tech debt","text":""},{"location":"practices/tech-debt-notes/#understanding-tech-debt-status","title":"Understanding tech debt status","text":""},{"location":"practices/tech-debt-notes/#documentation","title":"Documentation","text":"<p>A lack of essential documentation increased the effort required to maintain and extend an existing codebase</p> <ul> <li>Define common practices in highly accessible and well structured documentation</li> </ul>"},{"location":"practices/tech-debt-notes/#tooling","title":"Tooling","text":"<p>Time should be factored in to ensure effecive use of the core development tools and delivery tool-chain (Continuous Integraion, Continuous Delivery, etc.)</p> <p>Tools not fit for purpose</p> <p>The alure and adoption of (shiny) tools without due dilligance - tools and workflows should be evaluated sufficiently to</p>"},{"location":"practices/tech-debt-notes/#team","title":"Team","text":""},{"location":"practices/tech-debt-notes/#technology","title":"Technology","text":""},{"location":"practices/tech-debt-notes/#process","title":"Process","text":""},{"location":"practices/tech-debt-notes/#techniques-to-manage-technical-debit","title":"Techniques to manage technical debit","text":"<p>Combatting technical debt due to communication and understanding</p> <ul> <li> <p>5 whys: ensuring a deeper understanding of a situation or requirement is atained</p> </li> <li> <p>6 thinking hats: a critical thinking approach to discussion that seeks to create a rounded, positive and effective discussion on a particular subject.  Discussion is carried out with one focus at a time and with all participants aligned in their thinking approach at any one time.  </p> </li> <li> <p>MoSCow rules: priorities within the short term (day, week) to ensure essential features are delivered and critical issues are resolved (must have, should have, could have priority of tickets)</p> </li> <li> <p>rolling wave planning: ensures daily work is organised within a wider context of goals for the comming months</p> </li> <li> <p>value stream mapping: identify the critical path for delivering valuable product/service to the customer and use as a basis for optomising the delivery workflow (technical &amp; non-technical)</p> </li> </ul>"},{"location":"practices/tech-debt-notes/#case-study-parrot-cages","title":"Case Study - Parrot cages","text":"<p>A person in the sales in the company tries to buy a parrot cage online and has a very unsuccessful experience, very limited choice and very expensive.  This raises the question, does everyone have this esperience and is there a viable business to be created.</p> <p>The sales person works with engineer to build the essence of a website that will guage if there is interest in an online shop for parrot cages.  It is a simple html page with some CSS that shows parrot cages of different styles and costs.</p> <p>Rather than build a full shopping cart experience, when customers press the buy button the website explains that the business is just starting and allows them to register interest via a form that captures contact details and preferences of bird cages.</p> <p>When significant interest was measured, the app was redeveloped into a shopping cart application using external partners to manage payment, manufacturing and distribution of popular parrot cages.</p>"},{"location":"practices/tech-debt-notes/#case-study-twitter-technical-stack","title":"Case Study - Twitter technical stack","text":"<p>Twitter built their service using Ruby Rails architecture which enabled rapid development and architected to support the estimated user base.</p> <p>Twitter quickly exceeded all expectations and the \"fail whale\" became a regular appearance on their website, indicating that the service was unavailable due to too many user.  The use of the service had also switched from a mobile driven service to heavy website use.</p> <p>This is an example of success driving technical debt, because the technical architecture decisions were based on significantly inaccurate estimations and a different business model.</p> <p>An extensive change to the technical architecture of twitter was required to meet the ever increasing capacity, which continued exponential growth.  However, more technical debt was created by working on the existing architecture in order to provide some level of service</p> <p>The new architecure was built in parallel with the original service and was a significantly different approach that required new skills as well as technologies.</p> <p>Too much success too quickly can cause technical debt.</p> <p>Reference: https://www.theatlantic.com/technology/archive/2015/01/the-story-behind-twitters-fail-whale/384313/</p>"},{"location":"practices/tech-debt-notes/#case-study-rework-a-legacy-system","title":"Case study - Rework a legacy system","text":"<p>A legacy system is running successfully in producition but is very costly to maintain and challenging to extend.</p> <p>A large international finance organisation had a system that collected valuable business data from 20 different sources and distributed the data in near-realtime across 3 geographical regions.</p> <p>The legacy system was able to distribute data, although had a significant scalability problem when data levels became too high.</p> <p>The team who created and maintained the system were no longer at the organisation, all that remained was the undocumented codebase.</p> <p>The core workflow of the system was assertained and the interaction from all the other applications was documented, to  understand the communication recieved.  This allowed a detailed application program interface (API) to be created and an appropriate solution created without concern of most of the inner workings of the legacy system.</p> <p>Understanding the workflow and interaction with external systems allowed a new implementation of the system to be quickly created.  Tests discovered in the legacy system were applied to the new implementation to help verify the correct behaviour.</p> <p>A testing environment was established and each development team responsible for an external system was invited and supported during the testing phase, to provide as close to live implementation testing as possible.</p> <p>With an acceptance that techical debt may be neccessary at times,</p>"},{"location":"practices/technical-debt/","title":"Technical Debt Overview","text":"<p>Technical debt occurs when balancing the needs of the business with the productivity of engineering teams. Choosing tactical solutions, proceeding with limited understanding and taking short-cuts are all generators of technical debt to varying degrees.</p> <p>Intentionally acquiring technical debt can have significant business value if more can be achieved in a shorter time-frame, providing much needed revenue and time to implement a longer term solution.</p> <p>Inadvertently acquiring debt beyond the capability of a team leads to spiralling debt, increasingly limiting the ability to get work done.</p> <p>Unresolved debt leads to feature delivery delays, hidden risks, spiraling project costs, engineer burnout and eventually leaving in frustration.  </p> <p>Engineers use the term technical debt to communicate tasks are needed to improving the current system, along-side tasks only focused on business features. Naturally a system upon which teams can deliver timely features has intrinsic value for the business and its customers.</p> <p>As a metaphor, technical debt has grown to reflect any issue having a negative impact on the productivity of technical teams, e.g. infrastructure challenges, architectural decisions, design complexity, test coverage, documentation quality and development practices.</p> Technical Debt original definition - Ward Cunningham, 1992, OOPSLA Conference <p>Ward Cunningham suggested technical debt could be a benefit.  An engineer creates a quick solution with limited understanding of the problem, with the intention to rewrite the code once understanding had improved and alleviate the debt.</p> <p>The danger occurs when the debt is not repaid. Every minute spent on code that is not quite right for the programming task of the moment counts as interest on that debt. </p> <p>Entire engineering organizations can be brought to a stand-still under the debt load of an unfactored implementation, object-oriented or otherwise.</p> Comparison to financial debt <p>A debt is a value gained that must be repaid along with the accumulated interest over the duration of the debt.  </p> <p>Aquiring debt can be a wise investment in the future or become an increasingly unsustainable cost.</p> <p>Intentional Debt     A mortgate allowing the purchase of a house, repaying a large debt in managable amounts over an established time period</p> <p>Inadvertant debt     Credit card debt where funds available for monthly repayments become insufficient to prevent spiraling interest growing the debt to unsustainable levels</p>"},{"location":"practices/technical-debt/#debt-scenarios","title":"Debt scenarios","text":"<p>Intentional technical debt</p> <p>The benefit out-ways the potential issues, which are resolved across the next releases</p> <ul> <li>exploring a new market, creating tactical (throw-away) solutions whilst retaining the learning required to implement a strategic longer-term solution (or develop a strategic solution in parallel).</li> <li>continuing with a releasing that delivers customer partial set of features, or has known issues, rather than delay the release.</li> </ul> <p>Inadvertent technical debt</p> <ul> <li>hack together a solution to meet unrealistic deadline pressures.  No time is available to document lessons learned or plan time to mitigate the risks taken due to working on the next deadline.</li> <li>technical decisions made with limited understanding lead to a solutions that struggles to fit the long term strategy, but the system is so ingrained into development it is perceived as unchangeable.  Work continues to be hampered until ultimately a major redevelopment of the solution is agreed.</li> </ul> <p>Death March (Edward Yourdon) is a term describing projects that continue to wallow in significant technical debt.</p>"},{"location":"practices/technical-debt/#identifying-debt","title":"Identifying debt","text":"<p>Time may be limited when first starting to address technical debt as time pressures are a major factor in creating technical debt.</p> <p>Identifying debt and its risks can be time-consuming, so start by apply simple techniques</p> <ul> <li>journal technical debt as its is discovered during feature development</li> <li>use a shared document to capture potential issues e.g. wiki (minimise interruption to engineer flow)</li> <li>carve out time to expand on potential issues, individually and collectively as a team</li> <li>review at retrospectives to decide on actions (see Managing debt)</li> <li>convert agreed upon issues to actionable tickets, labelled as technical-debt</li> <li>create an Epic style ticket linking all technical debt issues to help understand the scale of debt and if the debt is growing</li> </ul>"},{"location":"practices/technical-debt/#managing-debt","title":"Managing debt","text":"<p>When the driving factors of technical debt are understood, organisations can choose an appropriate level of technical debt and carefully manage it as part of the software development workflow.</p> <p>Project management and engineering teams should agree on what constitutes technical debt and balance the value provided by the debt with its detrimental effects.  </p> <p>Consider these aspects when reviewing the technical debt of a project:</p> <ul> <li>Type: intentional or inadvertent</li> <li>Impact: how is affecting the team, other systems and the business</li> <li>Amount: perception of how large the debt is</li> <li>Time-frame: duration of positive and negative effects</li> <li>Age: from active work or inherited legacy</li> </ul> <p>Opportunities to alleviate technical debt should be included during planning work and experiences shared during retrospectives.</p> <p>The collective positive and negative effects of technical debt are specific to an organisation, so should be agreed &amp; understood across that organisation.</p>"},{"location":"practices/technical-debt/#references","title":"References","text":"<p>The McKinsey survey shows it is common to have 20% of technical budgets diverted to resolving issues related to technical debt. [1]</p> <ol> <li>Demystifying digital dark matter: A new standard to tame technical debt</li> </ol> <p>Wikipedia: Technical Debt</p>"},{"location":"practices/test-driven-development/","title":"Test Driven Development","text":"<p>Test driven development is a technique to help write the correct code and write it well.</p> <p>TDD is design tool to support understanding of thebehaviour the software should satisfy.  The ability to test the code is a consequence of this design activity.</p>"},{"location":"practices/test-driven-development/#tdd-approach","title":"TDD approach","text":"<p>Start by writing a single test representing a single piece of behaviour, an assumption of the understanding so far gathered from the requirements or other communications.  </p> <p>Behaviour defined through a test provides a specific constraint to focuses the implementation of code that satisfies the test.</p>"},{"location":"practices/test-driven-development/#tdd-basic-steps","title":"TDD basic steps","text":"<ol> <li> <p>Write a test</p> <ul> <li>this may require creation of a non-functioning class or mock object to compile the test</li> </ul> </li> <li> <p>Run the test</p> <ul> <li>the test should fail, go red, so that you know it is testing something</li> </ul> </li> <li> <p>Write the minimum amount of code in your class to make the test pass</p> <ul> <li>write your code as simple as possible, but no simpler - leave options open to last responsible moment</li> </ul> </li> <li> <p>Run the tests</p> <ul> <li>the test should pass, go green</li> </ul> </li> <li> <p>Write another test / refactor code</p> </li> </ol>"},{"location":"practices/test-driven-development/#tools","title":"Tools","text":"<p>Unit Testing frameworks JUnit is the most common unit test framework for Java development and is widely supported by IDE's, build tools, continuous integration servers and many other supporting tools.  Typically, when you create a new Java project in your IDE, JUnit is included as a project library or added when you create a JUnit Test class from your IDE menu.</p> <p>Hamcrest is often used with JUnit to make the tests more human readable, great when you are coming to a new code base and are reading through lots of tests or when you are debugging code you wrote more than few days ago.</p> <p>Hamcrest core is now included in JUnit since version 4.5 onwards.</p>"},{"location":"practices/test-driven-development/#learning-tdd-well","title":"Learning TDD well","text":"<p>The best way to learn TDD well and feel comfortable with this technique is to practice, see the section on deliberate practice for ideas on how to practice TDD.</p>"},{"location":"practices/test-driven-development/#tips","title":"Tips","text":"<p>Use meaningful test names With JUnit 4.x you use the @Test Java annotation, meaning you no longer need to use the word test in the method name.  The test method name should represent what you are trying to test from the user or domain point of view.</p> <p>A nice way to start the test is to use the word should, as this invites anyone looking at the test to consider if it represents meaningful behaviour and change or throw away the test if it does not add value.</p> <p>Test one thing at once Be wary about multiple assert statements in a test.  If you have multiple asserts in your test method, you may be too ambitious in what you are trying to test.  Or you may have a behaviour that needs further decomposition.</p> <p>When refactoring code, those tests that have a number of assertions are more likely to break if they are testing several concepts within the same test.</p> <p>Managing test data Each test will reset the state - that's the way JUnit works (nUnit maintains state, so has to be cleared manually)</p>"},{"location":"practices/test-driven-development/#tdd-kanban","title":"TDD Kanban","text":"<p>To help keep you in a good flow when you are following the test first approach, it can be useful to use a simple kanban board to manage your flow.</p> <p>The kanban board has three lanes as follows</p> <ul> <li>Test - you are writing a single test</li> <li>Code - you are writing code to pass a particular test that is failing</li> <li>Refactor - you are changing the internal workings of your code</li> </ul> <p>You only have one card on your kanban board (this is your work in progress limit), this reminds you which activity you should be working on and should help you get into the test-code-refactor routine.</p> <p>Using the TDD Kanban board</p> <p>To start, place your one and only card on the test lane of the kanban board.</p> <p>Once you have written a failing test and run your tests, move the card on the kanban board to the code lane and write enough code to make the test pass.</p> <p>Once you have written enough code to make the test pass (running all tests), move the card on the kanban board to the refactor lane.</p> <p>When you have finished your refactor work and have run the tests, move the card back to the test lane and write another failing test.</p>"},{"location":"practices/test-driven-development/#resources","title":"Resources","text":"<ul> <li>List of TDD Problems</li> <li>Effective exercises for teaching TDD</li> </ul>"},{"location":"practices/behaviour-driven-development/","title":"Behaviour Driven Development","text":"<p>Behaviour-driven development (BDD) is an evolution of the ideas behind agile software delivery. With its roots in test-driven development, domain-driven design, and automated acceptance testing, BDD focuses on the ways an application is expected to work - its behaviour.</p> <p>Understanding your domain and who your stakeholders are, identifying and exploring requirements, automating acceptance criteria, and delivering working and tested software.</p> <p>BDD is writing software that matters</p> <p>Introducing BDD - Dan North</p>"},{"location":"practices/behaviour-driven-development/#bdd-concepts","title":"BDD concepts","text":"Concept Description Outside-in understand system usage from the outside (user interfaces) inwards Pull based system Produce only that which is ready to be used by people, systems, or code closer to a boundary or UI Multiple stakeholder Core and incidental stakeholders - Core stakeholders define the vision and often provide budget; Incidental stakeholders support the solution delivery and may influence that solution Multiple scale BDD applied from end to end, from small feature set to enterprise projects High automation Automation of testing (acceptance &amp; unit) hence reducing the expense of scenario and examples testing; Enabling and inviting change through acceptance tests and reducing the impact of change Features A slice through the whole solution; defined by multiple scenarios describing interactions stemming from user interface (outside-in). Features are defined with respect to business value Feature Injection Only work on features that deliver value, according to the project vision/goals (in the context of the organisations goals) Ubiquitous language Using the language of the domain consistently, end to end in the solution artefacts; name things with respect to their behavior in the domain. Using Given,When,Then also provides structure to the ubiquitous language"},{"location":"practices/behaviour-driven-development/#example-exercise","title":"Example Exercise","text":"<p>This is a BDD style description of the case study requirements for the service called Boris Bikes.</p> <p>The scenarios give you ideas of behaviour you should consider when trying to implement the case study in a Test Driven Development approach.</p> <p>Pick a scenario and write one or more tests that will satisfy that scenario.</p> <p>The Features here are not necessarily in line with the scenarios, please feel free to create new features that are more relevant.</p> <p>City Bike Hire Service</p> <pre><code>Feature: City Bikes Service\n  In order to provide a convenient way to get around the City of London\n  As the Mayor of a city\n  I want to provide a bike hire service\n\n  Scenario: scenario name\n    Given .\n    When .\n    Then .\n\n\nFeature: Commute around the city quickly\n  In order to get around London quickly\n  As a commuter tired of using the tube (and tube strikes)\n  I want to hire a bike to get me around central London\n\n\n  Scenario: Hire a bike\n    Given there is a bike available for hire\n    When I identify myself to the bike hire station\n    Then I get access to the bike\n\n  Scenario: Return a bike\n    Given I have taken a bike for hire\n    When I identify myself to the bike hire station\n    And return the bike correctly\n    Then my bike hire ends\n    And payment is taken\n\n  Scenario: Find a parking slot\n    Given I have taken a bike for hire\n    And my current bike station has no free slots\n    When I ask the hire station for available slots\n    Then I get the number of slots for the 3 nearest bike stations\n\n  Scenario: Register\n    Given .\n    When .\n    Then .\n\n  Scenario: Identify myself as a registered user\n    Given I am a regular rider\n    When I hire a bike\n    Then I use my key at the bike station to identify myself\n\n  Scenario: See if a particular bike station has bikes\n    Given .\n    When .\n    Then .\n\n  Scenario: Find nearest location with available bikes\n    Given .\n    When .\n    Then .\n\n  Scenario: Book a bike in advance\n    Given .\n    When .\n    Then .\n</code></pre>"},{"location":"practices/behaviour-driven-development/#references","title":"References","text":"<p>Introducing BDD - Dan North Cucumber - Behaviour Driven Development Behaviour Driven Development - Wikipedia</p>"},{"location":"practices/deliberate-practice/","title":"Deliberate Practice","text":"<p>People learn by doing. People need to do something many times before it becomes instinctive or ingrained.  </p> <p>Deliberate practice is about doing something in order to learn rather than produce a finished product with all the presures that entails.</p> <p>Deliberate practice is not aimed at being an effective way to develop a software product, rather training tools which encourage high levels of learning and improve skills effectively.</p> Academic Research <p>In studies of accomplished individuals, researchers have found few signs of precocious achievement before the individuals started intensive training. Similar findings have turned up in studies of musicians, tennis players, artists, swimmers, mathematicians, and others.</p> <p>Delivery can limit learning</p> <p>Working on a software project under deadlines and other pressures detracts from what you can learn, unless you take the time to setp back from the \"doing\" and \"deadline\".  </p> <p>Most fundamentally, what we generally do\u2026. isn\u2019t designed by anyone to make us better at anything. Usually it isn\u2019t designed at all: We are just given an objective that\u2019s necessary to meeting the employer\u2019s goals and then expected to get on with it.</p>"},{"location":"practices/deliberate-practice/#kata","title":"Kata","text":"<p>Kata is a form of deliberate practice that uses repetative actions to improve skills.  Code kata involves solving different challenges or the same challenge in different ways.</p> <p> Code kata is used to implement different code solutions to the same design challenge.</p> <p> Architecture kata is a way practice creating high-level solutions to business challenges.</p>"},{"location":"practices/deliberate-practice/#code-dojo","title":"Code dojo","text":"<p>A coding dojo is a group event to help you learn a language, a framework, tool or development technique.  Ideally the scope of the challenge should be designed around what is most useful to learn each time.</p> <p>Attending a coding dojo on a regular basis gains a lot of experience to become more effective when working on projects.  </p> <p>Dojo experiences create a familarity and higher comfort level with pair programming, increating the levels of pair programming and promiscuous pairing happen at work.  </p> <p>A coding dojo provides exposure to the practical experience of others, especially things people forget they know or do almost instinctively.</p> <p>Next simplest thing that can be done</p> <p>Dojo events are only 1 to 2 hours long, so only doing the next simplest thing each time minimises the risk of analysis paralysis (discussing many options and paths without actually coding anything).</p> <p>By considering the next simplest thing that can be done, a specific piece of code can be written and evaluated once it has been created.  Then the next simplest thing can be done.</p> Classic style dojo can be challenging <p>The original coding dojo approach was to have a single laptop connected to a projector (or very large screen monitor) so everyone present can see the coding taking place.</p> <p>At regular intervals a pilot and co-pilot take there positions at the computer and have a set time to create a new test, implement code to fix a failing test or refactor the code.</p> <p>The pilot writes tests or code whilst the co-pilot asks questions and discusses the approach they are taking.  These discussions are spoken aloud so the audience can clearly hear them.</p> <p>If the pilot and co-pilot need help, they should ask the audience for guidance.</p> <p>This approach can feel quite high pressure, especially when in a group of people that are not well known.</p>"},{"location":"practices/deliberate-practice/#group-show-and-tell-dojo","title":"Group show and tell dojo","text":"<p>Collectively identify one or more challenges to work on and divide up in to small groups of 2 to 3 people.  Larger groups tend to lead to disjointed discussions and very limited progress.</p> <p>Each group decides how to tackle the challenge and takes turn driving the code.</p> <p>Toward the end of the event, each group presents their design decisions and interesting aspects of the code.</p> <p>Share code dojo results</p> <p>Use a code sharing service such as GitHub / GitLab to share the code with the group and wider community, continuing the discussion and learning opportunity beyond the dojo event.</p> <p>London Clojurians Org is an example of a GitHub Organisation which the community shares there dojo experiences.</p> <p> London Clojurians Organisation</p>"},{"location":"practices/deliberate-practice/#code-retreat","title":"Code Retreat","text":"<p>A group event where the same code challenge (code kata) is attempted over again with different pairs of people each time.</p> <p>The group is organised into pairs (or trio's if there is an uneven number in the group).</p> <p>Each pair starts the challenge from scratch and has a set time to attempt the solution (typically 45 - 60 minutes).</p> <p>Pairs show and tell their approaches, with the ephasis on why a design was chosen.</p> <p>After a short break, the group is organised into different pairs and the same challenge is attempted from scratch.</p> <p>A retrospective is run at the end of the day, reflecting on the code retreat approach and designs shared throughout the day.</p>"},{"location":"practices/deliberate-practice/architecture-kata/","title":"Architectural Katas","text":"<p>An architectural kata is a deliberate practice technique to grow experience and skills with respect to architectural design.</p> <p>A challenge is selected (or assigned), discussed and a solution presented - usually in a group setting.</p> <p>The Architectural Katas website generates kata challenges to solve as well as the recommended rules to maximise the value of the activity.</p> <p>Architectural Katas website</p> Kata: Mock Internet UN <p>Organization running \"Mock UN\" events wants to take its events online, permitting students to participate online</p> <p>Requirements: student-diplomats must be able to video-chat with one another; student-diplomats must be able to \"give speeches\" to the \"assembly\" (video-chat to the entire group); (mocked) world events (created by moderators) distributed via (mock) \"news sites\"; moderators must be able to monitor any video chat for appropriateness</p> <p>Users: 500 or so \"diplomats\" per \"mock UN\" gathering; dozens of moderators per \"mock UN\"; many \"mock UN\"s simultaneously; no new hardware requirements on students</p> <p>A kata is a technique from martial arts training, encouraging repetition of actions to increase skill.</p> <p>Related: Code kata</p> <p>Code kata encourages the same challenge to be solved in several different ways, to grow experience with solving a solution with a programming language.</p>"},{"location":"practices/deliberate-practice/architecture-kata/#group-practice","title":"Group practice","text":"<p>The architectural kata technique can be used within a group setting, with one or more groups.</p> <p>Groups of 3 to 5 people tend to be the most effective, especially where there are a good mixture of experiences.</p> <p>Architectural kata workshops are commonly run at technical conferences and Ted Neward has run many of these workshops</p>"},{"location":"practices/deliberate-practice/code-kata/","title":"Code Kata","text":"<p>A code kata is a small coding exercise that you repeat over and over again, helping you improve your coding skills.</p> <p>The kata is not about producing a usable solution as this detracts from the learning experience.</p> <p>A coding kata for a developer is like an athlete doing warming up exercises before a run.</p> <p>A coding kata is often used to warm up your skills before you start work each day.  Working on a kata problem that is not related to any of your work allows you to think about how you write code rather than thinking of delivery and meeting estimates.</p> <p>Coding kata often uses a test first approach, as in TDD.</p> <p> Kata Challenges - Practicalli Clojure</p> <p>Coding Dojo website</p> <p>Randori - Wikipedia</p>"},{"location":"practices/deliberate-practice/code-kata/#prepared-kata","title":"Prepared Kata","text":"<p>A presenter shows how to solve a challenge using TDD using baby steps</p> <p>Each step must make sense to everyone present</p> <p>Audience only interrupts if something is not understood.</p>"},{"location":"practices/deliberate-practice/code-kata/#group-kata","title":"Group Kata","text":""},{"location":"practices/discussions/","title":"Effective discussions","text":"<ul> <li>one-to-one meetings</li> <li>standup</li> <li>retrospective</li> </ul>"},{"location":"practices/discussions/asking-questions/","title":"Asking Questions","text":"<p>The way to get useful answers is to ask effective questions.</p> <p>The act of creating an effective question can often produce the answer without even needing to share the question with others (although it is valuable to still ask for a wider scope of answers).</p> <p>The act of creating a question allows for a clear analysis of what the problem is, what has been tried already.</p> <p>No need to apologies for asking</p> <p>People enjoy answering other peoples questions as it provides a sense of satisfaction that they could help.  There is never a need to apologies that your are asking a question, especially if you think the answer should be obvious</p> <p>There are no silly questions</p> <p>People are not born omnicient, they spend their lifetime learning on a unique path.  No two people will understand all the same things in the same way, even when working in the same area.</p>"},{"location":"practices/discussions/asking-questions/#ask-an-engineering-question","title":"Ask an Engineering question","text":"<p>The more detailed an engineering question is, the more likely that an appropriate answer will be given.</p> <ol> <li>Convey the goal of your question (what are you trying to achieve)</li> <li>Establish the facts as clearly as possible<ul> <li>What steps did you try</li> <li>How does someone else reproduce this issue (create the simplest possible example)</li> </ul> </li> <li>Convey assumptions made</li> <li>Examples, examples, examples</li> <li>Review and refactor (remove unnecessary words and information, reread and rewrite to ensure the question is easy to understand)</li> </ol> <p>Fast questions take more time</p> <p>Asking questions with little or no thought leads to much more time spent explaining the question once its out there.</p>"},{"location":"practices/discussions/asking-questions/#example-questions","title":"Example questions","text":"<p>Some example engineering questions (TODO: review questions in slack / stack exchange for useful, not so useful examples)</p>"},{"location":"practices/discussions/asking-questions/#the-art-of-asking-questions","title":"The Art of Asking Questions","text":"<ol> <li>Demonstrate preparedness for the conversation</li> <li>Illustrate your expertise without showing off</li> <li>Invite others to deepen or broaden their thinking and challenge held beliefs</li> </ol> <p>Article from the Harvard Business School</p>"},{"location":"practices/discussions/five-whys/","title":"Five Whys","text":"<p>Five whys is a simplistic technique to explore cause-and-effect relationships underlying a particular problem.</p> <p>The primary goal is to quickly determine the root cause of an issue by repeating the question \"Why?\" five times. The answer to the fifth why should reveal the root cause of the problem.</p> <p>5 Whys may not be enough</p> <p>More questions or a deeper analysis may be required to find the actual root cause, especially for more intricate challenges</p> <p>Process or systemic issues can often generate responses about lack of time, resources or understanding.  Whilst these may be an effect, they are not specific enough to be the root cause.</p> <p>The troubleshooter should avoid assumptions and logic traps and focus on tracing the chain of causality in increments to a root cause which has connection to the original problem.</p> <p>The technique was described by Taiichi Ohno at Toyota Motor Corporation.</p>"},{"location":"practices/discussions/five-whys/#example","title":"Example","text":"<p>Challenge: The web service is not responding</p> <ol> <li>Why? - the service monitor displays an error</li> <li>Why? - pinging the heartbeat of the service returns a 404</li> <li>Why? - the request is not recognised by the server</li> <li>Why? - the heartbeat url has changed</li> <li>Why? - a change was made to the code but monitor configuration not updated</li> </ol> <p>This example shows a disconnect between development and operations</p> <p>Five Whys can feel negative to the reciever</p> <p>Practice the technique on yourself to understand how the tone of questions can be percieved as annoying (negative) or curious (constructive)</p> <p> Five Whys - Wikipedia</p> <p> Root Cause Analysis - Wikipedia</p>"},{"location":"practices/discussions/one-to-one-meeting/","title":"One to One meeting","text":"<p>One to one meetings are an opportunity for two people to improve their relationship via regular discussions, within a safe environment of open and honest communication.</p> <p>Managers can guide their reports as well as ensure their own objectives are aligned with the major needs of the team.</p> <p>Discussions can cover a wide range of topics about personal and career development, first checking with each person what they are comfortable discussing.  A person is far more than the constraints of their current role and wider discussions can help create a str</p> <p>The meetings should continually look for challenges and identify action points on either side to make the meetings even more valuable.</p>"},{"location":"practices/discussions/one-to-one-meeting/#aspects","title":"Aspects","text":"<ul> <li>As a manager, ensure personal goals and objectives are aligned with each team member\u2019s priorities</li> <li>Provide regular, open feedback, both positive and constructive. Identify any minor issues quickly through discussion before they escalate and become substantive problems</li> <li>Facilitate open and honest communication on a regular basis</li> <li>Help staff to feel motivated, listened to and valued</li> <li>Build your interpersonal team relationships and encourage your direct reports to have trust in you, allowing them to speak freely without any peer pressure from the broader team</li> <li>Show team members that you respect and value their time</li> </ul>"},{"location":"practices/discussions/one-to-one-meeting/#safe-environment","title":"Safe environment","text":"<p>Each person should feel able to talk as a peers, ignoring any positional power or roles.</p> <p>Those in positional power should demonstrate patience and gratitude throughout the meeting, thanking the person for their time and for the information they have shared.</p>"},{"location":"practices/discussions/one-to-one-meeting/#people-and-roles","title":"People and roles","text":"<p>Who may have this type of meeting:</p> <ul> <li>manager or team leader and a direct report</li> <li>mentor and mentee</li> <li>engineering colleagues</li> <li>engineer and business specialist</li> <li>sales or marketing with customers</li> </ul> <p>Aims</p> <ul> <li>build a closer relationship between the two people and provide</li> <li>understand an individuals needs and goals as well as understanding what motivates them</li> </ul> <p>Motivation</p> <ul> <li>Manager wants to support the employee so they are effective and motivated to continue their career at the company</li> <li>Mentor supports career and personal development, which may also</li> <li>Engineer can address a wider set of topbic or those challenges they are unclear how to resolve,</li> </ul> <p>Outcomes</p> <ul> <li>a clearer understanding the other person in the meeting (from both sides)</li> <li>actions to support the person, with priority and scope of value (when to action)</li> </ul> Anti-patter: Ego boost <p>The meeting should not be used by the Engineering manager to exert control or to derive feelings of superiority.</p> <p>The meeting is to enhance the relationship between two people and one person trying to exert pressure or control will damage that relationship.</p> <p>The manager or mentor should demonstrate genuine interest in the person being met with, listening to their answers carefully and helping them identify actions they can try to evolve situations discussed</p>"},{"location":"practices/discussions/one-to-one-meeting/#establishing-discussion","title":"Establishing discussion","text":"<p>An initial meeting should establish the following aspects early on, to help frame the meetings and identify any specific aspect to address or avoid.</p> <ul> <li>Identify and agree on purpose of the meetings</li> <li>Identify meaningful frequency of meeting, e.g. every 2 weeks or once a month</li> <li>Topics to be discussed during these meetings: project, company, career or personal</li> <li>identify issues that need to be addressed</li> <li>identify issues/topics to avoid or have addressed by others</li> <li>level of detail each person is confortable adressing</li> <li>Share and agree on the value of the meeting for each person involved</li> <li>Discuss the need to establish a level of trust for meetings to be of value</li> </ul> <p>revisit this establishing discussion at regular points to understand if the dynamics of the meeting should change</p> <p>Evolve the meetings</p> <p>Ensure the meetings remain valuable to both parties by regularly disussing the meeting format and topics discussed.</p> <p>Meetings may benefit from a change in duration, time or day.  Dont be afraid to end or extend a meeting as needed (as long as both parties agree)</p> <p>Manager Tools Basics</p>"},{"location":"practices/discussions/one-to-one-meeting/#attentive-listening","title":"Attentive listening","text":"<p>Attentive listening involves not only listening to the works and their underlying intent, but also observing tone, facial expressions and body language.</p> <p>An empathic listener is one who listens \u201cwith the intent to understand everything that the other person is trying to communicate, knowing that words are only part of the message.\u201d</p>"},{"location":"practices/discussions/one-to-one-meeting/#regular-feedback","title":"Regular Feedback","text":"<p>Provide positive feedback regularly to provide a source of motivation to the other person.</p> <p>Provide constructive feedback as early as possible to allow the other person to action improvements before a situation may deteriorated.</p> <p>Constructive feedback is a valuable experience when offered in a way intended to help.</p> <p>Prepare what you want to say in advance and be as specific as possible, inviting questions and thoughts from the other person  and agree together a plan of action for moving forward.</p> <p>Collaborative people should welcome feedback in return, encouraging others to tell them how they can better support them going forward.</p>"},{"location":"practices/discussions/one-to-one-meeting/#consistency","title":"Consistency","text":"<p>Both people involved should have an appreciation of the value of the one-to-one meeting and regularly work to ensure the meeting continues to deliver value for both over time.</p> <p>Avoid cancellation of the meeting without good reason and providing an explianation. Cancelation, especially last minute can signal to the other person that time with them is not as valued as it should be.</p> <p>Transparency, understanding and mutual support are the rewards for regular and consistent meetings.</p>"},{"location":"practices/discussions/retrospective/","title":"Retrospective","text":"Work in progress <p>A regular group discussion to share experiences about a previous period of work.</p> <p>A Retrospective is carried out at the conclusion of a significant piece of work, e.g. major feature delivered or project delivered.</p> <p>Feedback should be captured during a retropective and action points identified, along with a person responsible to ensure that action is completed.</p> Shared Drawing tool for Retrospectives <p>Many online drawing tools provide features to support an online interactive tool for retrospectives, e.g</p> <p> Atlassian Confluence retrospective template</p> <p> Miro retrospective templates</p> Mini-retrospective <p>A Mini-retrospective can be useful to look at a specific issue as a group or as an individual.</p> <p>Feedback can be captured in an individual journal when reviewing their own work and practices.</p>"},{"location":"practices/discussions/retrospective/#references","title":"References","text":"<p> Retrospective - Wikipedia</p> <p> Agile Retrospective Resource Wiki</p>"},{"location":"practices/discussions/rolling-wave-planning/","title":"Rolling Wave Planning","text":"<p>Rolling wave planning is an incremental project management technique for planning successive projects or successive features over time.  This planning technique works well when critical deadlines are required or imposed on delivery.</p> <p>A project plan is frequently updated, gaining more detail over time as more is learned about the project needs and constraints.  This information is regularly communicated to stakeholders to ensure expectaions continue to be met.</p>"},{"location":"practices/discussions/rolling-wave-planning/#waves","title":"Waves","text":"<p>Features (or projects) are planned in waves and at increasing levels of details</p> <ul> <li>features due soon have been defined in great detail over several iterations as undertanding was gained</li> <li>features due much further away are initially defined at a very high level, raising awareness of longer term work</li> </ul> <p>On a larger scale, projects can be priorities by agreeing acrosss all stakeholders the value of each project to the business and the timeliness in which they should be delivered.</p> <p>On a smaller scale, features from one or more projects can be priorities by value (and technical neccessity), again agreeing with all stakeholders.</p> Agreed deadlines should not change by stakeholders without concequences <p>When priorities and deadlines are agreed or imposed then they should remain fixed for all stakeholders, unless significant justification from stakeholders can be provided.</p> <p>Project managers should communicate the risks and delays inherent when stakeholders attempt to change priorities of work, shorten deadlines or extend the scope of work within an agreed deadline.</p>"},{"location":"practices/discussions/rolling-wave-planning/#pragmatic-plans","title":"Pragmatic plans","text":"<p>Realistic plans should be devised around value-based or imposed deadlines, ensuring there are sufficient resources available.</p> <p>Slack time should be included in plans to allow for unexpected challenges and to ensure engineering teams are not pushed into unsustainable workloads, e.g. avoid burnout.</p> Prevent Burnout by having a long-term view <p>One cause of Engineer burnout is continous sequence of deadlines, espcially when there are less than realistic time-frames involved.</p> <p>Establishing a longer-term view of project work allows managers to realistically plan the work at realistic schedules for the engineering teams.  </p> <p>Engineers gain an appreciation of the longer term work and consider designs that are more likely to optomise the amout of work required to deliver successive projects over time.</p>"},{"location":"practices/discussions/rolling-wave-planning/#iterative-plan","title":"Iterative plan","text":"<p>For each iteration the plan should identify enough details to progress the project or feature in the immedate term.  Those details  should be communicate effectively to all people involved to provide a clear picture of the work and an indication of who is responsible for each work item.</p> <p>Planning should be kept as short as possible, but no shorter.</p>"},{"location":"practices/discussions/rolling-wave-planning/#establish-scope","title":"Establish scope","text":"<p>Agree with stakeholders the scope of the work and identify the value it will bring to the buisiness.</p> Undefined value should be de-prioritised <p>Where a stakeholder cannot communicate why a piece of work is of value to the business, then the work should be de-prioritised.  This is essential where there are multiple stakeholders or one stakeholder with many requests.</p> <p>Clarify with the stakeholder that value should be provided in an understandable context for the engineering team</p> <p>Where there is a single or dominant stakeholder that cannot communicte value, they must take sole and full responsibility of delivery (or lack of).</p>"},{"location":"practices/discussions/rolling-wave-planning/#analysis","title":"Analysis","text":"<p>Engeering management and teams identify risks and constraints within the scope of the work and plan to reduce potential impact.</p> <p>Discuss the requirements and identify the most critical work and percieved gaps in knowledge or understanding.</p> <p>Create an initial break down work into managable pieces of work (e.g. 1 day to 1 week time frame),starting with the most critical work or areas of greatest uncertanty (risk).</p> <p>Highlight deadlines and interconnected work and devise a specific assignment of work where neccessary.</p> <p>When forming a new team, identify roles and responsibilities for individuals in the team and as well as the team overall.</p>"},{"location":"practices/discussions/rolling-wave-planning/#develop","title":"Develop","text":"<p>Quickly review the plan for the current iteration (wave).</p> <p>Feedback on assumptions as more is learned about the work whilst doing the work.  </p> <p>As the team commences work, the manager starts to plan the next wave and updates the relevant stakeholders.</p> <p>Teams work on their tasks and the manager monitors and tracks progress and performance.</p> <p>The manager will also work towards reducing uncertainties in the later stages of the project.</p>"},{"location":"practices/discussions/rolling-wave-planning/#repeat","title":"Repeat","text":"<p>Continue this process of iteration until you\u2019ve reached the end of the project.</p> <p>When the project or feature has been delivered, conduct a post mortem retrospective to discuss</p> <ul> <li>what worked well</li> <li>what did not work so well</li> <li>what to do more of</li> <li>what to do less of</li> </ul> <p>Run a retrospective</p>"},{"location":"practices/discussions/rolling-wave-planning/#managing-uncertanty","title":"Managing Uncertanty","text":"<p>Rolling wave plan is an effective way to manage high levels of uncertainty and risk.</p> <p>Taking an incremental approach provides regular analysis and feedback into the project plans, allowing timely changes and supporting considerable piviots in direction.</p> <p>Progress of the plan should be monitored and challenges identified quickly so that enough time is available to make considered changes to the plan, with the conseus and understanding from the wider team.</p>"},{"location":"practices/discussions/standup/","title":"Standup meetings","text":"<p>Aim</p> <ul> <li>a brief meeting, preferably with a hard time limit (e.g. max 10 minutes)</li> <li>share only essential and important information</li> </ul> <p>Anti-pattern: using standups for status updates</p> <p>There are many far more effective approaches to track the status of a team.</p> <p>It is a waste of everyones time at a standup to talk about what was done and what will be done next.</p> <p>Topics</p>"},{"location":"practices/discussions/standup/#-identify-experiences-that-are-useful-to-share","title":"- identify experiences that are useful to share","text":""},{"location":"practices/discussions/standup/#tips","title":"Tips","text":"<ul> <li>stand up at the standup, encourages brevity</li> <li>its okay not to have anything to share on any given day (never sharing should be addressed though)</li> </ul>"},{"location":"practices/discussions/thinking-hats/","title":"Six Thinking Hats","text":"<p>An effective group discussion technique where everyone thinks with certain focus (hat) at the same time, avoiding conflicting disussion and inefficient arguments.</p> <p>The techniquie is most effective when tackling a specific challenge and achieving consensus on an appropriate solution.</p> HAT OVERVIEW TECHNIQUE BLUE \"The Big Picture\" &amp; Managing CAF (Consider All Factors); FIP (First Important Priorities) WHITE \"Facts &amp; Information\" Information RED \"Feelings &amp; Emotions\" Emotions and Ego BLACK \"Negative\" PMI (Plus, Minus, Interesting); Evaluation, dot voting YELLOW \"Positive\" PMI GREEN \"New Ideas\" Concept Challenge; Yes, No, Po <p>Unstructured discussions</p> <p>Discusisons without structure can become confrontational or supress ideas without due consideration.</p>"},{"location":"practices/discussions/thinking-hats/#run-a-discussion","title":"Run a discussion","text":"<p>A facilitator runs the discussion and explains</p> <ul> <li>the concept of six thinking hats and the purpose of each hat</li> <li>agree the default time to be spend thinking with each hat</li> <li>the topic to be discussed (can be done as the white hat)</li> </ul> <p>The facilitator manages an appropriate time to spend discussing with a specific hat, consulting with the group to ensure there is consensus.</p> <p>The facilitator ensures disussions stay within the focus of the currently selected hat.</p>"},{"location":"practices/discussions/thinking-hats/#hat-sequences","title":"Hat sequences","text":"<p>Common activities are supported by specific hat sequences to support an effective thinking process.</p> ACTIVITY HAT SEQUENCE Initial Ideas Blue, White, Green, Blue Choosing between alternatives Blue, White, (Green), Yellow, Black, Red, Blue Identifying Solutions Blue, White, Black, Green, Blue Quick Feedback Blue, Black, Green, Blue Strategic Planning Blue, Yellow, Black, White, Blue, Green, Blue Process Improvement Blue, White, White (Other People's Views), Yellow, Black, Green, Red, Blue Solving Problems Blue, White, Green, Red, Yellow, Black, Green, Blue Performance Review Blue, Red, White, Yellow, Black, Green, Blue"},{"location":"practices/discussions/thinking-hats/#tips","title":"Tips","text":"<p>Display the summary of the topic being discussed throughout the meeting.</p> <p>Display a summary of a hats purpose whilst that hat is active.</p> <p>Capture disussions as they happen on a whiteboard or shared document, ensuring information is catagorised under the appropriate hat.</p> <p> Six Thinking Hats - De Bono Group</p>"},{"location":"practices/kanban/","title":"Kanban","text":"<p>A lean approach to organising tasks and continually improving ways of working</p> <p>Kanban encourages teams to</p> <ul> <li>visualise the work of the team (kanban board)</li> <li>review and optomise they way the work</li> <li>limit work in progress &amp; task switching (stop starting and start finishing)</li> <li>flow</li> <li>define work in consistent sizes (a day to a week)</li> <li>think of work as a value driven system</li> <li>pull system from the end of the workflow to the start</li> </ul> <p>Theory of Constraints</p> <p>The theory of constraints is a management paradigm that views any manageable system as being limited in achieving more of its goals by a very small number of constraints</p> <p>Theory Of Constraints website</p> <p>Wikipedia: Theory Of Constraints</p> <p>Wikipedia: The Goal(novel) - Goldratt</p>"},{"location":"practices/kanban/#visualising-work","title":"Visualising work","text":"<p>A kanban board visualises the flow of tickets through a workflow, highlighting any number of factors</p> <ul> <li>status of work (todo, doing, done or more involved steps)</li> <li>who is working on the ticket</li> <li>history of the ticket (moving backward and forward between states shows churn)</li> <li>limits for work in progress (limited WIP), limits on particular workflow steps (e.g. doing) or limits on type of tickets</li> </ul>"},{"location":"practices/kanban/value-stream-mapping/","title":"Value Stream Mapping","text":"<ul> <li> <p>identify route cause of waste, to reduce or eliminating</p> </li> <li> <p>improve behavior, culture, communication, and collaboration</p> </li> </ul> <p>Teams discard individual opinions and prioritize towards a customer delivery perspective.</p> <p>Cross team software development processes or where teams depend on each other</p>"},{"location":"practices/kanban/value-stream-mapping/#identify-waste","title":"Identify waste","text":"<p>waste can appear between</p> <ul> <li>the business / product owners and the engineering team</li> <li>operations delay developers (infrastructure limitations or issues can delay deployment)</li> <li>development teams delay operations by not providing details on how to support deployed systems</li> <li>support teams unable to support a system due to limited or no documentation</li> <li>customer facing teams unable to support customers effectively as documentation or updates were not communicated effectively</li> </ul> <p>Value stream mapping can be used to improve any process where there are repeatable steps \u2013 and especially when there are multiple handoffs.</p> <p>Waste in knowledge work occurs very often in handoffs or wait time between team members, not within the steps themselves.</p> <p>&lt;!-- Inefficient handoffs lead to low productivity and poor quality. Value stream mapping helps identify waste and streamline the production process. Value stream mapping can be applied to both the product and customer delivery flows. Product flow focuses on steps required to optimize product delivery and completion. The customer flow focuses on the steps required to deliver on end user requests and expectations. -&gt;</p> <p>Value stream mapping (VSM) is a lean manufacturing technique to optomise the flow of materials and information required to bring a product to a customer.</p> <p>Valu stream mapping can be also be used to optomise software development and continuous delivery workflows.</p>"},{"location":"practices/kanban/value-stream-mapping/#effective-approach","title":"Effective approach","text":"<p>Balance the cost of conducting value stream mapping with the potential for value (reducing waste)</p> <p>Appreciate the value particular activities and ensure that percieved waste is actual waste.</p> <ul> <li> <p>involve people experienced with the process, espcially where the mapping process is cross-functional and complex</p> </li> <li> <p>avoid over-identifying waste (waste obsession)</p> </li> <li> <p>small savings may not directly translate to significant process improvements or be immediately obvious (ensure to record potential savings no matter how small)</p> </li> <li> <p>start with simple tools to focus on the activity</p> </li> </ul>"},{"location":"practices/kanban/value-stream-mapping/#symptoms","title":"Symptoms","text":""},{"location":"practices/kanban/value-stream-mapping/#over-production","title":"Over-production","text":"<ul> <li>deliverying features not required</li> <li>delivering features an the incorrect time,to late or too early</li> <li>delivering features too early may have prevented other features being delivered on time</li> </ul> <p>Inventory - maintaining features that are of little or no value</p> <p>Defects - high level of bugs or low quality software   - rushed delivery, limited understanding, lack of support</p> <p>Over-processing - an overburdend test suite that only delivers partial value   - code coverage is only a number and should only be a simple indicator of the value of tests   - unit tests should include the public API of the system and only essential supporting code where testing provides significant value</p>"},{"location":"practices/kanban/value-stream-mapping/#waiting","title":"Waiting","text":"<p>Transport - waste when delivering products to the customer,</p> <p>Partially completed work - software delivered in an incomplete state   - lack of complete specification or automated test coverage   - can cause a cascade of waste fro additional work required to push more updates and provide missing features</p> <p>Delays - breaking changes to dependant systems causing delays   - overly coupled system or system integration increases potential issues   - non-breaking (additive) changes avoid delays and waste - workflow delays   - unit testing that takes a long time, limiting feedback and making it less likely that unit tests are run</p>"},{"location":"practices/kanban/value-stream-mapping/#task-switching","title":"Task switching","text":"<p>Creative thought context switching is expensive. There is a cadence or \u201cflow\u201d that software engineers achieve to optimally produce good code.</p> <p>Efficient organizations work to optimize the creative state for their engineers. Inefficient organizations bombard their engineers with non-critical distractions like meetings and emails that disrupt their workflow.</p> <p>Mute Slack channels that are not high priority to avoid being distracted by chat, reviewing interesting channels at a time that does not distract</p> <p>Limit @ mentions and encourage most of the team to switch off from slack when focusing deeply on work</p> <ul> <li>switching prioritories and focus too often</li> <li>creative thought is very Inefficient when regularly task switching<ul> <li>context is disrupted</li> <li>context takes time to be reestablished, a few minutes interuption can waste 30-60 minutes</li> </ul> </li> <li>meetings disrupt productivity (organise meeting free days, or one day a week for meetings, e.g. Monday used for planning and meetings, leaving the rest of the week free for productive work)</li> <li>limiting opportunities for meetings encourages more effecive communication via meetings and limits waste within meetings - as does a stand-up meeting as people dont want to stand for too long, forcing the meeting to be quicker)</li> </ul> <p>Task switching (within a person) waste has similar qualities as handoff waste (beteween people)</p> <p>Defects Defect waste happens when bugs are pushed in software. Defects are similar to partially completed work but can be more wasteful because defects are unknown and partially completed work is usually known ahead of time. Defects may be identified by customers and then reported to customer support, which can be an expensive pipeline that causes delays and task switching.</p> <ul> <li>Wikipedia: Value-stream mapping</li> <li>Atlassian: Value Stream mapping - optomise the continuous-delivery pipeline</li> </ul>"},{"location":"practices/kanban/value-stream-mapping/#waste","title":"Waste","text":"<p>Examples of percieved waste that is not wasteful</p> <ul> <li>pair / mob programming is not a waste of resources when valuable learning and experiences are shared</li> <li>building parallel solutions is not wasteful when understanding is gained as to the most relevant architecture and designs to persue</li> </ul>"},{"location":"programming-languages/","title":"Programming Languages","text":""},{"location":"programming-languages/javascript/","title":"JavaScript","text":""},{"location":"programming-languages/javascript/nodejs/","title":"Node.js","text":"<p>Node.js is an environment to run JavaScript on the operating system, without the need for a browser.</p> <p>Node.js is useful for running JavaScript tools and server-side services.</p>"},{"location":"programming-languages/javascript/nodejs/#install","title":"Install","text":"Debian/UbuntuMacOSX Homebrew <p>The nodejs package available in Ubuntu required a scary amount of dependencies, so it may be preferable to install nodejs locally, in your own account.  This does mean node is only available to this specific user account.</p> Avoid nodejs debian packages when using Neovim Mason <p>Mason is a Neovim plugin to manage installation of LSP servers, format and lint tools.</p> <p>The debian packages potentially cause issues with Mason updates for specific format and lint tools.</p> <p>Install Node.js via the Linux archive available from the Nodejs Website - Download </p> <p>Nodejs Website - Download </p> <p>Download the relevant version of nodejs from the website, either the Long Term Support version (recommended) or the latest release</p> <p>Create a suitable directory path to extract the downloaded nodejs archive to</p> <pre><code>$HOME/.local/apps/nodejs/\n</code></pre> <p>Extract the downloaded nodejs archive into this path</p> <pre><code>tar -Jvxf node-v20.7.0-linux-x64.tar.xz -C $HOME/.local/apps/nodejs/\n</code></pre> <p>The <code>tar</code> flag <code>-J</code> uses the XZ compression algorithm, <code>v</code> for verbose list of extracted files, <code>x</code> to extract all files, <code>f</code> to specify file (or remote file, tape drive)</p> <p>Create a symbolic link called current that points to the top level directory of the nodejs archive just unpacked, e.g. node-v <pre><code>ln -s node-v20.7.0-linux-x64 current\n</code></pre> <p>Add nodejs directory to the execution path, so that any additional node executables will automatically be included</p> <p>Edit the <code>~/.zshenv</code> file, adding a conditional path entry on the nodejs directory existing</p> <p><pre><code>## Nodejs local install\nif [[ -d $HOME/.local/apps/nodejs/current ]]; then\n  path=($HOME/.local/apps/nodejs/current/bin(/N) $path)\nfi\n</code></pre> This example adds the nodejs bin directory at the start of the current executable path.</p> <p>Use <code>path+=($HOME/.local/apps/nodejs/current/bin(/N))</code> instead to append the nodejs bin directory to the end of the PATH.</p> <p>Open a new terminal to pick up the change or source the <code>.zshenv</code> file in an existing terminal</p> <pre><code>source ~/.zshenv\n</code></pre> <p>Install using brew.sh and the Homebrew Formulae for node</p> <pre><code>brew install node\n</code></pre>"},{"location":"software-design/","title":"Software Design","text":"<p>Initial draft - feedback welcome</p>"},{"location":"software-design/open-standards/","title":"Open Standards","text":"<ul> <li>TCP/IP</li> <li>HTML / CSS</li> </ul>"},{"location":"software-design/anti-patterns/","title":"Anti-Patterns","text":"<p>Anti-patterns arise as people translate business needs into software solutions, especially where the human process requires a clear and well communicated shared vision.</p> <p>Anti-patterns take a similar documentation style to the definition of Design Patterns, although anti-patterns can also cover behaviour so cover a far wider scope than the (mostly OO) software design covered by Design Patterns.</p> <p>Anti-patterns are often inadvertently applied, so awareness allow those situations be avoided before they occur.</p> <p>Categories of anti-patterns</p> <ul> <li>Software Development</li> <li>Software Architecture</li> <li>Software Project Management</li> </ul>"},{"location":"software-design/architecture/","title":"Architecture tools and techniques","text":"<p>Tools and techniques to support design and communication of archiectural decisions and techniques to elicit archiectural designs</p> <ul> <li>Domain Driven Design</li> <li>Architecture Patterns</li> <li> Architectural Katas</li> <li>Composition (Aggregation)</li> <li> Command Query Separation</li> <li> Command Query Resposibility Segregation (CQRS)</li> </ul> <p>Architectural Kata - Deliberate Practice</p> <p> Architecture Kata is a group activity for deliberate practice, providing a rich environment to stretch architecture design skills.</p>"},{"location":"software-design/architecture/#organising","title":"Organising","text":"<ul> <li> Architectural Decision Records</li> </ul>"},{"location":"software-design/architecture/#visual-tools","title":"Visual tools","text":"<ul> <li> ExcaliDraw - online tool to sketch designs and export to SVG or PNG format</li> <li> Miro - comprehensive online diagram tool with numerous templates</li> <li> Inkscape - open desktop diagram tool using SVG</li> </ul>"},{"location":"software-design/architecture/#diagram-as-code","title":"Diagram as Code","text":"<ul> <li> Structurizr architeture tool using the C4 model approach</li> </ul>"},{"location":"software-design/architecture/adr/","title":"Architecture Decision Record","text":"<p>Architecture Decision Record (ADR) is a formal way to capture a significant decision which impacts the current or future system.</p> <p>Everyone can learn from the history of desisions taken during the evolution of the project without adding burden to the current project team.</p> <p>Once decisions are made and delivered the ADR document should be treated as immutable, so it becomes a perminant record.  Should a decision need to be changed or significantly altered, then a new ADR document should be created that references the orginal ADR.</p>"},{"location":"software-design/architecture/adr/#scope","title":"Scope","text":"<p>Create an ADR for architecturally significant decision that affects the software project or product</p> <ul> <li>Structure (patterns such as microservices)</li> <li>Non-functional requirements (security, high availability, and fault tolerance)</li> <li>Dependencies (coupling of components)</li> <li>Interfaces (APIs and published contracts)</li> <li>Construction techniques (libraries, frameworks, tools, and processes)</li> </ul> <p>Functional and non-functional requirements are the most common inputs to the ADR process.</p>"},{"location":"software-design/architecture/adr/#scenarios","title":"Scenarios","text":"<p>ADRs work well in the following scenarios:</p> <p>Onboarding</p> <p>New team members are able to find answers to why decisions were made in the project, allowing focus on new challenges whilst appreciating the existing approach.</p> <p>Review ADRs when</p> <ul> <li>specific questions arise from learning a codebase</li> <li>when making related architectural decisions</li> <li>better understand how the team think and their priorities</li> </ul> <p>Ownership handover:</p> <p>During a transfer of project the new maintenance team sees the resulting code, but that does not show how and why the project got to its current state.</p> <p>It is hard to know what are the critical paths of the project, the parts that are very robust and parts that should be treated delicately.</p> <p>The new maintainers can</p> <ul> <li>review past decisions to understand the current state.</li> <li>avoid repeating the same discussion points</li> <li>revisit topics in the future with knowledge of the historical context.</li> <li>review decisions quickly and effectively to understand priorities at the time they were made</li> </ul> <p>Alignment:</p> <p>Teams can align on best practices across the organization when ADRs detail why certain decisions were made and alternatives were decided against.</p> <p>ADR's could also be used to capture decisions that form an aspect of the overall engineering strategy, e.g. choice of programming language</p> <p>Simple ADR template</p> <pre><code># Title\n\n## Contributors\n- who owns the responsibility for the decision\n- contact details for all those involved (if the decision has follow-on or questions arise)\n\n## Status\n\nDefine the status of the decision along with the date of change into that state\n\n- proposed &lt;date&gt; \n- accepted &lt;date&gt; \n- rejected &lt;date&gt;  \n- deprecated  &lt;date&gt; \n- superseded &lt;date&gt;\n\n## Context\n\n- Primary Issue motivating the decision or change?\n- Business context &amp; value\n- Timeframe for decision (usually tied to value)\n\n## Decision\n\nDetail of the decision and any change to be made, including how that change is to be realised (i.e. 3rdc party product, bespoke development, migration, testing, deployment, etc.)\n\nReference additional documentation already created or too be created as part of the change.\n\n## Consequences\n\nDefine how the decision impacts the project (and/or organisation), highlighing the benefits and constraints \n\n- aleviated constraints\n- added constraints\n- intentional technical debt\n- business risks\n\n## Alternatives\n\nSummary of the alternatives considered, incluing references to related documentation\n</code></pre>"},{"location":"software-design/architecture/adr/#backfill-decisions","title":"Backfill decisions","text":"<p>Decisions made before ADRs became part of the engineering workflow can be retrospectively created, although they may only be a partial record and contain historical inaccuracies regarding the original decsion.</p> <p>Identify undocumented decisions during a retrospective or an architectural peer review, deciding which decisions are the most valuable to document.</p>"},{"location":"software-design/architecture/adr/#references","title":"References","text":"<p> ADR management tools</p> <p> Architectural Decision Records - Amazon</p> <p> Example ADR - Amazon</p> <p> Architecture Decision Record Examples</p>"},{"location":"software-design/architecture/structurizr/","title":"Structurizr - Architecture diagrams as code","text":"<p>Structurizr is a tool for expressing and visualising architecture using the C4 model</p> <p>Define a single model divided into softwareSystems within which services and persistent data stores are defined. Relationships are defined between services and persistence stores.</p> <p>Many views can be generated from the single model and changes to the model automatically update those views to ensure all views are always up to date.</p> <p></p> <p>Colours of exported SVG image enhanced using Inkscape.org and exported as PNG file</p> <p>Mock Fintech Starup - Practicalli Services practicalli/structurizr Git repository</p>"},{"location":"software-design/architecture/structurizr/#c4-model-summary","title":"C4 Model summary","text":"<ul> <li>Level 1: Software system - a system context composed of one or more containers</li> <li>Level 2: Container - application or data store, each component is separately deployable/runable, composed of one or more components</li> <li>Level 3: Component - grouping of functionality with a well defined interface, implemented by one or more code artefacts</li> <li>Level 4: Code - code artefact (function, object/class)</li> </ul> <p>Quick try with Structurizr DSL Editor</p> <p>Structurizr DSL online editor provides an instant way to try structurizr without install or sign-up</p>"},{"location":"software-design/architecture/structurizr/#install","title":"Install","text":"<p>Use the free Structurizr Lite locally (via docker) or use Structurizr Cloud service (free for Open Source &amp; Academic projects on request)</p> <p>Project models can be imported into the cloud-based tool from Structurizr Lite when team or company wide collaboration is required.</p> Structurizr liteStructurizr Cloud <p>Structurizr Lite - Getting Started</p> <p>Install Structurizr locally with the free structurizr Lite product via the Docker image. Ensure Docker or Docker Desktop is running  (image approximately 450Mb in size)</p> Docker ComposeDocker <p>Include Structurizr Lite in a <code>docker-compose.yaml</code> file in the root of the project.  Docker compose is especially useful when running one or more services to automatically update views from the model.</p> <p>A volume is use to persist the model <code>workspace.dsl</code> file, enabling local editing of the model while the docker container is running. docker-compose.yaml<pre><code>---\nversion: \"3.9\"\n\nservices:\n  # --- System Model --- #\n  structurizr:\n    container_name: system-architecture\n    image: \"structurizr/lite:latest\"\n    ports:\n      - \"8080:8080\"\n    volumes:\n      - \"./model:/usr/local/structurizr:rw\"\n</code></pre></p> <p>Run structurizr lite using the command: <pre><code>docker compose up\n</code></pre></p> <p>Structurizr can also be defined in its own docker compose file and called separately, e.g. <code>strucurizr.yaml</code> configuration file <pre><code>docker compose -f structurizr.yaml up\n</code></pre></p> <p>Pull the docker image</p> <pre><code>docker pull structurizr/lite\n</code></pre> <p>Set the Structurizr project path to define the location of the <code>.dsl</code> or <code>.json</code> workspace definitions (or enter the path directly in the docker command)</p> <p>Practicalli uses the <code>model</code> directory to keep the configuration files</p> <pre><code>export STRUCTURIZR_DATA_PATH=/home/practicalli/projects/practicalli/structurizr/model\n</code></pre> <p>Run docker with the Structurizr data path <pre><code>docker run -it --rm -p 8080:8080 -v $STRUCTURIZR_DATA_PATH:/usr/local/structurizr structurizr/lite\n</code></pre></p> <p> </p> <p><code>workspace.dsl</code> and <code>workspace.json</code> files are created if they do not already exist.</p> <p>Structurizr Cloud</p> <p>Sign up for a free account which provides 1 workspace on the cloud service.</p> Free Cloud product for Open Source &amp; Academic use <p>Access to 5 workspaces on the Structurizr paid cloud service for each open source project or academic     establishment on request</p> <p>Select New workspace after login to Structurizr</p> <p>A summary page of the new workspace is shown, showing the last modified time and an option to load a previous version.</p> <p></p> <p>Scroll the page vertically to see the available diagrams, or click More Diagrams... at the bottom of the page</p>"},{"location":"software-design/architecture/structurizr/#structurizr-dsl-editor","title":"Structurizr DSL editor","text":"<p>The model is defined by a domain specific language (DSL) and the DSL editor should be used to add and update all configuration.</p> <p>Select DSL editor from the left hand navigation bar</p> <p>Structurizr DSL Language Reference documentation</p> <p>Select the Source text icon to see only the code window, allowing for easier editing</p>"},{"location":"software-design/architecture/structurizr/#define-the-system","title":"Define the system","text":"<p>Structurizr DSL Language Reference documentation Structurizr DSL Cookbook Structurizr DSL online editor</p> <p>The Structurizr DSL online editor is a useful tool to help learn the syntax of the Structurizr DSL, using the Render button to see the results of the model as it is defined.</p> <p>Basic structure:</p> <ul> <li><code>workspace</code> is composed of the system model and views derived from that model</li> <li><code>model</code> is composed of one or more <code>softwaresystems</code></li> <li><code>views</code> define one or more views of the model, using autoLayout to organise diagrams or manually specifying layouts, sytlyes, themes, etc.</li> <li><code>softwaresystem</code> defines specific services along container boundaries (application * data store components)</li> </ul> <p>Add an entry for each service within the specific <code>softwareSystem</code> using the form</p> <pre><code>unique_name_id = container \"Service or data store name\" \"Description of service or database\" \"Container name\" \"View type - Tag name\"\n</code></pre> Tags and themes <p>Tags are used to define the appearance in a view.  Structurizr default theme contains simple tags.  Amazon AWS theme contains a wide range of icons and styles to represent its many services, although these are more suitable for deploymentEnvironment views.  Adding themes section covers this in more detail.</p> <p>For a persistent store, e.g. relational database, use the form</p> <pre><code>fraud_data = container \"Fraud History\" \"A complete history of transactions and reports\" \"fraud-data\" \"database\"\n</code></pre> <p>For an elastic search service, use the form</p> <pre><code>unique_name_id = container \"Display name\" \"Description of service or database\" \"Elastic Search\" \"Elastic\"\n</code></pre> <p>simple example</p> <ul> <li>Create a model with a user and a software system, where the user uses the software system.</li> <li>Create a system context view for the software system, adding the default set of elements, using auto-layout.</li> <li>Use the default theme for styling elements and relationships.</li> </ul> <pre><code>workspace {\n    model {\n        user = person \"User\"\n        softwareSystem = softwareSystem \"Software System\"\n\n        user -&gt; softwareSystem \"Uses\"\n    }\n    views {\n        systemContext softwareSystem \"Diagram1\" {\n            include *\n            autoLayout\n        }\n        theme default\n    }\n}\n</code></pre>"},{"location":"software-design/architecture/structurizr/#common-values","title":"Common values","text":"<pre><code>!constant ORGANISATION_NAME \"Practicalli\"\n!constant GROUP_NAME \"Fintech\"\n\nworkspace {\n    model {\n        enterprise \"${ORGANISATION_NAME} - ${GROUP_NAME}\" {\n            user = person \"User\"\n        }\n    }\n}\n</code></pre>"},{"location":"software-design/architecture/structurizr/#grouping-services","title":"Grouping services","text":"<p>Grouping services within a softwareSystem keeps closely related service together and are rendered in the group within a view. A group can also be included or excluded from a view</p> <p>The Banking software system contains groups: shared, credit, transaction and fraud. Each group containing a number of services</p> <pre><code>risk = softwareSystem \"Risk\" {\n  shared_services_risk = group \"Shared Services Risk\" {\n    company_info = container \"Company WhoIs\" \"Company search service\" \"Clojure API\"\n    company_info_database = container \"Company WhoIs database\" \"\" \"Relational database schema\" \"Database\"\n    company_info_search = container \"Company Search Aggregator\" \"Company full-text search index\" \"Elastic Search\" \"Elastic\"\n    risk_data_providers = container \"Risk Data Providers\" \"Data Provider Service\" \"PHP Symphony service\"\n    risk_data_providers_database = container \"Risk Data\" \"\" \"Relational database schema\" \"Database\"\n  }\n  credit_risk = group \"Credit Risk\" {\n    score = container \"Credit risk scoring\" \"Scoring organizations Credit Risk\" \"Clojure Service\"\n    score_data = container \"Credit Risk Scoring Service database\" \"\" \"Relational database schema\" \"DatabaseWip\"\n    assessment = container \"Credit Assessment\" \"Credit risk assessment Service\" \"Clojure\"\n  }\n  transaction = group \"Transaction\" {\n    guardian = container \"Transaction Guardian\" \"Transaction monitoring and transaction Screening service\" \"Clojure\"\n    guardian_database = container \"Transaction Guardian Database\" \"\" \"Relational database schema\" \"Database\"\n    limiter = container \"Limiter\" \"Limits Service\" \"ClojureKafka\"\n  }\n  fraud_risk = group \"Fraud Risk\" {\n    detection = container \"Fraud Service\" \"Detect fraudulent transactions via Fraud Scoring Data Science models \" \"Clojure API\"\n    detection_data = container \"Fraud Database\" \"TODO: Define the kind of data persisted\" \"Relational database schema\" \"Database\"\n    ml_model = container \"Machine learning model service\" \"Sagemaker\"\n    feature_store_data = container \"Feature store\" \"Pre-calculated feature values\" \"Key value database\" \"Database\"\n    manual_review = container \"Review Transactions\" \"Manually review transactions for fraud\" \"\" \"WebBrowser\"\n  }\n}\n</code></pre>"},{"location":"software-design/architecture/structurizr/#define-relationships","title":"Define relationships","text":"<p><code>-&gt;</code> defines a relationships between two id's defined in the <code>softwareSystem</code> part of the model, along with a description of the relationship that is added to the arrow joining the artefacts in a view.</p> <pre><code>service_name -&gt; service_or_datastore_name \"Description of connection\"\n</code></pre> <p>The relationships are used to draw connections between services and the descriptions name those connections</p> <pre><code>    user -&gt; transaction \"Triggers\"\n\n    transaction -&gt; risk \"Uses\"\n    guardian -&gt; guardian_data \"Persists\"\n    guardian -&gt; limiter \"Uses\"\n\n    detection -&gt; detection_data \"Reads and writes to\"\n    detection -&gt; ml_model \"score transaction\"\n    ml_model -&gt; feature_store_data \"Collect features\"\n    ml_model -&gt; feature_schema_data \"Request feature set &amp; model\"\n</code></pre>"},{"location":"software-design/architecture/structurizr/#defining-views","title":"Defining Views","text":"<p>A workspace contains views which visualise artefacts defined in any <code>softwareSystem</code> within the model.</p> <p>A view can include everything * within the softwareSystem or use <code>include</code> and <code>exclude</code> to refine the view based on groups or specific containers.</p> <p>View of the form: view-type softwareSystem-name view-name</p> <p><code>container risk fraud-detection</code></p> <pre><code>container risk \"RiskContainersAfterSE\" {\ninclude *\nautoLayout\n}\ncontainer risk \"CreditRiskServices\" {\ninclude *\nexclude fraud_risk\nautoLayout\n}\ncontainer risk \"FraudServices\" {\ninclude *\nexclude credit_risk\nautoLayout\n</code></pre>"},{"location":"software-design/architecture/structurizr/#deployment-infrastructure","title":"Deployment Infrastructure","text":"<p>An example of production deployment environment for the Practicall Mock Fintech Startup</p> <pre><code>  production = deploymentEnvironment \"Production\" {\n    aws = deploymentNode \"Amazon Web Services\" \"\" \"\" \"Amazon Web Services - Cloud\" {\n      region = deploymentNode \"US-East-1\" \"\" \"\" \"Amazon Web Services - Region\" {\n        route53 = infrastructureNode \"Route 53\" \"\" \"\" \"Amazon Web Services - Route 53\"\n        elb = infrastructureNode \"Elastic Load Balancer\" \"\" \"\" \"Amazon Web Services - Elastic Load Balancing\"\n        autoscalingGroup = deploymentNode \"Autoscaling group\" \"\" \"\" \"Amazon Web Services - Auto Scaling\" {\n          ec2 = deploymentNode \"Amazon EC2\" \"\" \"\" \"Amazon Web Services - EC2\" {\n\n            webApplicationInstance = containerInstance detection\n            elb -&gt; webApplicationInstance \"Forwards requests to\" \"HTTPS\"\n          }\n        }\n        rds = deploymentNode \"Amazon RDS\" \"\" \"\" \"Amazon Web Services - RDS\" {\n          mysql = deploymentNode \"MySQL\" \"\" \"\" \"Amazon Web Services - RDS MySQL instance\" {\n            databaseInstance = containerInstance detection_data\n          }\n        }\n        route53 -&gt; elb \"Forward requests to\" \"HTTPS\"\n      }\n    }\n  }\n</code></pre>"},{"location":"software-design/architecture/structurizr/#embedding-documentaion-in-views","title":"Embedding Documentaion in views","text":"<p>Create a directory called <code>docs</code> to contain markdown files with system descriptions.</p> <p>Include <code>![](embed:DiagramName)</code> in the markdown file to include the text in the view called <code>DiagramName</code></p> <pre><code>## Context\n\nHere is a description of my software system...\n\n![](embed:Diagram1)\n</code></pre> <p>The view should be defined in the workspaces.dsl</p> <pre><code>views {\n        systemContext softwareSystem \"DiagramName\" {\n            include *\n            autoLayout\n        }\n</code></pre> Example views from Mock Fintech Startup <p>Views defined in the Practicalli Enterpirse for the Mock Fintech Startup architecture <pre><code>  views {\n   /*Overall system */\n    systemContext risk \"EnterpriseView\" \"Practicalli Enterprise Application\" {\n      include *\n      autoLayout\n    }\n   /* Entire Risk system */\n    container risk riskView \"Complete Risk system\" {\n      include *\n      autoLayout\n    }\n    /* View of shared_services group in risk system */\n    container risk sharedServicesView \"Services shared across the organisation\" {\n      include shared_services\n      autoLayout\n    }\n    /* View of fraud_risk group in risk system */\n    container risk fraudRiskView \"Fraud Risk Services Only\" {\n      include fraud\n      autoLayout\n    }\n    /* View of credit_risk group in risk system */\n    container risk creditRiskView \"Credit Risk Services only\" {\n      include credit\n      autoLayout\n    }\n    /* View of fraud &amp; shared_services group without credit*/\n    container risk fraudSharedView \"Fraud and shared services\" {\n      include *\n      exclude credit\n      autoLayout\n    }\n    container transaction transactionView \"Current Transaction system\" {\n      include*\n      autoLayout\n    }\n\n    deployment risk \"Production\" \"AmazonWebServicesDeployment\" {\n      include *\n      autolayout lr\n      animation {\n        route53\n        elb\n        autoscalingGroup\n        webApplicationInstance\n        databaseInstance\n      }\n    }\n\n    /* Theme for views */\n    themes default https://static.structurizr.com/themes/amazon-web-services-2022.04.30/theme.json https://raw.githubusercontent.com/practicalli/structurizr/main/themes/practicalli/theme.json\n\n    branding {\n      logo https://raw.githubusercontent.com/practicalli/graphic-design/live/logos/practicalli-logo.png\n    }\n  }\n</code></pre></p>"},{"location":"software-design/architecture/structurizr/#adding-themes","title":"Adding themes","text":"<p>Themes add styles or icons to the diagrams rendered by Structurizr tools, e.g. Amazon AWS icons.</p> <p>Add a theme to the workspace.dsl configuration and add a theme tag to a component definition.</p> <p><code>theme</code> key is added as a value within the <code>views</code> key of the <code>workspace</code>.  <code>themes</code> is used to include multiple themes.</p> <p>Structurizr language reference - theme</p> <p>Example: using the Amazon icons</p> <pre><code>    views {\n        systemContext softwareSystem \"Diagram1\" {\n            include *\n            autoLayout\n        }\n        theme https://static.structurizr.com/themes/amazon-web-services-2022.04.30/theme.json\n    }\n</code></pre> <p>Use icons from the theme by adding one of the theme tags</p> <pre><code>identifier = type \"\" \"\" \"\" \"tag-name\"\n</code></pre> <p>Exmaple: the <code>\"Amazon Web Services - SageMaker\"</code> tag is added to a machine learning model</p> <pre><code>ml_model = container \"AWS Sagemaker\" \"Machine learning model service\" \"\" \"Amazon Web Services - SageMaker\"\n</code></pre> Useful Amazon Web Services Tags <p>Amazon Web Services theme - tags commonly used:</p> <ul> <li>Amazon Web Services - Lambda</li> <li>Amazon Web Services - Fargate</li> <li>Amazon Web Services - Route 53</li> <li>Amazon Web Services - Simple Storage Service</li> <li>Amazon Web Services - EC2</li> <li>Amazon Web Services - EC2 AMI</li> <li>Amazon Web Services - EC2 Auto Scaling</li> <li>Amazon Web Services - Elastic Load Balancing</li> <li>Amazon Web Services - Elastic Container Service</li> <li>Amazon Web Services - Elastic Container Kubernetes</li> <li>Amazon Web Services - Elastic Container Registry</li> <li>Amazon Web Services - Elastic Container Registry Image</li> <li>Amazon Web Services - EKS Cloud</li> <li>Amazon Web Services - Elastic Beanstalk</li> <li>Amazon Web Services - Category Database</li> <li>Amazon Web Services - Category Machine Learning</li> <li>Amazon Web Services - Category Serverless</li> <li>Amazon Web Services - Corretto</li> <li>Amazon Web Services - Data Pipeline</li> <li>Amazon Web Services - Deep Learning Containers</li> <li>Amazon Web Services - RDS</li> <li>Amazon Web Services - DocumentDB</li> <li>Amazon Web Services - DynamoDB</li> <li>Amazon Web Services - Managed Service for Grafana</li> <li>Amazon Web Services - Managed Service for Prometheus</li> <li>Amazon Web Services - Managed Streaming for Apache Kafka</li> <li>Amazon Web Services - Managed Workflows for Apache Airflow</li> <li>Amazon Web Services - Secrets Manager</li> <li>Amazon Web Services - Single Sign On</li> <li>Amazon Web Services - Virtual Private Cloud</li> </ul>"},{"location":"software-design/architecture/structurizr/#custom-theme","title":"Custom Theme","text":"<p>Create a theme as a <code>.json</code> file that containes a collection of definitions within <code>\"elements\": [ ]</code></p> <p>Each element should define the <code>\"tag\"</code> name used in the view definition to identify the type of element to use.</p> <p>The look of the element is defined by <code>\"colour\"</code>, <code>\"background\"</code>, <code>\"stroke\"</code> hex color values and a <code>\"shape\"</code> type, e.g. <code>\"RoundedBox\"</code>, <code>\"Cylinder\"</code></p> <p>Include an <code>\"icon\"</code> as a further visual representation of the element, e.g. using AWS theme icons.</p> <p>Element style DSL reference Practicalli Structurizr theme</p> Practicalli Custom theme and AWS theme practicalli/structurizr theme/practicalli/theme.json<pre><code>{\n  \"name\" : \"Practicalli theme\",\n  \"elements\" : [ {\n    \"tag\" : \"Clojure Service\",\n    \"background\" : \"#8FB5FE\",\n    \"color\" : \"#EEECE6\",\n    \"stroke\" : \"#5881D8\",\n    \"shape\" : \"RoundedBox\",\n    \"icon\" : \"https://raw.githubusercontent.com/practicalli/graphic-design/live/logos/clojure-logo-64.png\"\n  } , {\n    \"tag\" : \"Database\",\n    \"background\" : \"#EEECE6\",\n    \"color\" : \"#3f51d4\",\n    \"stroke\" : \"#3f51d4\",\n    \"shape\" : \"Cylinder\",\n    \"icon\" : \"https://static.structurizr.com/themes/amazon-web-services-2022.04.30/Arch_Amazon-RDS_48.png\"\n  } , {\n    \"tag\" : \"Web Browser\",\n    \"shape\" : \"WebBrowser\"\n  } , {\n    \"tag\" : \"AWS SageMaker\",\n    \"background\" : \"#EEECE6\",\n    \"stroke\" : \"#2d8f7a\",\n    \"color\" : \"#2d8f7a\",\n    \"icon\" : \"https://static.structurizr.com/themes/amazon-web-services-2022.04.30/Arch_Amazon-SageMaker_48.png\"\n  } ]\n}\n</code></pre> <p>Practicalli Structurizr theme provides <code>Clojure Service</code>, <code>Database</code>, <code>Web Browser</code> and <code>AWS SageMaker</code> tags to customise the views generated.</p> <p>The <code>Database</code> tag uses the icon from the AWS theme, although changes colors and shape to improve readability of the diagrams. The <code>AWS SageMaker</code> tag overrides that provided by AWS theme, again improving the readability of the diagrams.</p> <p>The Mock Fintect Startup example uses a custom theme by Practicalli and the standard AWS theme.  It also includes the Practicalli Logo that appears next to the name of the view in each diagram</p> practicalli/structurizr model/workspace.dsl extract<pre><code>themes default https://static.structurizr.com/themes/amazon-web-services-2022.04.30/theme.json https://raw.githubusercontent.com/practicalli/structurizr/main/themes/practicalli/theme.json\n\nbranding {\n  logo https://raw.githubusercontent.com/practicalli/graphic-design/live/logos/practicalli-logo.png\n}\n</code></pre>"},{"location":"software-design/architecture/structurizr/#organisation-branding","title":"Organisation branding","text":"<p>Add a PNG or Jpeg graphic as a logo on all diagrams, appearing in the bottom left of each page</p> <p>Define a specific font to use for all text in the digram (font downloaded from Google Fonts)</p> <p>Define a <code>branding</code> section in the workspace &gt; views section of the <code>workspace.dsl</code> file.</p> <pre><code>branding {\n    logo &lt;file|url&gt;\n    font &lt;name&gt; [url]\n}\n</code></pre> <p>Practicalli uses the following branding in the structurizr <code>workspace.dsl</code></p> practicalli/structurizr model/workspace.dsl extract<pre><code>branding {\n  logo https://raw.githubusercontent.com/practicalli/graphic-design/live/logos/practicalli-logo.png\n}\n</code></pre>"},{"location":"software-design/architecture/structurizr/#comments","title":"Comments","text":"<p><code>/* */</code> for line comments.</p> <pre><code>/* single-line comment */\n/*\n    multi-line comment\n*/\n</code></pre> <p>Note: line comments within parens or directly after a closing paren cause syntax error</p>"},{"location":"software-design/architecture/structurizr/#practicalli-system","title":"Practicalli System","text":"<p>The Practicalli Mock Fintech Startup is defined as workspace that contains a model with an enterprise \"Practicalli\" key defining the high-level software systems</p> <ul> <li>Credit Assesment</li> <li>Risk Analysis</li> <li>Transactions (credit card, National bank deposit/transfer, BACS)</li> <li>Shared Services (account management, etc)</li> </ul> <p>A simple mock up of a Fintech managing transactions that may be susceptible to various risks, including fraud</p> <p>Practicalli Structurizr project - Mock Fintech Startup</p> <p><code>workspace.dsl</code> defined a Practicalli Enterprise with several softwareSystem defintions, each representing aspects of the business</p> <p>Each softwareSystem is composed of containers that represent a logical service and related containers are grouped together</p> <p>Relationships between containers are defined, stating the direction and relationship name, represented as arrows in the model views</p> <p><code>workspace.dsl</code> also contains a deploymentEnvironment for production, defining the infrastructure that containers are deployed into</p> <p>A range of views are defined, using include and exclude options to refine the containers that are shown</p> <p>Practicalli Structurizr custom theme and AWS theme are included, along with Practicalli logo in the branding</p>"},{"location":"software-design/architecture/structurizr/#resources","title":"Resources","text":"<ul> <li>Diagrams as Code 2.0 - Simon Brown, 2021.</li> <li>Getting started with Structurizr Lite - Simon Brown, 2021</li> <li>Structurizr DSL: AWS Example deployment</li> <li>Structurizr DSL: Big Bank</li> <li>Structurizr/dsl code examples</li> <li>Model example: Amazon Web Services local</li> </ul>"},{"location":"software-design/design-patterns/","title":"Design Patterns","text":"<p>A pattern is</p> <ul> <li>an arrangement of repeated parts</li> <li>a design or shape to direct the cutting of cloth for clothes</li> <li>a model or specimen</li> </ul>"},{"location":"software-design/design-patterns/command-query-separation/","title":"Command\u2013query separation","text":"<p>every method should either be a command that performs an action, or a query that returns data to the caller, but not both. In other words, asking a question should not change the answer.[1] More formally, methods should return a value only if they are referentially transparent and hence possess no side effects.</p> <p>Bertrand Meyer - Wikipedia</p> <p>Command-query separation is particularly well suited to a design by contract (DbC) methodology, in which the design of a program is expressed as assertions embedded in the source code, describing the state of the program at certain critical times.</p> <p>In DbC, assertions are considered design annotations\u2014not program logic\u2014and as such, their execution should not affect the program state. CQS is beneficial to DbC because any value-returning method (any query) can be called by any assertion without fear of modifying program state.</p> <p>In theoretical terms, this establishes a measure of sanity, whereby one can reason about a program's state without simultaneously modifying that state. In practical terms, CQS allows all assertion checks to be bypassed in a working system to improve its performance without inadvertently modifying its behaviour. CQS may also prevent the occurrence of certain kinds of heisenbugs.</p> <p>Design By Contract</p>"},{"location":"software-design/design-patterns/cqrs/","title":"Command Query Resposibility Segregation","text":"<p>system architecture that extends the idea behind command\u2013query separation to the level of services.[1][2] Such a system will have separate interfaces to send queries and to send commands. As in CQS, fulfilling a query request will only retrieve data and will not modify the state of the system (with some exceptions like logging access), while fulfilling a command request will modify the state of the system.</p> <p>Many systems push the segregation to the data models used by the system. The models used to process queries are usually called read models and the models used to process commands write models.</p>"},{"location":"software-design/domain-driven-design/","title":"Domain Driven Design","text":"<ul> <li>Ubiquitous Language, applicable withing a single bounded context</li> </ul> <p>A Bounded Context is a conceptual boundary where a domain model is applicable. It provides a context for the Ubiquitous Language that is spoken by the team and expressed in its carefully designed software model, as shown in Figure G.1.</p>"},{"location":"software-design/domain-driven-design/#terminology","title":"Terminology","text":""},{"location":"software-design/domain-driven-design/#domain","title":"Domain","text":"<p>A sphere of knowledge, influence or activity. The subject area to which the user applies a program is the domain of the software</p>"},{"location":"software-design/domain-driven-design/#model","title":"Model","text":"<p>A syste of abstractions that describes selected aspects of a domain and can be used to solve problems related to that domain</p>"},{"location":"software-design/domain-driven-design/#ubiquitous-language","title":"Ubiquitous Language","text":"<p>A language structred around the domain model and used by all team members to connect all the activities of the team with the software</p> <ul> <li>Data Dictionary</li> </ul>"},{"location":"software-design/domain-driven-design/#context","title":"Context","text":"<p>The setting in which a workd or statement appears that detemines its meaning</p>"},{"location":"software-design/domain-driven-design/ubquitous-language/","title":"Ubiquitous Language","text":"<p>A Ubiquitous language is built on the model, making that model integral to all communication between domain experts and engineering.</p> <p>Many project challenges arise when the language used is fractured.  </p> <p>Domain experts often use their own very familiar terms that may have great nuaunce and abstraction.</p> <p>Engineering team members tune their language to solving design and coding challenges within the domain.</p> <p>Communication limitations and time pressures constrain understanding, driving a gap between domain experts and engineering languages.</p> <p>The terminonlogy of day-to-day discussions is disconnected from the terminology embedded in the code.</p> <p>Language can often differ between informal speech and more formal writing of documents, even when communicated from the same person.</p> <p>Defining a versatile language that is universally defined and understood builds a solid foundation to build and evolve solutions effectively.</p>"},{"location":"software-design/domain-driven-design/ubquitous-language/#defining-language","title":"Defining language","text":"<p>Use the model as the backbone of a language</p> <p>Commit the team to exercising that language relentlessly in all communication within and across teams, especially the resulting source code and design documentation.</p> <p>Use the same language in diagrams, writing and speech.</p>"},{"location":"software-design/domain-driven-design/ubquitous-language/#enhance-language","title":"Enhance language","text":"<p>Experiment with alternative ways to express the language, reflecting alternative models.</p> <p>Conversations should seek to resolve all confusion of terms used, as society has always sought to do.</p> <p>Accept that a change in the language constitutes a change to the model.</p> <p>Domain experts should block to terms that inadequately convey domain understanding.</p> <p>Engineers should be observant for ambiguity or inconsistency which drives complexity in the design.</p> <p>Experiment with the model during disussions of the model.</p> <p>Describe scenarios out loud uisng the elements and interactions of the model, combining concepts in ways allowed by the model.  Find easier ways to say what you need to say, then take thouse new ideas back down to the diagrams and code.</p>"},{"location":"software-design/microservices/","title":"Microservices","text":"<p>Microservice architecture is a pattern for building scalable and easy-to-maintain web-based applications.</p> <p>Importance of code that is asynchronous and non-blocking.</p> <p>Reactive microservices in Clojure, which adhere to the principles underlying the Reactive Manifesto.</p> <ul> <li>Monitor</li> <li>Testing</li> <li>Security</li> </ul>"},{"location":"source-control/","title":"Source Control","text":"<p>Managing code changes is an important part of maintaining a usable code base and is an essential step to having a robust development workflow.</p> <p>Versioning code tracks all the changes over the life of a project, providing an audit trail of change.</p>"},{"location":"source-control/#git","title":"Git","text":"<p>Git is the defacto tool to manage code and other text artifacts of a project.</p> <p>Git uses a distributed model for sharing changes which also encourages branching to support easily experimenting with changes.</p>"},{"location":"source-control/#github-and-gitlab","title":"GitHub and GitLab","text":"<p>Code sharing services support Git and also add pull requests to enable people to contribute to projects they are not maintainers for.  This is especially useful for open source projects, allowing the community to contribute.</p> <p>Emoji guide for commit messages and pull requests</p> <p>gitmoji provides an interactive search for emoji that can be used with Git commit messages and pull request descriptions &amp; templates</p> <p>emojipedia provides a wider support for emoji across multiple services</p> <p>Collective Ownership of Code</p> <p>The distributed nature of Git supports and encourages collective ownership of the project code, as all developers take responsibility for the project assets</p>"},{"location":"source-control/#access-remote-repositories","title":"Access remote repositories","text":"<p>Sharted Git repositories can be accessed via HTTPS or SSH URL.</p> <p>SSH approach is typically more secure, especially as the files holding your keys on disk are encrypted.  SSH connections can be tunnelled through HTTPS if connecting to a remote repository via a very restricted firewall.</p> <p>HTTPS connection requires a personal access token is required (GitHub blocks HTTP access via password).  A personal access token can be create with limited access, only allowing access to specific services and information.</p> <p>A personal access token is saved in a plain text file, e.g. <code>~/.github</code>.  Should a token be compromised, it does not give access to the account on the remote repository, so the token can be deleted and recreated without compromising the service login account.</p>"},{"location":"source-control/#what-to-version","title":"What to Version","text":"<p>Text based files such as source code, unit and acceptance tests, web pages, cascading stlye sheets, database scripts and configuration files should all be versioned to ensure that at any time you can build the whole project from the version control system.</p> <p>Binary files such as images and logos should also be versioned, even though you cannot usually do meaningful diffs.  It is important to version those files that are part of the software project so your build, test and release scripts are always using the correct version.</p> <p>It is important to also version your build files (build.xml, myproject.pom) so that you always have the correct script versions to build, test and deploy your project.  If all your assets are in the version control system, it make it very easy to set up and maintain a continuous integration server.</p> <p>Compiled assets such as JavaDoc output, class files, jar and war files are not usually stored in the version control system as these can be generated.  This avoids duplication of information and ensures that information does not become outdated.</p>"},{"location":"source-control/git-concepts/","title":"Git Concepts","text":""},{"location":"source-control/git-concepts/#repository","title":"Repository","text":"<p>A repository is the magic box that contains all the information about your checked in items, the full history of your changes.</p>"},{"location":"source-control/git-concepts/#trunk","title":"Trunk","text":"<p>The trunk refers to the main set of files and changes that make up the project.    If you create a new repository and commit / push a change (eg. a new file) then you have added that change to the trunk.</p> <p>A trunk is what you typically release and refer to as the main version of the project.</p>"},{"location":"source-control/git-concepts/#branch","title":"Branch","text":"<p>A project under version control may be branched to work on new functionality or experimental development.  The branch becomes a separate independent copy, even though it is not actually be stored as a copy in the version control system.</p> <p>Changes committed / pushed to the trunk after the branch are not applied to the branch and changes committed / pushed to the branch are not applied to the trunk.</p>"},{"location":"source-control/git-concepts/#tag","title":"Tag","text":"<p>A tag or label refers to an important snapshot in time of your repository. The current version of all files can be tagged with a meaningful name or revision number. Tags are often used to indicate milestones or releases of a project.</p>"},{"location":"source-control/git-concepts/#head","title":"Head","text":"<p>The head is a special label that represents the latest version of your files in the repository.  When you checkout or clone a project you get the version that is pointed to by the head label - unless you specify a particular version.</p>"},{"location":"source-control/git-concepts/#commit","title":"Commit","text":"<p>A commit (check-in) is the action of writing or merging the changes made in the working copy back to the repository. The terms 'commit' and 'check-in' can also used in noun form to describe the new revision that is created as a result of committing.</p>"},{"location":"source-control/git-concepts/#update","title":"Update","text":"<p>An update merges changes made in the central repository (by other people, for example) into the local working copy / local repository</p>"},{"location":"source-control/git-concepts/#clone","title":"Clone","text":"<p>Creates a complete copy of a repository locally (used in distributed version control systems such as Git, Mercurial, Bazaar)</p>"},{"location":"source-control/git-concepts/#push","title":"Push","text":"<p>Merges change sets from your local repository to the main (parent) of your repository.</p>"},{"location":"source-control/git-concepts/#pull","title":"Pull","text":"<p>Merge change sets from other repositories - you pull in change sets and merge them into your repository</p>"},{"location":"source-control/git-concepts/#promote","title":"Promote","text":"<p>A more abstract term for committing or pushing, where changes are \"promoted\" from a less controlled location into a more controlled location. For example, from a user's workspace into a repository, or from a local clone to its main (parent).</p>"},{"location":"source-control/git-concepts/#diff","title":"Diff","text":"<p>A diff (or delta) represents a specific change to a project under version control.  Diffs between different versions of files are created to see what changes have been between two versions and giving you information on how to merge two files together.</p>"},{"location":"source-control/git-concepts/#merge","title":"Merge","text":"<p>A merge where two sets of changes are applied to a file or set of files. Some sample scenarios for merging include:</p> <p>You update your working copy or local repository with changes made by other users, either from a central repository or from another repository.</p> <p>You try to commit / push changes to files that have been updated by others since you checked out those files (or you last update).  The version control software automatically merges the files (typically, after prompting the user if it should proceed with the automatic merge, and in some cases only doing so if the merge can be clearly and reasonably resolved).</p> <p>The project is branched before a problem that existed is fixed in the main trunk.  The fix is done in the trunk and then merged into the branch.</p> <p>A branch is created, the code in the files is independently edited, and the updated branch is later incorporated into a single, unified trunk.</p>"},{"location":"source-control/git-concepts/#conflict","title":"Conflict","text":"<p>A conflict occurs when different changes are being applied to the same file, and the version control system is unable to reconcile the changes. The person making a merge needs to resolve the conflict by combining the changes manually, or by telling the version control system which version to use.</p>"},{"location":"source-control/git-concepts/#diff-and-merge","title":"Diff and Merge","text":"<p>When you want to merge code you may experience conflicts between your code and the code your merging.  If your version control tool is not able to automatically manage these conflicts, you will need to merge manually.</p> <p>To see the differences between two code files you use a diff tool.  A diff tool will show text references or a visual representation of the differences.</p> <p>Using the diff tool you can change your code so the conflicts are addressed and changes can be merged.</p> <p>Meld - visual diff and merge tool</p> <p>Meld is a visual diff and merge tool. You can compare two or three files and edit them in place (diffs update dynamically). You can compare two or three folders and launch file comparisons. You can browse and view a working copy from popular version control systems such such as CVS, Subversion, Bazaar-ng and Mercurial.</p> <p>Calling Meld from git-merge</p> <pre><code>git mergetool -t meld\n</code></pre>"},{"location":"source-control/git-concepts/#git-advanced-branching-and-merging","title":"Git Advanced branching and merging","text":"<p>All of the changes that git was able to merge automatically are already added to the index file, so git diff shows only the conflicts. It uses an unusual syntax:</p> <p>The commit which will be committed after we resolve this conflict will have two parents instead of the usual one: one parent will be HEAD, the tip of the current branch; the other will be the tip of the other branch, which is stored temporarily in MERGE_HEAD.</p> <p>http://book.git-scm.com/5_advanced_branching_and_merging.html</p>"},{"location":"source-control/git-configuration/","title":"Git Client Configuration","text":"<p>Git uses either <code>XDG_CONFIG_HOME/git/config</code> or <code>$HOME/.gitconfig</code> configuration file for user level settings for the Git client.</p> <p>Editor Git support should use the Git client configuration.</p> Practicalli Dotfiles Git Configuration <p> Practicalli Dotfiles contains an example Git user configuration, with separate identity configuration files for commercial and open source work.</p> <p>The Git configuration also provides global Git ignore patterns for Clojure and MkDocs projects.</p> <p>practicalli/dotfiles Git config files</p>"},{"location":"source-control/git-configuration/#git-identity","title":"Git identity","text":"<p>An identity is required when sharing commits via services such as GitHub/GitLab and so that each commit you make is associated to you.</p> <p>Define your git identity using the following commands in a terminal window</p> Use the GitHub Email Mask address <p>To minimise Email spam, use the email address provided by GitHub as a mask to your primary email address on the GitHub account.  The mask address is of the form <code>***+github-account@noreply.github.com</code>.</p> <p>Visit the email settings of the GitHub account and tick Keep my email addresses private.</p> <p>A new email of the form <code>******+github-account-name@users.noreply.github.com</code> is created which must be set as your user email address</p> <p>For additional security, select the option Block command line pushes that expose my email to prevent commits being pushed to GitHub using your public email address.</p> <pre><code>git config --global user.name \"practicalli-johnny\"\n\ngit config --global user.email ***+github-account@noreply.github.com\n</code></pre> <p>The <code>[user]</code> section of the Git configuration file is updated by these commands, automatically creating the file and section if it does not exist.</p> Git Configuration .config/git/identity-clojure-inc<pre><code># Add identity to all commits (required for GitHub / GitLab)\n[user]\n name = Practicalli Johnny\n\n    # add email to Personal GitHub account via Settings &gt; Email\n email = \"johnny@clojure.inc\"\n\n## Identity for using GitHub API\n[github]\n user = practicalli-johnny\n\n## SSH Keys - add key passphrase to MacOSX key chain\n[credential]\n    helper = osxkeychain\n</code></pre>"},{"location":"source-control/git-configuration/#multiple-git-identities","title":"Multiple Git Identities","text":"<p>When working on a mixture of commercial and Open Source projects, configure the Git client with multiple identities</p> <p>Git Clone alias</p> .config/git/identity-clojure-inc<pre><code>## ------ Git Config: Identity ------ ##\n\n# Default identity configuration\n[include]\n  path = ~/.config/git/identity-practicalli-johnny\n\n# Override identify for specific directories\n[includeIf \"gitdir:~/projects/identity-clojure-inc\"]\n  path = ~/.config/git/identity-clojure-inc\n</code></pre> MacOSX Path expansion not working <p>MacOSX did not expand ~ or $HOME for relative paths for identity files when using the latest MacOSX version and Git client from homebrew.</p> <p>Included configure file with company identity.</p> Git Clone alias .config/git/identity-clojure-inc<pre><code>## ------ Company Identity ------ ##\n# Add details for specific company identity\n\n# Add identity to all commits (required for GitHub / GitLab)\n[user]\n name = Practicalli Johnny\n\n    # add email to Personal GitHub account via Settings &gt; Email\n email = \"johnny@clojure.inc\"\n\n## Identity for using GitHub API\n[github]\n user = practicalli-johnny\n\n## SSH Keys - add key passphrase to MacOSX key chain\n[credential]\n    helper = osxkeychain\n</code></pre>"},{"location":"source-control/git-configuration/#ssh-key","title":"SSH Key","text":"<p>Use an SSH key for secure access to a remote Git repository. The SSH key removes the need to enter GitHub credentials each time a command is used that accesses a remote repository (push, pull, clone, etc.).</p> <p>Generate an SSH key and add it to the GitHub account.</p> <p></p> <p>Generate an SSH Key with a secure passphrase for access to GitHub repositories (pull, push over SSH)</p> <p>Create an SSH key with the <code>ssh-keygen</code> command, using the <code>-C</code> argument to specify the Email address added to your GitHub accounty, replacing with your own name.</p> <p>Generate an SSH Key with Git email identity</p> <pre><code>ssh-keygen -t ed25519 -C \"654321+practicalli-johnny@users.noreply.github.com\"\n</code></pre> <p>Enter a passphrase.  A 12 character or greater passphrase should provide adequate security.</p> SSH Key Passphrase <p>Practicalli recommends setting a passphrase when generating an SSH key to add an extra layer of security.  If the computer containing keys should be compromised then a passphrase is requires to use the private keys.</p> <p>The passphrase can be added to the operating system key ring, unlocking the key when logging into the operating system account.</p> Use the GitHub Email Mask address <p>Minimise Email spam by using the email address provided by GitHub as a mask to your primary email address on the GitHub account.  The mask address is of the form <code>***+github-account@noreply.github.com</code>.</p>"},{"location":"source-control/git-configuration/#add-ssh-key-to-keychain","title":"Add SSH key to Keychain","text":"<p>Add Git SSH key passphrase to Operating System keychain to avoid typing in the passphrase each time a Git command interacts with a remote repository.</p> LinuxMacOSX <p>Add the SSH private key to the ssh-agent to avoid typing in the passphrase each time.  Logging into the operating system with the user account will unlock the key ring and enable access to the passphrase.</p> <pre><code>ssh-add ~/.ssh/id_ed25519\n</code></pre> <p>Ubuntu desktop has a key-ring tool which will display a pop-up dialog to save the passphrase to the key-ring the first time the SSH key is used. Once saved, the key is unlocked when login into the desktop.</p> <p>Add SSH key passphrase to MacOSX Keychain</p> <pre><code>ssh-add --apple-use-keychain ~/.ssh/id_ed25519\n</code></pre> <p>Edit the <code>~/.ssh/config</code> file and add/modify to include the following configuration</p> <p>SSH key Key Chain Git Configuration</p> <pre><code>Host github.com\n  AddKeysToAgent yes\n  UseKeychain yes\n  IdentityFile ~/.ssh/id_ed25519\n</code></pre> <p>If there is an issue with the passphrase, delete the key passphrase using the MacOSX Keychain Access App.  Select <code>Local Items</code> to see a list of keys that includes the SSH key.  Select the key to show a menu that allows deletion.</p> <p>The command line terminal can be used to delete keys from the MacOSX keychain using the ssh-add command with the -d keyname to delete a specific key or -D option to delete all user added keys.</p>"},{"location":"source-control/git-configuration/#commit-signing-with-ssh-key","title":"Commit signing with SSH Key","text":"<p>The SSH key can be registered with your GitHub account as a signing key, as opposed to an authorization key used to access a remote repository securely.</p> <p>Use an existing SSH key to sign commits and tags, or generate a new one specifically for signing.</p> <p>Configure Git client to use SSH to sign commits and tags for all local repositories</p> Git Configuration SSH Key sigining <p>```config</p> <p>## ------ Git Behaviour ------ ##     [commit]       # Automatically sign every commit      gpgsign = true</p> <pre><code>[tag]\n  # Automatically sign every tag\n gpgsign = true\n\n# SSH Key signing \n[user]\n signingkey = ~/.ssh/id_ed25519.pub\n[gpg]\n format = ssh\n[gpg \"ssh\"]\n allowedSignersFile = ~/.config/git/allowed-signatures\n```\n</code></pre> <p>Configure SSH key as signing format</p> <pre><code>git config --global gpg.format ssh\n</code></pre> <p>Specify the file that contains the public SSH key to use for signing</p> <pre><code>git config --global user.signingkey $HOME/.ssh/id_ed25519.pub\n</code></pre> <p>Automatically sign commits and tags when creating a commit</p> <pre><code>git config --global commit.gpgsign true &amp;&amp; \\\ngit config --global tag.gpgsign true\n</code></pre>"},{"location":"source-control/git-configuration/#allowed-ssh-keys","title":"Allowed SSH keys","text":"<p>The <code>--show-signature</code> flag with Git <code>log</code> and <code>show</code> commands checks the contents of the <code>gpg.ssh.allowedSignersFile</code> to know which keys are valid</p> <p>Create an <code>$HOME/.config/git/allowed-signatures</code> file to list the SSH keys that you wish to define as allowed to sign commits.  </p> <p>Each key entry should start with the email address used for commits, followed by the full public key value (which also ends with the email)</p> <p>Set the <code>gpg.ssh.allowedSignersFile</code> file in the Git Configuration</p> <pre><code>git config gpg.ssh.allowedSignersFile \"$HOME/.config/git/allowed-signatures\"\n</code></pre> SSH keys on multiple machines <p>When using different SSH keys across multiple computers, add all public keys to the <code>allowed-signatures</code> file.</p> <p>Use a secret GitHub gist if you do not wish to add public keys to a shared git repository for the Git configuration.</p>"},{"location":"source-control/git-configuration/#clone-aliases-for-a-github-domain","title":"Clone aliases for a GitHub domain","text":"<p>Define a short-cut alias to simplify the URL argument for the repository when using the Git clone command, e.g <code>git clone p:clojure-cli-config</code> rather than <code>git clone git@github.com:practicalli/clojure-cli-config</code></p> <p>Git Clone alias</p> .config/git/config<pre><code># Clone short-cuts\n[url \"git@github.com:practicalli/\"]\n # git clone p:repo-name\n insteadOf = p:\n</code></pre>"},{"location":"source-control/git-configuration/#diff-3-support","title":"Diff 3 Support","text":"<p>Diff 3 standard included the parent of two changes in conflict, providing additional context when deciding which change should take precedence</p> <pre><code>git config --global merge.conflictstyle diff3\n</code></pre> <p>This command adds a <code>conflictstyle</code> entry in the <code>[merge]</code> section of the Git configuration file.</p> <pre><code>[merge]\n    # Include common parent when merge conflicts arise\n    conflictstyle = diff3\n</code></pre> <p>Magit supports the Diff3 standard, so a common parent will be shown when this feature is enabled.</p> <p></p>"},{"location":"source-control/git-overview/","title":"Git Overview","text":"<p>Decentralised approach Local repository (clone) as well as central (master) Can clone a clone Can work disconnected from the central repository Designed for branching and merging and long lived branches Each change is a delta, strung together so merging can be done step by step (change sets) Easy to take a repository, clone it to experiment with the code and then request to the original repository owner to merge your changes if they like what they see Adopted by a very large number of open source projects, most new open source projects use git now. Well supported by main developer IDEs (Netbeans built in, Eclipse)</p> <p>Decentralised model When ever you work with a project in a git repository, you take a clone copy.  A clone is a fully fledged Git repository in itself, with complete history and full revision tracking capabilities.</p> <p>Having your own local git repository allows you to commit code as and when you like without affecting anyone else working on the project.  You are creating your own local changes which can be pushed back to the orignial project repository at a convenient opportunity (eg. when all your tests are passing)</p> <p>Limitations? Whole repository needs to be checked out - usually results in separating out code bases into their own repositories - aligns well with Maven modular project approach - change deltas stored very efficiently keeping size to minimum</p> <p>Several new commands to learn if coming from a central repository approach, a small learning curve to understand the concepts.</p> <p>Getting started with Git</p> <p>Create git repository</p> <p>Local repository and central repository (master)</p> <p>Local Commit</p> <p>Pushing to central repository</p> <p>Merging (pulling in? ) changes from other repositories</p> <p>Advanced code management</p> <p>Operational Concerns Backups Security</p>"},{"location":"source-control/git-personal-access-token/","title":"Git Personal Access Token","text":"<p>A personal access token is required when accessing a remote repository over HTTPS.</p>"},{"location":"source-control/git-personal-access-token/#generate-a-token","title":"Generate a token","text":"<p>Visit the remote repository service and generate a personal access token with at least <code>repo</code> permission.</p> <ul> <li>GitHub personal access token documentation</li> <li>GitLab personal access token documentation</li> </ul> <p>Whilst the token could be added to the <code>~/.gitconfig</code>, as this file is plain text it is not particularly secure (especially if committed into a dotfiles repository and shared).</p> <pre><code>git config --global oauth.token \"tokens-in-plain-text-files-are-not-very-secure\"\n</code></pre> <p>To provide greater security when using the token, consider using the Git Credential Manager.  </p> Magit Forge uses personal access token <p>Magit Forge also requires a personal access token, although this can be saved in the encrypted file <code>~/.authinfo.gpg</code> for greater security.  The Magit Forge token includes permissions required to access remote repositories over HTTPS</p> Octo plugin for Neovim uses token from GitHub CLI"},{"location":"source-control/git-status/","title":"Git Status","text":"<p>Show the status of the files in the source controlled project, comparing the working copy with the latest commit on the current branch.</p> <p>A file can have one of the following status values</p> <ul> <li>untracked</li> <li>modified</li> <li>staged</li> </ul> <p>A file can have both modified and staged statuses if only specific lines or hunks were staged.</p>"},{"location":"source-control/git-status/#git-alias","title":"Git Alias","text":"<p>Define a command line alias for Git, e.g. <code>git sr</code> or <code>git sitrep</code> that includes status command flags to show a short-format report that also includes the branch name</p> <p>Define Git alias for status</p> .config/git/config<pre><code>[alias]\n sitrep = status --short --branch\n sr = status --short --branch\n</code></pre>"},{"location":"source-control/git-status/#report-across-repositories","title":"Report across repositories","text":"<p>Use the <code>mgitstatus</code> command line tool to show the status of all the repositories under a give directory</p> <pre><code>cd ~/projects/practicalli &amp;&amp; mgitstatus\n</code></pre> <p>The status includes the following types:</p> <ul> <li>ok - no changes</li> <li>untracked files</li> <li>uncommitted changes</li> <li>needs push (branchs)</li> <li>needs pull (branches)</li> </ul> <p></p>"},{"location":"source-control/github/","title":"GitHub","text":"<p>An online service for sharing public repositories for open source projects and private repositories for commercial work or other private work.</p>"},{"location":"source-control/github/#accounts","title":"Accounts","text":"<p>User account</p> <p>Organisation</p>"},{"location":"source-control/github/#ssh-access","title":"SSH access","text":"<p>Set up a public private key on each computer to be used as a GitHub client, to push commits over an SSH connection.</p>"},{"location":"source-control/github/#github-tips","title":"GitHub tips","text":"<p>Collapsible section in readme, issue or pull request</p> <pre><code>&lt;details&gt;\n  &lt;summary&gt;Click to expand&lt;/summary&gt;\n\n  Leave a blank space after the closing summary tag so that the markdown content renders correctly.\n&lt;/details&gt;\n</code></pre>"},{"location":"source-control/pull-request/","title":"Pull Requests","text":"<p>Emoji guide for commit messages and pull requests</p> <p>gitmoji provides an interactive search for emoji that can be used with Git commit messages and pull request descriptions &amp; templates</p>"},{"location":"technical-writing/","title":"Technical writing","text":""},{"location":"technical-writing/#a-practical-approach","title":"A Practical approach","text":"<p>Write what you want to say, then rewrite the way you wish to say it, e.g brain-dump thoughts at first without concern for the prose, refactor to make thoughts intelligable and easily consumable.</p> <p>The hardest part of writing can be getting over the blank page.</p> <p>A focus on capturing thoughts first ... a wider or deeper coverage of what is to be said.  Once extensive thoughs are captured, apply refinement to how thoughts are express (without risk of loosing thoughts).</p>"},{"location":"technical-writing/#website-generators","title":"Website generators","text":"<p>Static web sites are fast to serve and low maintenance, providing an excellent approach to serving up technical documentation.</p> <p>Using Markdown or Asciidoc minimises the learning curve for creating documentation, allowing thoughts to be captured quickly without distracting concerns on visual presentation.</p>"},{"location":"technical-writing/#clojure-tech-docs","title":"Clojure tech docs","text":"<p>cljdoc</p> <p>cljdoc is a website building &amp; hosting documentation for Clojure/Script libraries</p>"},{"location":"technical-writing/#tips","title":"Tips","text":"<ul> <li>learn English grammar</li> <li>avoid English pronouns, pronouns are often an indicator of verbosity</li> </ul>"},{"location":"technical-writing/static-site/mkdocs/","title":"MkDocs Document website Generator","text":"<p>MkDocs is a python tools for generating documentation sites from Markdown.</p>"},{"location":"technical-writing/static-site/mkdocs/#install","title":"install","text":""},{"location":"technical-writing/static-site/mkdocs/#additional-plugins","title":"Additional Plugins","text":""},{"location":"technical-writing/static-site/mkdocs/#github-workflow","title":"GitHub workflow","text":""},{"location":"technical-writing/static-site/mkdocs/#reference","title":"Reference","text":""}]}